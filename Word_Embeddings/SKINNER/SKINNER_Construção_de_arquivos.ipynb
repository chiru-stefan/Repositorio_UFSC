{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "TkuU4Bh7qZHf",
        "r5RYe0uqRLOw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Link para melhor visualização do Notebook em seu ambiente de execução: [Notebook Colab](https://colab.research.google.com/drive/1qeqlEviy6-S9Ms99F6uJjIgUm7FVNGsn?usp=sharing)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Este notebook foi desenvolvido para construção dos arquivos utilizados pelo Semantic Knowledge and Interpretation Navigator for Nurturing Exact References (SKINNER) voltado para modelos WOKE. Esta estratégia de extração de referências visa realizar operações semelhantes às do treinamento de modelos Word2Vec para construção de Word Embeddings, agrupando tokens de contexto referentes aos seus respectivos tokens presentes no vocabulário dos modelos, analisando assim a formação das palavras de contexto para um determinado vetor de palavras. Esse agrupamento leva em consideração o vocabulário do modelo, o tamanho da janela de contexto utilizado no parâmetro de treinamento do modelo, e a frequência da co-ocorrencia de tais palavras no contexto para quantificar o grau de influência."
      ],
      "metadata": {
        "id": "_7RKOZ9vTjyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuração de ambiente\n",
        "Instalação e importação de bibliotecas que serão utilizadas ao decorrer das execuções.\n",
        "\n",
        "Recomenda-se que sempre execute os programas usando o Google Colab, no qual os testes foram desenvolvidos."
      ],
      "metadata": {
        "id": "TkuU4Bh7qZHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  from google.colab import output\n",
        "  drive.mount('/content/drive')\n",
        "except Exception:\n",
        "  GOOGLE_COLAB = False\n",
        "else:\n",
        "  GOOGLE_COLAB = True\n",
        "finally:\n",
        "  import os\n",
        "  import msgpack\n",
        "  import re\n",
        "  from gensim.models import Word2Vec\n",
        "  !pip install openpyxl\n",
        "  import openpyxl\n",
        "  !pip install gdown\n",
        "  import gdown\n",
        "  import platform\n",
        "\n",
        "  OS_ATUAL = platform.system()\n",
        "  CAMINHO_EXEC_ATUAL = os.getcwd()\n",
        "  CAMINHO_SKINNER = os.path.join(CAMINHO_EXEC_ATUAL,'SKINNER_files')\n",
        "  CAMINHO_PLANILHA_METADADOS = os.path.join(CAMINHO_SKINNER,'planilha_metadados_woke.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNkXiFmpRZ4H",
        "outputId": "e56bd4bc-2c81-4ef7-d34c-4d5e71884b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções"
      ],
      "metadata": {
        "id": "r5RYe0uqRLOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def organizarAmbienteExecucao():\n",
        "  \"\"\"\n",
        "  Função responsável por organizar as pastas que serão utilizadas pelo SKINNER.\n",
        "  É onde ocorre a criação da pasta \"SKINNER_files\" no diretório de execução.\n",
        "  Esta pasta armazenará tanto a planilha de metadados, onde serão puxados os\n",
        "  assuntos, links para a página do RI e para o PDF de cada trabalho, além dos\n",
        "  arquivos de cada modelo analisado.\n",
        "  \"\"\"\n",
        "  global CAMINHO_SKINNER\n",
        "  try:\n",
        "    os.makedirs(CAMINHO_SKINNER,exist_ok=True)\n",
        "  except Exception:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def coletar_ID_link_Dive(link_drive : str) -> str:\n",
        "  \"\"\"\n",
        "  Função responsável por coletar o ID de um arquivo do Google Drive com base no\n",
        "  seu link de compartilhamento.\n",
        "\n",
        "  ### Parâmetros:\n",
        "  - link_drive: String contendo o link de compartilhamento do arquivo ou pasta\n",
        "  no Google Drive.\n",
        "\n",
        "  ### Retornos:\n",
        "  - String contendo o ID do arquivo ou None, caso o link não esteja no padrão\n",
        "  esperado.\n",
        "  \"\"\"\n",
        "  padrao_regex = r'\\/d\\/(.+)/' # Padrão regex para identificar o link no meio do URL\n",
        "  busca = re.search(padrao_regex,link_drive) # Buscando padrão dentro do link fornecido\n",
        "  if busca: # Verificando se a busca pelo padrão dentro do link foi bem sucedida\n",
        "    return busca.group(1)\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def baixarArquivoDrive(ID_arquivo : str,\n",
        "                       caminho_destino : str,\n",
        "                       silencio : bool = False) -> bool:\n",
        "  \"\"\"\n",
        "  Função responsável por baixar um arquivo do Drive por meio do seu ID e salvá-lo\n",
        "  numa pasta no diretório de execução deste programa.\n",
        "\n",
        "  ### Parâmetros:\n",
        "  - ID_arquivo: String contendo o ID do arquivo do Drive a ser baixado.\n",
        "  - caminho_destino: String contendo o caminho completo (contendo o nome e\n",
        "  extensão do arquivo no final).\n",
        "  - silencio: Bool responsável por imprimir no output ou não informações a respeito\n",
        "  da execução deste processo.\n",
        "\n",
        "  ### Retornos:\n",
        "  - Bool referente ao sucesso (True) ou não (False) da execução total deste processo.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # URL no formato que o gdown utiliza para baixar arquivos do Google Drive por meio do ID\n",
        "    url = f'https://drive.google.com/uc?export=download&id={ID_arquivo}'\n",
        "    os.makedirs(os.path.dirname(caminho_destino),exist_ok=True)\n",
        "    # Execução do processo de baixar o arquivo do Drive e trazê-lo para este ambiente de execução\n",
        "    gdown.download(url, caminho_destino, quiet=silencio)\n",
        "  except Exception as e:\n",
        "    if not silencio:\n",
        "      print(f'Erro na hora de baixar arquivo do Drive: {e.__class__.__name__}: {str(e)}')\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def limparConsole():\n",
        "    \"\"\"\n",
        "    Função responsável por limpar a tela de output/console.\n",
        "    \"\"\"\n",
        "    global GOOGLE_COLAB\n",
        "    global OS_ATUAL\n",
        "    if GOOGLE_COLAB: # Se estivermos executando no Colab, melhor utilizar a função própria para limpar o output neste ambiente\n",
        "        output.clear()\n",
        "    elif OS_ATUAL.lower() == 'windows': # Se for Windows\n",
        "        os.system('cls')\n",
        "    else:   # Se for MAC/Linux\n",
        "        os.system('clear')\n",
        "\n",
        "def carregarPlanilha(caminho_planilha : str = ''):\n",
        "  \"\"\"\n",
        "  Função responsável por carregar a planilha de metadados na memória RAM do programa.\n",
        "\n",
        "  ### Parâmetros:\n",
        "  - caminho_planilha: String contendo o caminho até o arquivo referente a\n",
        "  planilha de metadados.\n",
        "\n",
        "  ### Retornos:\n",
        "  - None ou a instância da planilha carregada via openpyxl.\n",
        "  \"\"\"\n",
        "  global CAMINHO_SKINNER\n",
        "  if not caminho_planilha:\n",
        "    caminho_planilha = os.path.join(CAMINHO_SKINNER,'planilha_metadados_woke.xlsx')\n",
        "  try:\n",
        "    # Carregar o workbook usando openpyxl\n",
        "    wb = openpyxl.load_workbook(caminho_planilha)\n",
        "  except Exception:\n",
        "    return None\n",
        "  else:\n",
        "    return wb\n",
        "\n",
        "# coletarInfoPlanilha(wb=wb,nome_colecao='HST',nome_trabalho='Trabalho 189')\n",
        "def coletarInfoPlanilha(wb,\n",
        "                        nome_colecao : str,\n",
        "                        nome_trabalho : str) -> dict:\n",
        "  \"\"\"\n",
        "  Função responsável por coletar e retornar as informações sobre Assuntos, Link\n",
        "  da página no site do Repositório Institucional da UFSC e Link para o arquivo\n",
        "  PDF do trabalho em questão.\n",
        "\n",
        "  ### Parâmetros:\n",
        "  - wb: Instância da planilha carregada via openpyxl.\n",
        "  - nome_colecao: String contendo o nome da coleção onde se encontra o trabalho\n",
        "  em questão.\n",
        "  - nome_trabalho: String contendo o nome do Trabalho em questão.\n",
        "\n",
        "  ### Retornos:\n",
        "  - Dicionário contendo as informações do trabalho, caso o processo de coleta\n",
        "  seja bem sucedido, caso contrário os valores das chaves do dicionário serão\n",
        "  None.\n",
        "  \"\"\"\n",
        "  global dic_nomes_alterados_abas_planilha\n",
        "  dic_resultado = {'assuntos':None,'link_pagina':None,'link_pdf':None}\n",
        "  try:\n",
        "    nome_colecao_planilha = formatarNomeArquivoPlanilha(nome_colecao)\n",
        "    if nome_colecao_planilha in dic_nomes_alterados_abas_planilha.keys():\n",
        "      nome_colecao_planilha = dic_nomes_alterados_abas_planilha[nome_colecao_planilha]\n",
        "    else:\n",
        "      nome_colecao_planilha = nome_colecao_planilha[:31]\n",
        "    ws=wb[nome_colecao_planilha]\n",
        "    numero_trabalho = int(nome_trabalho.split()[-1])\n",
        "    dic_resultado['assuntos'] = ws.cell(row=numero_trabalho+1, column=5).value\n",
        "    dic_resultado['link_pagina'] = ws.cell(row=numero_trabalho+1, column=11).hyperlink.target\n",
        "    dic_resultado['link_pdf'] = ws.cell(row=numero_trabalho+1, column=12).hyperlink.target\n",
        "    return dic_resultado\n",
        "  except Exception as e:\n",
        "    print(e.__class__.__name__,str(e))\n",
        "    return {'assuntos':None,'link_pagina':None,'link_pdf':None}\n",
        "\n",
        "\n",
        "def salvarResultadosEmMsgPack(nome_variavel : str,\n",
        "                              variavel_em_questao,\n",
        "                              pasta_para_salvar : str) -> tuple[bool,str]:\n",
        "    \"\"\"\n",
        "    Função responsável por salvar, no formato \"msgpack\", alguma variável deste\n",
        "    ambiente numa pasta especificada.\n",
        "\n",
        "    ### Parâmetros:\n",
        "    - nome_variavel: String contendo o nome da variável que será dado ao arquivo.\n",
        "    - variavel_em_questao: Variável propriamente dita que se deseja salvar num\n",
        "    arquivo.\n",
        "    - pasta_para_salvar: String contendo o caminho até a pasta onde se deseja\n",
        "    salvar o arquivo da variável em questão.\n",
        "\n",
        "    ### Retornos:\n",
        "    - Tupla com dois elementos. O primeiro do tipo bool que faz referência ao\n",
        "    status do processo (True bem sucedido, False falhou) e o segundo do tipo\n",
        "    string contendo a mensagem referente ao status obtido.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Se a pasta não existir, efetuamos sua criação\n",
        "        if not os.path.exists(pasta_para_salvar):\n",
        "            os.makedirs(pasta_para_salvar)\n",
        "\n",
        "        # Obtendo os bytes da variável que queremos salvar\n",
        "        variable_bytes = msgpack.packb(variavel_em_questao)\n",
        "\n",
        "        # Se o nome do arquivo que salvará a variável não tiver o formato msgpack ao final, iremos adicioná-lo.\n",
        "        if not nome_variavel.endswith('.msgpack'):\n",
        "            nome_variavel += '.msgpack'\n",
        "\n",
        "        # Salvando a variável desejada em bytes no formato .msgpack\n",
        "        with open(os.path.join(pasta_para_salvar,nome_variavel),'wb') as f:\n",
        "            f.write(variable_bytes)\n",
        "            f.close()\n",
        "            return True, f'Variável {nome_variavel} salva com sucesso no formato .msgpack'\n",
        "\n",
        "    # Caso ocorra algum erro inesperado, trataremos o enviando como mensagem de falha juntamente com um status False (de falha/erro no processo)\n",
        "    except Exception as e:\n",
        "        error_message = f'{e.__class__.__name__}: {str(e)}'\n",
        "        return False, error_message\n",
        "\n",
        "def abrirArquivoMsgPack(full_filepath : str,\n",
        "                        encoding_type : str = None):\n",
        "    \"\"\"\n",
        "    Função responsável por abrir os arquivos no formato msgpack.\n",
        "\n",
        "    ### Parâmetros:\n",
        "    - full_filepath: String contendo o caminho completo até o arquivo que deseja-se\n",
        "    abrir e extrair o conteúdo (variável salva).\n",
        "    - encoding_type: String contendo o tipo de encoding, caso desejar.\n",
        "\n",
        "    ### Retornos:\n",
        "    - Variável salva (e agora aberta e lida) no arquivo msgpack.\n",
        "    \"\"\"\n",
        "    if not full_filepath.endswith('.msgpack'):\n",
        "        full_filepath += '.msgpack'\n",
        "    if encoding_type:\n",
        "        with open(full_filepath,'rb',encoding=encoding_type) as f:\n",
        "            variable_bytes = f.read()\n",
        "            variable_loaded = msgpack.unpackb(variable_bytes, raw=False)\n",
        "            f.close()\n",
        "            return variable_loaded\n",
        "    else:\n",
        "        with open(full_filepath,'rb') as f:\n",
        "            variable_bytes = f.read()\n",
        "            variable_loaded = msgpack.unpackb(variable_bytes, raw=False)\n",
        "            f.close()\n",
        "            return variable_loaded\n",
        "\n",
        "\n",
        "def extracaoTokensContexto(frase : list[str],\n",
        "                           token : str,\n",
        "                           i_token : int,\n",
        "                           window : int,\n",
        "                           dic_contexto : dict) -> dict:\n",
        "  \"\"\"\n",
        "  Função responsável por atualizar, com base numa dada frase e no tamanho da janela\n",
        "  de contexto, o dicionário que armazena as informações dos tokens de contexto para\n",
        "  um determinado token central fornecido.\n",
        "\n",
        "  ### Parâmetros:\n",
        "  - frase: Lista de strings/tokens sendo uma frase tokenizada.\n",
        "  - token: String contendo o token central/alvo, que está sendo analisado.\n",
        "  - i_token: Índice do token na lista de tokens da frase atual.\n",
        "  - window: Inteiro referente ao tamanho da janela de contexto.\n",
        "  - dic_contexto: Dicionário contendo os tokens de contexto e suas ocorrências\n",
        "  para o token alvo atual.\n",
        "\n",
        "  ### Retornos:\n",
        "  - Dicionário de contexto atualizado para o token central atual.\n",
        "  \"\"\"\n",
        "  qtd_palavras = len(frase) # Coleta a quantidade de tokens presentes na frase fornecida para atualização de contexto\n",
        "  for n in range(1,window+1): # A iteração será apenas no intervalo de 1 ao tamanho da janela de contexto\n",
        "    indice_atual = i_token + n # Este laço for será responsável por coletar os tokens de contexto à direita do token central\n",
        "    if indice_atual < qtd_palavras and indice_atual >= 0: # Se o índice atual na lista de tokens da frase for válido\n",
        "      token_contexto = frase[indice_atual] # Armazenando o token de contexto analisado no momento\n",
        "      if token_contexto != token: # Se o token de contexto for diferente do token central\n",
        "        if token_contexto in dic_contexto.keys(): # Se o token de contexto encontrado já estiver dentro do dicionário de contexto do token central\n",
        "          dic_contexto[token_contexto] += 1 # Adicionamos mais uma ocorrência\n",
        "        else:\n",
        "          dic_contexto[token_contexto] = 1 # Criamos uma chave no dicionário para o token de contexto e botamos uma ocorrência para ele\n",
        "\n",
        "  # A mesma lógica do laço for de cima será aplicada neste laço for abaixo, porém contemplando os tokens de contexto à esquerda do token central/alvo\n",
        "  for n in range(1,window+1):\n",
        "    indice_atual = i_token - n  # Este laço for será responsável por coletar os tokens de contexto à direita do token central\n",
        "    if indice_atual < qtd_palavras and indice_atual >= 0:\n",
        "      token_contexto = frase[indice_atual]\n",
        "      if token_contexto != token:\n",
        "        if token_contexto in dic_contexto.keys():\n",
        "          dic_contexto[token_contexto] += 1\n",
        "        else:\n",
        "          dic_contexto[token_contexto] = 1\n",
        "\n",
        "  return dic_contexto # Após feitas as adições e alterações nos valores das ocorrências dentro do dicionário de contexto fornecido, retornamos ele atualizado\n",
        "\n",
        "def formatarNomeArquivoPlanilha(nome_colecao : str) -> str:\n",
        "  \"\"\"\n",
        "  Função responsável por formatar o nome da coleção encontrada nas pastas referentes\n",
        "  ao corpus de textos pré-processados para o nome da coleção presente na planilha\n",
        "  de metadados, o qual sofreu abreviações na tentativa de diminuir seu tamanho.\n",
        "\n",
        "  ### Parâmetros:\n",
        "  - nome_colecao: String contendo o nome da coleção referente a pasta da coleção\n",
        "  no corpus de documentos pré-processados.\n",
        "\n",
        "  ### Retornos:\n",
        "  - String do nome da coleção fornecido com a formatação para bater com o nome da\n",
        "  mesma coleção na planilha.\n",
        "  \"\"\"\n",
        "  return nome_colecao.replace('Mestrado_Profissional','MP').replace('Universitaria','UNIV').replace('Desenvolvimento','DESENV').replace('Biotecnologia','BIOTEC').replace('Biologia','BIO').replace('Administracao','ADM').replace('Engenharia','ENG').replace('Historia','HST').replace('__','_').replace('_de_','_').replace('Programa_Pos_Graduacao','PG').replace('Propriedade','Prop').replace('Intelectual','Intelec').replace('Transferencia','Transf').replace('Tecnologia','Tec')\n"
      ],
      "metadata": {
        "id": "YIBl3U0YNWM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregando planilha de metadados neste Programa"
      ],
      "metadata": {
        "id": "GkRa2e6-XRt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if organizarAmbienteExecucao():\n",
        "  # Link de compartilhamento do arquivo referente à Planilha de Metadados no Google Drive\n",
        "  link_planilha_metadados_drive = 'https://docs.google.com/spreadsheets/d/1bXMirZN8NoyaXiRDj9CY19QDIuw9cPyB/edit?usp=drive_link&ouid=107024036721805330434&rtpof=true&sd=true'\n",
        "\n",
        "  # Coletando o ID do arquivo\n",
        "  id_planilha = coletar_ID_link_Dive(link_planilha_metadados_drive)\n",
        "\n",
        "  if id_planilha:\n",
        "    # Baixando arquivo da Planilha de Metadados neste ambiente\n",
        "    status_download = baixarArquivoDrive(ID_arquivo=id_planilha,caminho_destino=CAMINHO_PLANILHA_METADADOS)\n",
        "    limparConsole()\n",
        "    if status_download:\n",
        "      # Carregando planilha na RAM\n",
        "      wb = carregarPlanilha()\n",
        "      if wb:\n",
        "        # Etapa adicional para lidar com nomes de abas iguais na Planilha de Metadados proveniente da limitação de caractéres (máximo de 31) no nome das abas\n",
        "        dic_nomes_alterados_abas_planilha = {}\n",
        "        lista_nomes_abas_planilha = []\n",
        "        # Coletando os nomes das coleções via pastas do corpus de documentos pré-processados\n",
        "        caminho_pasta_colecoes_pp = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Textos_pre_processados/Colecoes_textos_pre_processados'\n",
        "        for nome_colecao in [formatarNomeArquivoPlanilha(pasta) for pasta in os.listdir(caminho_pasta_colecoes_pp) if '.' not in pasta]:\n",
        "          nome_colecao_formatado = nome_colecao[:31] # Limitando o nome para 31 caractéres\n",
        "          if nome_colecao_formatado in lista_nomes_abas_planilha: # Se o nome se repetir\n",
        "            dic_nomes_alterados_abas_planilha[nome_colecao]=f'Página{len(dic_nomes_alterados_abas_planilha.keys())+1}' # O primeiro nome repetido terá seu nome na planilha como \"Página1\"\n",
        "          else:\n",
        "            lista_nomes_abas_planilha.append(nome_colecao_formatado)\n",
        "\n",
        "        print('Sucesso ao carregar Planilha de Metadados.')\n",
        "\n",
        "      else:\n",
        "        print('Falha ao carregar planilha.')\n",
        "    else:\n",
        "      print('Falha no Download do arquivo da Planilha de Metadados.')\n",
        "  else:\n",
        "    print('Falha ao encontrar ID do arquivo referente Planilha de Metadados.')\n",
        "else:\n",
        "  print('Falha na organização do ambiente de execução.')"
      ],
      "metadata": {
        "id": "aQTcmmEDQwqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criação de arquivos\n",
        "\n",
        "❗ *Executar apenas se a planilha for carregada adequadamente.*\n",
        "\n",
        "- Foi removido do contexto a palavra idêntica à palavra central.\n",
        "- Considerou que o contexto será construído apenas com os tokens que passarem no min_count."
      ],
      "metadata": {
        "id": "pQisjynHqb1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho até a pasta no Drive na qual será depositada os arquivos gerados por esse programa\n",
        "caminho_skinner = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/SKINNER'\n",
        "\n",
        "# Caminho até os corpus de treinos utilizados (HST, CFH, SAUDE-CORPO, UFSC)\n",
        "caminho_corpus_treinos = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização'\n",
        "\n",
        "# Lista contendo os nomes dos corpus de treinos ['HST-03-10', 'CFH-03-10', 'SAUDE-CORPO-03-10', 'Todas 2003 - 2006')\n",
        "corpus_treinos = [corpus_utilizado for corpus_utilizado in os.listdir(caminho_corpus_treinos) if '.' not in corpus_utilizado and '(' not in corpus_utilizado]\n",
        "\n",
        "for corpus_treino in corpus_treinos: # Iterando sobre os corpus para geração de arquivos referentes aos modelos treinados\n",
        "  print('Corpus de treino atual:',corpus_treino)\n",
        "  caminho_skinner_treino_atual = os.path.join(caminho_skinner,corpus_treino) # Caminho para salvar os arquivos gerados atualizado (caminho skinner + corpus de treino)\n",
        "  os.makedirs(caminho_skinner_treino_atual,exist_ok=True) # Criando, caso não exista, a pasta para armazenar os arquivos dos modelos desse corpus de treino utilizado\n",
        "\n",
        "  caminho_corpus_treino_inc = os.path.join(caminho_corpus_treinos,corpus_treino,'Com RE','Treinamento incremental') # Caminho até os modelos cuja as séries foram constrúidas de forma incremental (com atualização da rede)\n",
        "  caminho_skinner_treino_atual_tipo = os.path.join(caminho_skinner_treino_atual,'Incremental') # Atualizando caminho para salvar arquivos\n",
        "  os.makedirs(caminho_skinner_treino_atual_tipo,exist_ok=True)\n",
        "\n",
        "  # Obtendo os modelos que melhor perfomaram nas analogias com seu modelo base e tiveram as séries temporais construídas\n",
        "  caminhos_top_modelos_inc = sorted([os.path.join(caminho_corpus_treino_inc,top_modelo) for top_modelo in os.listdir(caminho_corpus_treino_inc) if top_modelo.startswith('Modelo')])\n",
        "  print('Tipo de treino: Incremental')\n",
        "  for n,caminho_top_modelo_inc in enumerate(caminhos_top_modelos_inc): # Iterando sobre os top modelos escolhidos para construção das séries temporais com base nos seus resultados nas analogias\n",
        "    print('Modelo',n+1,'de',len(caminhos_top_modelos_inc))\n",
        "    caminho_skinner_treino_atual_tipo_top_modelo = os.path.join(caminho_skinner_treino_atual,'Incremental',os.path.basename(caminho_top_modelo_inc)) # Atualizando caminho para salvar arquivos\n",
        "    os.makedirs(caminho_skinner_treino_atual_tipo_top_modelo,exist_ok=True)\n",
        "\n",
        "    # Obtendo os modelos referentes às séries temporais do Modelo X (top 3 ou top4)\n",
        "    caminhos_modelos_temporais_inc = sorted([os.path.join(caminho_top_modelo_inc,modelo_temporal) for modelo_temporal in os.listdir(caminho_top_modelo_inc) if modelo_temporal.endswith('.model')])\n",
        "    for s,caminho_modelo_temporal in enumerate(caminhos_modelos_temporais_inc): # Iterando sobre as séries temporais\n",
        "      print('Série temporal:',s+1,'de',len(caminhos_modelos_temporais_inc))\n",
        "      modelo = Word2Vec.load(caminho_modelo_temporal) # Carregando o modelo referente à série temporal atual\n",
        "      nome_modelo = os.path.basename(caminho_modelo_temporal).replace('.model','') # Coletando seu nome\n",
        "      window = modelo.window # Coletando a janela utilizada no treinamento\n",
        "      vocab = modelo.wv.index_to_key # Coletando os tokens presentes no vocabulário\n",
        "      print(nome_modelo)\n",
        "\n",
        "      caminho_modelo_atual = os.path.join(caminho_skinner_treino_atual_tipo_top_modelo,nome_modelo) # Atualizando o caminho para salvar arquivos (aqui que será, de fato, upado)\n",
        "      os.makedirs(caminho_modelo_atual,exist_ok=True)\n",
        "\n",
        "      search = re.search(r'(\\d{4})_(\\d{4})',nome_modelo) # Procurando no nome do modelo da série temporal os dígitos de início e final de anos contemplados neste treinamento\n",
        "      data_inicio = search.group(1) # Obtendo o ano inicial\n",
        "      data_fim = search.group(2) # Obtendo o ano final\n",
        "      print('Vocab:',len(vocab),'Data início:',data_inicio,'Data fim:',data_fim) # Printando informações do vocabulário e o intervalo de anos contemplado\n",
        "\n",
        "      # Caminho até as pastas das coleções no corpus de documentos de textos pré-processados\n",
        "      caminho_pasta_colecoes_pp = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Textos_pre_processados/Colecoes_textos_pre_processados'\n",
        "\n",
        "      if corpus_treino == 'CFH-03-10': # Se o corpus de treino for do CFH, a lista de coleções contempladas será:\n",
        "        lista_colecoes = ['Filosofia','Geografia','Geologia','Historia','Psicologia','Teses_e_dissertacoes_do_Centro_de_Filosofia_e_Ciencias_Humanas','Programa_de_Pos_Graduacao_Interdisciplinar_em_Ciencias_Humanas','Servico_Social','Sociologia_e_Ciencia_Politica','Sociologia_Politica','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Ensino_de_Historia_Mestrado_Profissional']\n",
        "\n",
        "      elif corpus_treino == 'HST-03-10': # Se o corpus de treino for do HST, a lista de coleções contempladas será:\n",
        "        lista_colecoes = ['Historia']\n",
        "\n",
        "      elif corpus_treino == 'SAUDE-CORPO-03-10': # Se o corpus de treino for do SAUDE-CORPO, a lista de coleções contempladas será:\n",
        "        lista_colecoes = ['Biologia_Celular_e_do_Desenvolvimento','Biotecnologia_e_Biociencias','Ciencias_da_Reabilitacao','Ciencias_Medicas','Cuidados_Intensivos_e_Paliativos_Mestrado_Profissional',\n",
        "                     'Educacao_Fisica','Enfermagem','Gestao_do_Cuidado_em_Enfermagem','Gestao_do_Cuidado_em_Enfermagem_Mestrado_Profissional','Medicina_Veterinaria_Convencional_e_Integrativa',\n",
        "                     'Neurociencias','Saude_Coletiva','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Saude_Publica','Programa_de_Pos_Graduacao_Multidisciplinar_em_Saude_Mestrado_Profissional']\n",
        "\n",
        "       # Se o corpus de treino for do UFSC, a lista de coleções contempladas será:\n",
        "      elif corpus_treino in ['Todas 2003 - 2006','UFSC 2003 - 2006', 'UFSC-03-06']: # Caso o modelo UFSC troque o nome da pasta de treino para algum destes outros\n",
        "        lista_colecoes = [pasta for pasta in os.listdir(caminho_pasta_colecoes_pp) if '.' not in pasta]\n",
        "\n",
        "      # Obtendo o caminho completo até as pastas das coleções no corpus pré-processado\n",
        "      caminho_colecoes = sorted([os.path.join(caminho_pasta_colecoes_pp,pasta) for pasta in os.listdir(caminho_pasta_colecoes_pp) if '.' not in pasta in lista_colecoes])\n",
        "\n",
        "      qtd_colecoes = len(caminho_colecoes) # Obtendo quantidade de coleções contempladas neste treinamento\n",
        "\n",
        "      for c,caminho_colecao in enumerate(caminho_colecoes): # Iterando sobre cada coleção\n",
        "        print('Coleção:',c+1,'de',qtd_colecoes)\n",
        "\n",
        "        anos_neste_treinamento = [str(ano) for ano in range(int(data_inicio),int(data_fim)+1)] # Criando lista com os anos contemplados neste modelo\n",
        "\n",
        "        # Obtendo caminho até os anos, dentro da pasta da coleção atual, que fizeram parte do treinamento do modelo da série temporal atual\n",
        "        caminho_anos = sorted([os.path.join(caminho_colecao,ano) for ano in os.listdir(caminho_colecao) if ano.isdigit() and ano in anos_neste_treinamento])\n",
        "\n",
        "        colecao = os.path.basename(caminho_colecao) # Obtendo o nome da coleção atual\n",
        "\n",
        "        if not os.path.exists(os.path.join(caminho_modelo_atual,f'dic_{colecao}.msgpack')): # Se não tiver sido salvo um arquivo para esta coleção no diretório que era para salvá-lo, vamos então construí-lo.\n",
        "          dic_info_colecao = {} # Inicialização do dicionário que irá armazenar as informações de contextos referentes à coleção atual\n",
        "\n",
        "          for caminho_ano in caminho_anos: # Iterando sobre as pastas dos anos dessa coleção que participaram do treinamento do modelo da série temporal atual\n",
        "            # Obtendo os trabalhos no ano atual\n",
        "            trabalhos = sorted([trabalho for trabalho in os.listdir(caminho_ano) if trabalho.startswith('Trabalho')], key=lambda x: int(x.split()[1]))\n",
        "            # Obtendo os caminhos completos até as pastas dos trabalhos do ano atual\n",
        "            caminho_trabalhos = [os.path.join(caminho_ano,trabalho) for trabalho in trabalhos]\n",
        "\n",
        "            for caminho_trabalho in caminho_trabalhos: # Iterando sobre as pastas dos trabalhos do ano atual\n",
        "              caminho_arquivo_pp = os.path.join(caminho_trabalho,'pre_processamento_c_re.msgpack') # Caminho até o arquivo de texto pré-processado para o trabalho atual\n",
        "              trabalho = os.path.basename(caminho_trabalho) # Obtendo nome do trabalho\n",
        "\n",
        "              if os.path.exists(caminho_arquivo_pp): # Verificando se o arquivo de pré-processamento para este trabalho realmente existe\n",
        "                # Criando chave no dicionário de contexto referente ao trabalho atual. Esta chave terá como valor um dicionário que conterá duas chaves: uma referente aos metadados que já será preenchida e outra aos tokens centrais/alvos que será preenchida ao decorrer da execução abaixo\n",
        "                dic_info_colecao[trabalho] = {'metadados':coletarInfoPlanilha(wb=wb,nome_colecao=colecao,nome_trabalho=trabalho),\n",
        "                                              'tokens_centrais':{}}\n",
        "                try:\n",
        "                  for frase in abrirArquivoMsgPack(caminho_arquivo_pp): # Iterando sobre cada frase presente no arquivo de texto pré-processado do trabalho atual\n",
        "                    frase = [token for token in frase if token in vocab] # Filtrando a frase para apenas conter os tokens presentes no vocabulário do modelo ( https://stackoverflow.com/questions/50723303/how-is-word2vec-min-count-applied )\n",
        "                    for indice_token,token in enumerate(frase): # Iterando sobre cada token presente na frase, identificando também seu índice\n",
        "                      if token not in dic_info_colecao[trabalho]['tokens_centrais'].keys(): # Se o token central que está sendo analisado agora ainda não for uma chave para o dicionário de tokens_centrais do trabalho atual\n",
        "                        dic_info_colecao[trabalho]['tokens_centrais'][token] = {} # Criaremos a chave com este token e atribuiremos o valor de um dicionário vazio (que será preenchido com os tokens de contexto e suas ocorrências nas frases em que estão dentro da janela de contexto para o token central atual)\n",
        "                      # Atualiza-se o dicionário de contextos do trabalho atual, na chave referente ao contexto do token central atualmente analisado\n",
        "                      dic_info_colecao[trabalho]['tokens_centrais'][token] = extracaoTokensContexto(frase=frase,token=token,i_token=indice_token,window=window,dic_contexto=dic_info_colecao[trabalho]['tokens_centrais'][token])\n",
        "\n",
        "                except Exception as e:\n",
        "                  print(e.__class__.__name__,str(e))\n",
        "                  pass\n",
        "                else:\n",
        "                  for token in dic_info_colecao[trabalho]['tokens_centrais'].keys(): # Iterando sobre cada token central identificado no trabalho\n",
        "                    # O dicionário de contexto do token central atual é ordenado para que os tokens de maior ocorrência fiquem em primeiro\n",
        "                    dic_info_colecao[trabalho]['tokens_centrais'][token] = dict(sorted(dic_info_colecao[trabalho]['tokens_centrais'][token].items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "          if dic_info_colecao.keys():\n",
        "            # Salva a variável referente ao dicionário de contextos da coleção atual nas pastas referentes ao treinamento da série temporal atual\n",
        "            salvarResultadosEmMsgPack(nome_variavel=f'dic_{colecao}.msgpack',variavel_em_questao=dic_info_colecao,pasta_para_salvar=caminho_modelo_atual)\n",
        "        else:\n",
        "          print('Dados desta coleção já armazenados!')\n",
        "\n",
        "  # Realiza-se o mesmo processo, mas agora contemplando os modelos em que as séries temporais foram construídas de maneira \"temporal\"\n",
        "  caminho_corpus_treino_tmp = os.path.join(caminho_corpus_treinos,corpus_treino,'Com RE','Treinamento temporal')\n",
        "  caminho_skinner_treino_atual_tipo = os.path.join(caminho_skinner_treino_atual,'Temporal')\n",
        "  os.makedirs(caminho_skinner_treino_atual_tipo,exist_ok=True)\n",
        "\n",
        "  caminhos_top_modelos_tmp = sorted([os.path.join(caminho_corpus_treino_tmp,top_modelo) for top_modelo in os.listdir(caminho_corpus_treino_tmp) if top_modelo.startswith('Modelo')])\n",
        "  print('Tipo de treino: Temporal')\n",
        "  for n,caminho_top_modelo_tmp in enumerate(caminhos_top_modelos_tmp):\n",
        "    print('Modelo',n+1,'de',len(caminhos_top_modelos_tmp))\n",
        "    caminho_skinner_treino_atual_tipo_top_modelo = os.path.join(caminho_skinner_treino_atual,'Temporal',os.path.basename(caminho_top_modelo_tmp))\n",
        "    os.makedirs(caminho_skinner_treino_atual_tipo_top_modelo,exist_ok=True)\n",
        "\n",
        "    caminhos_modelos_temporais_tmp = sorted([os.path.join(caminho_top_modelo_tmp,modelo_temporal) for modelo_temporal in os.listdir(caminho_top_modelo_tmp) if modelo_temporal.endswith('.model')])\n",
        "    for s,caminho_modelo_temporal in enumerate(caminhos_modelos_temporais_tmp):\n",
        "      print('Série temporal:',s+1,'de',len(caminhos_modelos_temporais_tmp))\n",
        "      modelo = Word2Vec.load(caminho_modelo_temporal)\n",
        "      nome_modelo = os.path.basename(caminho_modelo_temporal).replace('.model','')\n",
        "      window = modelo.window\n",
        "      vocab = modelo.wv.index_to_key\n",
        "      print(nome_modelo)\n",
        "\n",
        "      caminho_modelo_atual = os.path.join(caminho_skinner_treino_atual_tipo_top_modelo,nome_modelo)\n",
        "      os.makedirs(caminho_modelo_atual,exist_ok=True)\n",
        "\n",
        "      search = re.search(r'(\\d{4})_(\\d{4})',nome_modelo)\n",
        "      data_inicio = search.group(1)\n",
        "      data_fim = search.group(2)\n",
        "      print('Vocab:',len(vocab),'Data início:',data_inicio,'Data fim:',data_fim)\n",
        "\n",
        "      caminho_pasta_colecoes_pp = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Textos_pre_processados/Colecoes_textos_pre_processados'\n",
        "\n",
        "      if corpus_treino == 'CFH-03-10':\n",
        "        lista_colecoes = ['Filosofia','Geografia','Geologia','Historia','Psicologia','Teses_e_dissertacoes_do_Centro_de_Filosofia_e_Ciencias_Humanas','Programa_de_Pos_Graduacao_Interdisciplinar_em_Ciencias_Humanas','Servico_Social','Sociologia_e_Ciencia_Politica','Sociologia_Politica','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Ensino_de_Historia_Mestrado_Profissional']\n",
        "\n",
        "      elif corpus_treino == 'HST-03-10':\n",
        "        lista_colecoes = ['Historia']\n",
        "\n",
        "      elif corpus_treino == 'SAUDE-CORPO-03-10':\n",
        "        lista_colecoes = ['Biologia_Celular_e_do_Desenvolvimento','Biotecnologia_e_Biociencias','Ciencias_da_Reabilitacao','Ciencias_Medicas','Cuidados_Intensivos_e_Paliativos_Mestrado_Profissional',\n",
        "                     'Educacao_Fisica','Enfermagem','Gestao_do_Cuidado_em_Enfermagem','Gestao_do_Cuidado_em_Enfermagem_Mestrado_Profissional','Medicina_Veterinaria_Convencional_e_Integrativa',\n",
        "                     'Neurociencias','Saude_Coletiva','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Saude_Publica','Programa_de_Pos_Graduacao_Multidisciplinar_em_Saude_Mestrado_Profissional']\n",
        "\n",
        "      elif corpus_treino in ['Todas 2003 - 2006','UFSC 2003 - 2006','UFSC-03-06']: # Caso o modelo UFSC troque o nome da pasta de treino para algum destes outros\n",
        "        lista_colecoes = [pasta for pasta in os.listdir(caminho_pasta_colecoes_pp) if '.' not in pasta]\n",
        "\n",
        "      caminho_colecoes = sorted([os.path.join(caminho_pasta_colecoes_pp,pasta) for pasta in os.listdir(caminho_pasta_colecoes_pp) if '.' not in pasta in lista_colecoes])\n",
        "\n",
        "      qtd_colecoes = len(caminho_colecoes)\n",
        "\n",
        "      for c,caminho_colecao in enumerate(caminho_colecoes):\n",
        "        print('Coleção:',c+1,'de',qtd_colecoes)\n",
        "\n",
        "        anos_neste_treinamento = [str(ano) for ano in range(int(data_inicio),int(data_fim)+1)]\n",
        "\n",
        "        caminho_anos = sorted([os.path.join(caminho_colecao,ano) for ano in os.listdir(caminho_colecao) if ano.isdigit() and ano in anos_neste_treinamento])\n",
        "        colecao = os.path.basename(caminho_colecao)\n",
        "\n",
        "        if not os.path.exists(os.path.join(caminho_modelo_atual,f'dic_{colecao}.msgpack')):\n",
        "          dic_info_colecao = {}\n",
        "\n",
        "          for caminho_ano in caminho_anos:\n",
        "            trabalhos = sorted([trabalho for trabalho in os.listdir(caminho_ano) if trabalho.startswith('Trabalho')], key=lambda x: int(x.split()[1]))\n",
        "            caminho_trabalhos = [os.path.join(caminho_ano,trabalho) for trabalho in trabalhos]\n",
        "\n",
        "            for caminho_trabalho in caminho_trabalhos:\n",
        "              caminho_arquivo_pp = os.path.join(caminho_trabalho,'pre_processamento_c_re.msgpack')\n",
        "              trabalho = os.path.basename(caminho_trabalho)\n",
        "\n",
        "              if os.path.exists(caminho_arquivo_pp):\n",
        "                dic_info_colecao[trabalho] = {'metadados':coletarInfoPlanilha(wb=wb,nome_colecao=colecao,nome_trabalho=trabalho),\n",
        "                                              'tokens_centrais':{}}\n",
        "                try:\n",
        "                  for frase in abrirArquivoMsgPack(caminho_arquivo_pp):\n",
        "                    frase = [token for token in frase if token in vocab]\n",
        "                    for indice_token,token in enumerate(frase):\n",
        "                      if token not in dic_info_colecao[trabalho]['tokens_centrais'].keys():\n",
        "                        dic_info_colecao[trabalho]['tokens_centrais'][token] = {}\n",
        "                      dic_info_colecao[trabalho]['tokens_centrais'][token] = extracaoTokensContexto(frase=frase,token=token,i_token=indice_token,window=window,dic_contexto=dic_info_colecao[trabalho]['tokens_centrais'][token])\n",
        "\n",
        "                except Exception as e:\n",
        "                  print(e.__class__.__name__,str(e))\n",
        "                  pass\n",
        "                else:\n",
        "                  for token in dic_info_colecao[trabalho]['tokens_centrais'].keys():\n",
        "                    dic_info_colecao[trabalho]['tokens_centrais'][token] = dict(sorted(dic_info_colecao[trabalho]['tokens_centrais'][token].items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "          if dic_info_colecao.keys():\n",
        "            salvarResultadosEmMsgPack(nome_variavel=f'dic_{colecao}.msgpack',variavel_em_questao=dic_info_colecao,pasta_para_salvar=caminho_modelo_atual)\n",
        "        else:\n",
        "          print('Dados desta coleção já armazenados!')"
      ],
      "metadata": {
        "id": "QeX7cT2Zoekw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}