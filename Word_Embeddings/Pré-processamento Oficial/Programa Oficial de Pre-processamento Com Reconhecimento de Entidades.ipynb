{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1EF39X2AUOgcp1y8b4LUUVTc9gFYW_rQ6","timestamp":1713118626517}],"collapsed_sections":["ljzqD_RB4MDf","9gNb_CUr52h1","XkQdalFvtVLN","Ii0vahN5H4AH","j2IlDAJsGmw6"],"authorship_tag":"ABX9TyOx0Fwq+kQAIE0KfP2oCpux"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Sincronização com Google Drive"],"metadata":{"id":"ljzqD_RB4MDf"}},{"cell_type":"code","source":["import os, string, time\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from google.colab import output\n","\n","def atualizarConsole(nova_string : str) -> None:\n","  output.clear()\n","  print(nova_string)"],"metadata":{"id":"TzKr1KUP4HyR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713137094535,"user_tz":180,"elapsed":2515,"user":{"displayName":"Igor Caetano","userId":"08005013188852644934"}},"outputId":"ec5f9f3f-0b33-4536-daac-37dbd764b3fb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["\n","# Instalação e importação de bibliotecas que serão utilizadas"],"metadata":{"id":"Ztkfx0e42SzB"}},{"cell_type":"code","source":["string_exibida = 'Importando string, os, re...\\n'\n","atualizarConsole(string_exibida)\n","import string\n","import os\n","import re\n","# import msgpack\n","string_exibida = '''Importando string: OK\n","Importando OS: OK\n","Importando re: OK\n","Instalando e importando pt-br-verbs-lemmatizer...\\n'''\n","atualizarConsole(string_exibida)\n","try:\n","  !pip install pt-br-verbs-lemmatizer -U\n","  from pt_br_verbs_lemmatizer import lemmatize as lematizar, modifyDicAmbiguousVerbs as modificarAmbiguidadeVerbal\n","except Exception as e:\n","  print(f'Problema na instalação e importação de pt-br-verbs-lemmatizer:\\n{e.__class__.__name__}: {str(e)}')\n","else:\n","  string_exibida = '''Importando string: OK\n","Importando OS: OK\n","Importando re: OK\n","Instalando e importando pt-br-verbs-lemmatizer: OK\n","Instalando e importando ferramentas-basicas-pln...\\n'''\n","  atualizarConsole(string_exibida)\n","  try:\n","    !pip install ferramentas-basicas-pln -U\n","    from ferramentas_basicas_pln import formatarTexto,removerCaracteresEspeciais,removerCaracteresEstranhos,removerEspacosEmBrancoExtras,transformarTextoSubstituindoCaracteres,coletarTextoDeArquivoTxt\n","    from ferramentas_basicas_pln import STRING_CARACTERES_ESPECIAIS_PADRAO\n","  except Exception as e:\n","    print(f'Problema na instalação e importação de ferramentas:\\n{e.__class__.__name__}: {str(e)}')\n","  else:\n","    string_exibida = '''Importando string: OK\n","Importando OS: OK\n","Importando re: OK\n","Instalando e importando pt-br-verbs-lemmatizer: OK\n","Instalando e importando ferramentas-basicas-pln: OK\n","Importando NLTK e fazendo download de módulos...\\n'''\n","    atualizarConsole(string_exibida)\n","    try:\n","      import nltk\n","      from nltk.tokenize import sent_tokenize, word_tokenize\n","      nltk.download('punkt')\n","      nltk.download('stopwords')\n","    except Exception as e:\n","      print(f'Problema na instalação e importação ou download de módulos de NLTK:\\n{e.__class__.__name__}: {str(e)}')\n","    else:\n","      string_exibida = '''Importando string: OK\n","Importando OS: OK\n","Importando re: OK\n","Instalando e importando pt-br-verbs-lemmatizer: OK\n","Instalando e importando ferramentas-basicas-pln: OK\n","Importando NLTK e fazendo download de módulos: OK\n","Instalando e importando spaCy e fazendo download de módulos...\\n'''\n","      atualizarConsole(string_exibida)\n","      try:\n","        !pip install -U spacy\n","        !pip install -U spacy-lookups-data\n","        !python -m spacy download pt_core_news_lg\n","\n","        import spacy\n","        # nlp_spacy = spacy.load('pt_core_news_lg')\n","        nlp_spacy = spacy.load('pt_core_news_lg',enable=[\"tok2vec\",\"ner\",\"morphologizer\"])\n","        # nlp_spacy.enable_pipe(\"senter\")\n","      except Exception as e:\n","        print(f'Problema na instalação e importação ou download de módulos de spaCy:\\n{e.__class__.__name__}: {str(e)}')\n","      else:\n","        string_exibida = '''Importando string: OK\n","Importando OS: OK\n","Importando re: OK\n","Instalando e importando pt-br-verbs-lemmatizer: OK\n","Instalando e importando ferramentas-basicas-pln: OK\n","Importando NLTK e fazendo download de módulos: OK\n","Instalando e importando spaCy e fazendo download de módulos: OK\n","'''+'\\n'+'-'*100+'\\n'+'Ambiente configurado com sucesso.'+'\\n'+'-'*100+'\\n'\n","        atualizarConsole(string_exibida)"],"metadata":{"id":"LOuCjKzl2Vi9","executionInfo":{"status":"ok","timestamp":1713137188045,"user_tz":180,"elapsed":93513,"user":{"displayName":"Igor Caetano","userId":"08005013188852644934"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5bb3d793-28c0-4a30-8cbd-f855da4eee33"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Importando string: OK\n","Importando OS: OK\n","Importando re: OK\n","Importando msgpack: OK\n","Instalando e importando pt-br-verbs-lemmatizer: OK\n","Instalando e importando ferramentas-basicas-pln: OK\n","Importando NLTK e fazendo download de módulos: OK\n","Instalando e importando spaCy e fazendo download de módulos: OK\n","\n","----------------------------------------------------------------------------------------------------\n","Ambiente configurado com sucesso.\n","----------------------------------------------------------------------------------------------------\n","\n"]}]},{"cell_type":"markdown","source":["# Lista de entidades nomeadas personalizada"],"metadata":{"id":"9gNb_CUr52h1"}},{"cell_type":"code","source":["string_entidades_manuais = '''História cultural, história global, história social, história social da cultura, histórias entrelaçadas, histórias entrecruzadas, história conectada, história transnacional, filosofia da história, teoria da história, teoria e filosofia da história, teoria marxista, teoria queer, sistema mundo, racismo estrutural, história da historiografia, teoria do sistema-mundo, história ambiental, história das mulheres, história indígena, história global do trabalho, história do trabalho, história oral, estudos de gênero, História militar, história das religiões, história da arte, história econômica, Guerra do Contestado, Movimento Social do Contestado, Teoria Racial, Historiografia Literária'''\n","\n","string_entidades_manuais = removerCaracteresEstranhos(string_entidades_manuais)#.replace('\\xa0',' ').replace('\\n','')\n","\n","lista_entidades_nomeadas_manuais = [elemento.lower().strip() for elemento in string_entidades_manuais.split(',')]\n","\n","lista_entidades_nomeadas_manuais_ordenada = sorted(lista_entidades_nomeadas_manuais, key=len, reverse=True)\n","\n","lista_entidades_nomeadas_manuais_ordenada"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKiGQvpr53rQ","executionInfo":{"status":"ok","timestamp":1713137188046,"user_tz":180,"elapsed":5,"user":{"displayName":"Igor Caetano","userId":"08005013188852644934"}},"outputId":"1fb4a2ea-960b-4577-8336-862ab1bf2daa"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['teoria e filosofia da história',\n"," 'movimento social do contestado',\n"," 'história global do trabalho',\n"," 'história social da cultura',\n"," 'história da historiografia',\n"," 'historiografia literária',\n"," 'histórias entrecruzadas',\n"," 'teoria do sistema-mundo',\n"," 'histórias entrelaçadas',\n"," 'história transnacional',\n"," 'história das religiões',\n"," 'filosofia da história',\n"," 'história das mulheres',\n"," 'história do trabalho',\n"," 'guerra do contestado',\n"," 'história conectada',\n"," 'teoria da história',\n"," 'racismo estrutural',\n"," 'história ambiental',\n"," 'história econômica',\n"," 'história cultural',\n"," 'história indígena',\n"," 'estudos de gênero',\n"," 'história militar',\n"," 'história da arte',\n"," 'história global',\n"," 'história social',\n"," 'teoria marxista',\n"," 'sistema mundo',\n"," 'história oral',\n"," 'teoria racial',\n"," 'teoria queer']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# Lista de stopwords a ser utilizada"],"metadata":{"id":"XkQdalFvtVLN"}},{"cell_type":"code","source":["stopwords = '''#Artigos\n","o\n","os\n","a\n","as\n","uns\n","umas\n","#Preposições\n","à\n","às\n","aos\n","da\n","das\n","do\n","dos\n","no\n","nos\n","na\n","nas\n","numa\n","numas\n","num\n","nuns\n","dessa\n","dessas\n","desse\n","desses\n","desta\n","destas\n","deste\n","destes\n","ante\n","até\n","após\n","com\n","contra\n","de\n","desde\n","em\n","entre\n","para\n","perante\n","por\n","sem\n","sob\n","sobre\n","trás\n","conforme\n","consoante\n","mediante\n","tirante\n","senão\n","#Pronomes\n","eu\n","tu\n","ele\n","eles\n","ela\n","elas\n","nós\n","vós\n","me\n","te\n","lhe\n","lhes\n","se\n","nos\n","vos\n","mim\n","contigo\n","ti\n","contigo\n","si\n","consigo\n","conosco\n","convosco\n","#Conjunções\n","mas\n","e\n","também\n","só\n","todavia\n","ou\n","-seja\n","portanto\n","logo\n","porque\n","que\n","assim\n","já\n","embora\n","ainda\n","conforme\n","depois\n","antes\n","afim\n","quanto\n","quantos\n","quanta\n","quantas\n","#Extras\n","pela\n","pelas\n","pelo\n","pelos\n","qual\n","quais\n","quando\n","quem\n","aquele\n","àquele\n","aqueles\n","àqueles\n","aquela\n","àquela\n","aquelas\n","àquelas\n","aquilo\n","àquilo\n","naquele\n","naqueles\n","naquela\n","naquelas\n","naquilo'''\n","\n","lista_stopwords = [palavra.strip() for palavra in stopwords.split('\\n') if not (palavra.startswith('#') or palavra.startswith('-'))]\n","print('Lista de stopwords construída sem filtro de repetição, tamanho:',len(lista_stopwords))\n","print('-'*100)\n","print('Palavras repetidas encontradas:')\n","for p in set([elemento for elemento in lista_stopwords if lista_stopwords.count(elemento) > 1]):\n","  print(p)\n","print('-'*100)\n","print('Aplicando filtro...')\n","print('-'*100)\n","lista_stopwords = sorted(set([palavra.strip() for palavra in stopwords.split('\\n') if not (palavra.startswith('#') or palavra.startswith('-'))]),key=len)\n","print('Lista de stopwords construída com filtro de repetição, tamanho atualizado:',len(lista_stopwords))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y760YgahtU1t","executionInfo":{"status":"ok","timestamp":1713137188046,"user_tz":180,"elapsed":4,"user":{"displayName":"Igor Caetano","userId":"08005013188852644934"}},"outputId":"f81c7645-77e4-43ef-a811-eafdf02ca519"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Lista de stopwords construída sem filtro de repetição, tamanho: 118\n","----------------------------------------------------------------------------------------------------\n","Palavras repetidas encontradas:\n","conforme\n","nos\n","contigo\n","----------------------------------------------------------------------------------------------------\n","Aplicando filtro...\n","----------------------------------------------------------------------------------------------------\n","Lista de stopwords construída com filtro de repetição, tamanho atualizado: 115\n"]}]},{"cell_type":"markdown","source":["# Modificação da lematização baseada em regras (removendo ambiguidades/duplicidades)"],"metadata":{"id":"lL4t7BxjvGcL"}},{"cell_type":"code","source":["print('Modificando dataset ambiguidades verbais na lematização\\nSeleção Mateus...')\n","modificarAmbiguidadeVerbal(type='infinitive',verbs=['impostar','incender','interditar',\n","                                                    'interver','irar','iriar','fossar',\n","                                                    'liar','mantar','mentar','mentirar',\n","                                                    'podar','possar','preditar',\n","                                                    'postar','querar','querendar','raer',\n","                                                    'recobrar','redizer','refugir','remedar',\n","                                                    'repelar','revir','rer','ruar','segar',\n","                                                    'erar','seriar','suster'])\n","\n","modificarAmbiguidadeVerbal(type='flex',verbs=[('relemos','relar'),('relemo-lo','relar'),\n","                                              ('retemos','retar'),('retemo-lo','retar'),('retemo-nos','retar'),\n","                                              ('revira','rever'),('reviram','rever'),('reviras','rever'),('revirem','rever'),('revires','rever'),('revira-o','rever'),('revira-lo','rever'),\n","                                              ('reviste','rever'),('revisto','rever'),('revistes','rever'),('reviste-o','rever'),('revisto-a','rever'),('revisto-o','rever'),('reviste-te','rever'),('revisto-as','rever'),('revisto-me','rever'),('revisto-os','rever'),('revisto-se','rever'),('revisto-te','rever'),\n","                                              ('revisto-o','rever'),('revisto-me','rever'),\n","                                              ('ruido-a','ruidar'),('ruido-o','ruidar'),('ruido-as','ruidar'),('ruido-me','ruidar'),('ruido-os','ruidar'),('ruido-se','ruidar'),('ruido-te','ruidar'),\n","                                              ('sede','ser'),('sede-o','ser'),\n","                                              ('surta','sortir'),('surte','sortir'),('surto','sortir'),('surtam','sortir'),('surtas','sortir'),('surtem','sortir'),('surtes','sortir'),('surta-o','sortir'),('surtais','sortir'),('surte-o','sortir'),('surto-o','sortir'),('surta-se','sortir'),('surtamos','sortir'),('surte-se','sortir'),('surte-te','sortir'),('surto-me','sortir'),('surtam-no','sortir'),('surtam-se','sortir'),('surtem-no','sortir'),('surtem-se','sortir'),('surtamo-nos','sortir'),\n","                                              ('some-o','sumir'),('some-se','sumir'),('some-te','sumir'),('somem-no','sumir'),('somem-se','sumir')])\n","print('Feito')\n","\n","print('Modificando dataset ambiguidades verbais na lematização\\nSeleção Carlos...')\n","modificarAmbiguidadeVerbal(type='infinitive',verbs=['valar','vestar','vinhar','abolar',\n","                                                    'açalmar','alardar','amandar','amolecar',\n","                                                    'apresar','arrefeçar','beberar','bombardar'])\n","\n","modificarAmbiguidadeVerbal(type='flex',verbs=[('traga','trager'),('trago','trager'),('tragam','trager'),('tragas','trager'),('traga-o','trager'),('tragais','trager'),('trago-o','trager'),('tragamos','trager'),('tragam-no','trager'),\n","                                              ('abraque','abraçar'),('abracado','abraçar'),('abracará','abraçar'),('abrace-a','abraçar'),('abrace-o','abraçar'),('abraceis','abraçar'),('abraquei','abraçar'),('abraquem','abraçar'),('abraques','abraçar'),('abracai-a','abraçar'),('abracai-o','abraçar'),('abracarás','abraçar'),('abracarão','abraçar'),\n","                                              ('abracente','abraçar'),('abraque-o','abraçar'),('abraqueis','abraçar'),('abracado-a','abraçar'),('abracado-o','abraçar'),('abracai-as','abraçar'),('abracai-os','abraçar'),\n","                                              ('abracáveis','abraçar'),('abraquei-o','abraçar'),('abraquemos','abraçar'),('abracado-as','abraçar'),('abracado-la','abraçar'),('abracado-lo','abraçar'),('abracado-me','abraçar'),('abracado-na','abraçar'),('abracado-no','abraçar'),('abracado-os','abraçar'),('abracado-se','abraçar'),('abracado-te','abraçar'),('abracai-lhe','abraçar'),('abracai-vos','abraçar'),('abracaríeis','abraçar'),('abracei-lhe','abraçar'),('abracem-lhe','abraçar'),('abracem-nas','abraçar'),('abracem-nos','abraçar'),('abracemô-la','abraçar'),('abracemô-lo','abraçar'),('abracá-lo-á','abraçar'),('abracáramos','abraçar'),('abracásseis','abraçar'),('abracávamos','abraçar'),('abraquem-no','abraçar'),('abracado-las','abraçar'),('abracado-lhe','abraçar'),('abracado-los','abraçar'),('abracado-nas','abraçar'),('abracado-nos','abraçar'),('abracai-lhes','abraçar'),('abracando-la','abraçar'),('abracando-lo','abraçar'),('abracaríamos','abraçar'),\n","                                              ('abracá-lo-ei','abraçar'),('abracá-lo-ia','abraçar'),('abracá-lo-ás','abraçar'),('abracá-lo-ão','abraçar'),('abracárei-lo','abraçar'),('abracássemos','abraçar'),('abracávei-lo','abraçar'),('abraquemo-lo','abraçar'),('abracado-lhes','abraçar'),('abracando-las','abraçar'),('abracando-los','abraçar'),('abracemo-lhes','abraçar'),('abracá-lo-eis','abraçar'),('abracá-lo-iam','abraçar'),('abracá-lo-ias','abraçar'),('abracáramo-lo','abraçar'),('abracávamo-lo','abraçar'),('abracá-lo-emos','abraçar'),('abracá-lo-íeis','abraçar'),('abracá-lo-íamos','abraçar'),\n","                                              ('acaba-a','acaber'),('acaba-o','acaber'),('acabais','acaber'),('acabe-a','acaber'),('acabe-o','acaber'),('acabeis','acaber'),('acabo-a','acaber'),('acabo-o','acaber'),('acaba-as','acaber'),('acaba-me','acaber'),('acaba-os','acaber'),('acaba-se','acaber'),('acaba-te','acaber'),\n","                                              ('acabe-me','acaber'),('acabe-os','acaber'),('acabe-se','acaber'),('acabe-te','acaber'),('acabei-a','acaber'),('acabei-o','acaber'),\n","                                              ('acabo-as','acaber'),('acabo-me','acaber'),('acabo-os','acaber'),('acabo-se','acaber'),('acabo-te','acaber'),('acabam-me','acaber'),('acabam-na','acaber'),('acabam-no','acaber'),('acabam-se','acaber'),('acabam-te','acaber'),('acabei-as','acaber'),('acabei-os','acaber'),('acabem-me','acaber'),('acabem-na','acaber'),('acabem-no','acaber'),('acabem-se','acaber'),('acabem-te','acaber'),('acabam-lhe','acaber'),('acabam-nas','acaber'),('acabam-nos','acaber'),('acabamô-la','acaber'),('acabamô-lo','acaber'),('acabei-lhe','acaber'),('acabem-lhe','acaber'),('acabem-nas','acaber'),('acabem-nos','acaber'),('acabemô-la','acaber'),('acabemô-lo','acaber'),('acabam-lhes','acaber'),('acabamo-lhe','acaber'),('acabamo-nas','acaber'),('acabamo-nos','acaber'),('acabamô-las','acaber'),('acabamô-los','acaber'),('acabei-lhes','acaber'),('acabem-lhes','acaber'),('acabemo-lhe','acaber'),('acabemo-nas','acaber'),('acabemo-nos','acaber'),('acabemô-las','acaber'),('acabemô-los','acaber'),('acabamo-lhes','acaber'),('acabemo-lhes','acaber'),\n","                                              ('achara-a','acharar'),('achara-o','acharar'),('achara-as','acharar'),('achara-os','acharar'),\n","                                              ('acoitado-me','açoitar'),('acoitáveis','açoitar'),\n","                                              ('adia','adir'),('adiam','adir'),('adias','adir'),('adia-a','adir'),('adia-o','adir'),('adia-as','adir'),('adia-me','adir'),('adia-os','adir'),('adia-se','adir'),('adia-te','adir'),('adiá-la','adir'),('adiá-lo','adir'),('adia-lhe','adir'),('adiam-me','adir'),('adiam-na','adir'),('adiam-no','adir'),('adiam-se','adir'),('adiam-te','adir'),('adias-me','adir'),('adias-se','adir'),('adias-te','adir'),('adiá-las','adir'),('adiá-los','adir'),('adia-lhes','adir'),('adiam-lhe','adir'),('adiam-nas','adir'),('adiam-nos','adir'),('adiam-lhes','adir'),\n","                                              ('amente','amar'),\n","                                              ('assente','assar'),('atente','atar'),\n","                                              ('atendam-se','atendar'),('atendam-te','atendar'),('atendei-as','atendar'),('atendei-os','atendar'),('atendem-se','atendar'),('atendem-te','atendar'),('atendam-lhe','atendar'),('atendam-nas','atendar'),('atendam-nos','atendar'),('atendamô-la','atendar'),('atendamô-lo','atendar'),('atendei-lhe','atendar'),('atendem-lhe','atendar'),('atendem-nas','atendar'),('atendem-nos','atendar'),('atendemô-la','atendar'),('atendemô-lo','atendar'),('atendam-lhes','atendar'),('atendamo-lhe','atendar'),('atendamo-nas','atendar'),('atendamo-nos','atendar'),('atendamô-las','atendar'),('atendamô-los','atendar'),('atendei-lhes','atendar'),('atendem-lhes','atendar'),('atendemo-lhe','atendar'),('atendemo-nas','atendar'),('atendemo-nos','atendar'),('atendemô-las','atendar'),('atendemô-los','atendar'),('atendamo-lhes','atendar'),('atendemo-lhes','atendar'),\n","                                              ('avivente','avivar'),\n","                                              ('calque','calçar'),('calquei','calçar'),('calquem','calçar'),('calques','calçar'),('calquei-o','calçar'),('calquemos','calçar')])\n","print('Feito')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YhjshlcOvNHm","executionInfo":{"status":"ok","timestamp":1713137344903,"user_tz":180,"elapsed":310,"user":{"displayName":"Igor Caetano","userId":"08005013188852644934"}},"outputId":"682ab3bb-4aa7-40b7-b480-158fa965fe6d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Modificando dataset ambiguidades verbais na lematização\n","Seleção Mateus...\n","Feito\n","Modificando dataset ambiguidades verbais na lematização\n","Seleção Carlos...\n","Feito\n"]}]},{"cell_type":"markdown","source":["# Funções"],"metadata":{"id":"Ii0vahN5H4AH"}},{"cell_type":"code","source":["def padronizacaoTexto(texto : str) -> str:\n","  texto = texto.replace('\\n',' ')\n","  return formatarTexto(texto=texto,\n","                       padronizar_texto_para_minuscula=False,\n","                       remover_caracteres_especiais=False,\n","                       remover_caracteres_estranhos=True,\n","                       remover_espacos_em_branco_em_excesso=True,\n","                       padronizar_links=True,padrao_link='link_url_mask',\n","                       padronizar_dinheiros=True,padrao_dinheiro='R$',\n","                       padronizar_emails=True,padrao_email='e_mail_mask')\n","\n","def padronizacaoTokens(lista_frases_tokenizadas : list[str],\n","                       lista_tokens_personalizados : list[str] = []) -> str:\n","  lista_frases_tokenizadas_padronizada = []\n","  lista_tokens_personalizados += ['link_url_mask','R$','e_mail_mask']\n","\n","  for frase in lista_frases_tokenizadas:\n","    lista_frases_tokenizadas_padronizada_atual = []\n","    for token in frase:\n","      # if len(token.split()) > 1: # spaCy lematizando no = 'em o'\n","      #   for t in token.split():\n","      #     if (t not in STRING_CARACTERES_ESPECIAIS_PADRAO) and (t.lower() not in lista_stopwords):\n","      #       if t not in lista_tokens_personalizados:\n","      #         t = formatarTexto(texto=t,\n","      #                           remover_caracteres_especiais=True,\n","      #                           remover_caracteres_estranhos=False,\n","      #                           remover_espacos_em_branco_em_excesso=False,\n","      #                           padronizar_texto_para_minuscula=True).strip()\n","      #       else:\n","      #         t = t.strip()\n","\n","      #       if t:\n","      #         lista_frases_tokenizadas_padronizada_atual.append(t)\n","      # else:\n","      if (token not in STRING_CARACTERES_ESPECIAIS_PADRAO) and (token.lower() not in lista_stopwords):\n","        if token not in lista_tokens_personalizados:\n","          token = formatarTexto(texto=token,\n","                                remover_caracteres_especiais=True,\n","                                remover_caracteres_estranhos=False,\n","                                remover_espacos_em_branco_em_excesso=False,\n","                                padronizar_texto_para_minuscula=True).strip()\n","        else:\n","          token = token.strip()\n","\n","        if token:\n","          lista_frases_tokenizadas_padronizada_atual.append(token)\n","\n","\n","    if lista_frases_tokenizadas_padronizada_atual:\n","      lista_frases_tokenizadas_padronizada.append(lista_frases_tokenizadas_padronizada_atual)\n","\n","  if lista_frases_tokenizadas_padronizada:\n","    return lista_frases_tokenizadas_padronizada\n","\n","def substituirEntidadeNomeadaManual(texto : str,\n","                                    entidade_normal : str,\n","                                    entidade_formatada : str) -> str:\n","  return re.sub(r'{x}'.format(x=entidade_normal), entidade_formatada, texto, flags=re.IGNORECASE)\n","\n","\n","def salvarArquivoMsgPack(nome_variavel : str,\n","                         variavel_em_questao,\n","                         pasta_para_salvar : str = '') -> tuple[bool,str]:\n","    try:\n","        if not os.path.exists(pasta_para_salvar):\n","            os.makedirs(pasta_para_salvar)\n","\n","        variable_bytes = msgpack.packb(variavel_em_questao)\n","\n","        if not nome_variavel.endswith('.msgpack'):\n","            nome_variavel += '.msgpack'\n","\n","        with open(os.path.join(pasta_para_salvar,nome_variavel),'wb') as f:\n","            f.write(variable_bytes)\n","            f.close()\n","            return True, f'Variável {nome_variavel} salva com sucesso no formato .msgpack'\n","\n","    except Exception as e:\n","        error_message = f'{e.__class__.__name__}: {str(e)}'\n","        return False, error_message\n","\n","def preProcessarTextoComRE(texto : str,\n","                           caminho_para_salvar_tokenizacao : str) -> list:\n","\n","  # texto = coletarTextoDeArquivoTxt(caminho_arquivo=caminho_arquivo_texto,tipo_de_encoding='utf-8')\n","\n","  if texto:\n","\n","    texto = padronizacaoTexto(texto)\n","\n","    lista_entidades = []\n","    lista_frases_tokenizadas = []\n","\n","    sentencas = sent_tokenize(texto, language='portuguese')\n","\n","    for sentenca in sentencas:\n","      doc = nlp_spacy(sentenca)\n","\n","      lista_tuplas_entidades = [(ent.orth_,transformarTextoSubstituindoCaracteres(texto=formatarTexto(texto=ent.orth_,\n","                                                                                                      remover_caracteres_estranhos=False,\n","                                                                                                      remover_espacos_em_branco_em_excesso=False,),\n","                                                                                  caracteres=' ',\n","                                                                                  substituir_por='_')) for ent in doc.ents]\n","\n","      lista_tuplas_entidades_manuais = [(entidade_normal, transformarTextoSubstituindoCaracteres(texto=formatarTexto(texto=entidade_normal,\n","                                                                                                        remover_caracteres_estranhos=False,\n","                                                                                                        remover_espacos_em_branco_em_excesso=False),\n","                                                                                                  caracteres=' ',\n","                                                                                                  substituir_por='_')) for entidade_normal in lista_entidades_nomeadas_manuais_ordenada]\n","      lista_atual = []\n","\n","      for entidade_normal,entidade_formatada in lista_tuplas_entidades:\n","        sentenca = sentenca.replace(entidade_normal,entidade_formatada)\n","        lista_entidades.append(entidade_formatada)\n","\n","      for entidade_normal, entidade_formatada in lista_tuplas_entidades_manuais:\n","        sentenca = substituirEntidadeNomeadaManual(texto=sentenca,entidade_normal=entidade_normal,entidade_formatada=entidade_formatada)\n","        lista_entidades.append(entidade_formatada)\n","\n","      for palavra in doc:\n","        try:\n","          if (not palavra.is_punct):\n","            if palavra.orth_ not in lista_entidades:\n","              if palavra.pos_ in ['VERB','AUX']:\n","                lista_atual.append(lematizar(verb=palavra.orth_,ignore_ambiguous_verbs=False))\n","              else:\n","                lista_atual.append(palavra.orth_)\n","            else:\n","              lista_atual.append(palavra.orth_)\n","        except Exception as e:\n","          erro = f'{e.__class__.__name__}: {str(e)}'\n","          # print(f'Erro ao processar texto na frase \"{sentenca}\":\\n\\t-->{erro}.\\nProcesso de Tokenização com lematização total / Sem rec. de ents / spaCy')\n","          pass\n","\n","      if lista_atual:\n","        lista_frases_tokenizadas.append(lista_atual)\n","\n","    if lista_frases_tokenizadas:\n","      lista_tokens_personalizados = [entidade for entidade in lista_entidades]\n","      lista_frases_tokenizadas = padronizacaoTokens(lista_frases_tokenizadas=lista_frases_tokenizadas,lista_tokens_personalizados=lista_tokens_personalizados)\n","\n","      if lista_frases_tokenizadas:\n","        status_save, msg_save = salvarArquivoMsgPack(nome_variavel='pre_processamento_c_re',\n","                                                     variavel_em_questao=lista_frases_tokenizadas,\n","                                                     pasta_para_salvar=caminho_para_salvar_tokenizacao)\n","        if not status_save:\n","          print('\\n!',msg_save,'\\n\\n')\n","          return False\n","        else:\n","          return True\n","\n","  return False"],"metadata":{"id":"NcskKbXXH9ui","executionInfo":{"status":"ok","timestamp":1713139212758,"user_tz":180,"elapsed":273,"user":{"displayName":"Igor Caetano","userId":"08005013188852644934"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["# Execução"],"metadata":{"id":"0Uhqkpxq2K42"}},{"cell_type":"code","source":["caminho_colecoes_extracao_textos = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Extração de Dados/via API DSpace Protocolo OAI-PMH/Resultados/Coleções_Teste'\n","\n","caminho_colecoes_word_embeddings = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Resultados_Tokenizacao/Colecoes_Teste'\n","\n","lista_de_colecoes_extracao = [os.path.join(caminho_colecoes_extracao_textos,colecao) for colecao in os.listdir(caminho_colecoes_extracao_textos) if '.' not in colecao]\n","\n","for colecao in lista_de_colecoes_extracao:\n","  lista_de_anos_colecao_extracao = [os.path.join(colecao,ano) for ano in os.listdir(colecao) if ano.isdigit()]\n","  for ano in lista_de_anos_colecao_extracao:\n","    lista_trabalhos_ano_colecao_extracao = [os.path.join(ano,trabalho) for trabalho in os.listdir(ano) if trabalho.startswith('Trabalho')]\n","    for trabalho in lista_trabalhos_ano_colecao_extracao:\n","      lista_arquivos_trabalho_ano_colecao_extracao = [os.path.join(trabalho,arquivo) for arquivo in os.listdir(trabalho) if arquivo in ['notas_de_rodape.txt','texto_principal.txt']]\n","      txt_total = ''\n","      for arquivo in lista_arquivos_trabalho_ano_colecao_extracao:\n","        try:\n","          txt_total += coletarTextoDeArquivoTxt(caminho_arquivo=arquivo,tipo_de_encoding='utf-8')+' '\n","        except Exception as e:\n","          pass\n","      if txt_total != '':\n","        caminho_pasta_salvar_pre_processamento = os.path.join(caminho_colecoes_word_embeddings,os.path.basename(colecao),os.path.basename(ano),os.path.basename(trabalho))\n","        if not os.path.exists(caminho_pasta_salvar_pre_processamento):\n","          os.makedirs(caminho_pasta_salvar_pre_processamento)\n","        if preProcessarTextoComRE(texto=txt_total,caminho_para_salvar_tokenizacao=caminho_pasta_salvar_pre_processamento):\n","          pass\n","        else:\n","          print('\\n! Problema ao salvar arquivo de pre_processamento de',colecao,ano,trabalho,'\\n')"],"metadata":{"id":"adLv-IzcA48F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Teste de abertura dos arquivos armazenados"],"metadata":{"id":"j2IlDAJsGmw6"}},{"cell_type":"code","source":["def openMsgPackFile(full_filepath : str,\n","                    encoding_type : str = None):\n","    if not full_filepath.endswith('.msgpack'):\n","        full_filepath += '.msgpack'\n","    if encoding_type:\n","        with open(full_filepath,'rb',encoding=encoding_type) as f:\n","            variable_bytes = f.read()\n","            variable_loaded = msgpack.unpackb(variable_bytes, raw=False)\n","            f.close()\n","            return variable_loaded\n","    else:\n","        with open(full_filepath,'rb') as f:\n","            variable_bytes = f.read()\n","            variable_loaded = msgpack.unpackb(variable_bytes, raw=False)\n","            f.close()\n","            return variable_loaded\n","\n","for colecao in os.listdir(caminho_colecoes_word_embeddings):\n","  for ano in os.listdir(os.path.join(caminho_colecoes_word_embeddings,colecao)):\n","    for trabalho in os.listdir(os.path.join(caminho_colecoes_word_embeddings,colecao,ano)):\n","      for arquivo in os.listdir(os.path.join(caminho_colecoes_word_embeddings,colecao,ano,trabalho)):\n","        print(colecao,ano,trabalho,arquivo)\n","        frases_tokenizadas = openMsgPackFile(full_filepath=os.path.join(caminho_colecoes_word_embeddings,colecao,ano,trabalho,arquivo))\n","        print(frases_tokenizadas)\n","        print('--- frases separadas')\n","        for frase in frases_tokenizadas:\n","          print(frase)\n","        print('\\n'+'-'*100+'\\n')\n","      print('')\n","    print('')\n","  print('')\n",""],"metadata":{"id":"2gOoWE6HEU3I","executionInfo":{"status":"ok","timestamp":1713139578536,"user_tz":180,"elapsed":313,"user":{"displayName":"Igor Caetano","userId":"08005013188852644934"}}},"execution_count":30,"outputs":[]}]}