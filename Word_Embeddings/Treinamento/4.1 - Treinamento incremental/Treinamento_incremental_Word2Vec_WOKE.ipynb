{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bYpmoLN9gja4",
        "sZUQqMMPgj7H"
      ],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Link para melhor visualização do Notebook em seu ambiente de execução: [Notebook Colab](https://colab.research.google.com/drive/1j-w9D6jHp6UX6-E_GCWiAuSK3S4-IcoO?usp=sharing)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Notebook desenvolvido para construção de séries temporais baseada em atualização de treinamentos. Os treinamentos incrementais e temporais precisam ter um modelo dado como \"modelo de base\", pois os seus treinamentos (séries) posteriores herdarão os parâmetros deste \"treinamento base\" para possibilitar algumas operações de comparação entre uma série e outra (principalmente similaridade de vetores que precisam ter a mesma quantidade de dimensões).\n",
        "As séries temporais treinadas de forma incremental acontecem da seguinte forma:\n",
        "\n",
        "1. Escolhe-se o Modelo base.\n",
        "2. Ajusta-se os parâmetos para formação do corpus de atualização escolhendo as coleções (que deverão ser as mesmas do treinamento que originou o modelo base) e o intervalo de datas (o que, de fato, vai dizer quais textos das coleções selecionadas serão incrementados no treinamento do novo modelo).\n",
        "3. Executa-se o treinamento incremental (com atualização da rede).\n",
        "4. Aguarda-se pacientemente a finalização dos treinos.\n",
        "\n"
      ],
      "metadata": {
        "id": "zMefqf15WK15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparação/Configuração de ambiente"
      ],
      "metadata": {
        "id": "bYpmoLN9gja4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaAQDugcf4Xj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9eaa00f-975a-4253-e347-b017b4ba783e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Ambiente configurado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import os\n",
        "    import shutil\n",
        "    from gensim.models import Word2Vec\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    from google.colab import output\n",
        "    import msgpack\n",
        "except Exception as e:\n",
        "    erro = f'{e._class__.__name__}: {str(e)}'\n",
        "    print(f'Erro ao configurar ambiente:\\n--> {erro}')\n",
        "else:\n",
        "    print('Ambiente configurado com sucesso!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções"
      ],
      "metadata": {
        "id": "sZUQqMMPgj7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def copiarArquivo(caminho_arquivo_original : str,\n",
        "                  pasta_destino : str):\n",
        "  \"\"\"\n",
        "  Função responsável por copiar um arquivo de um diretório para outro.\n",
        "\n",
        "  ### Parâmetros:\n",
        "  - caminho_arquivo_original: String contendo o caminho até o arquivo que se deseja\n",
        "  copiar.\n",
        "  - pasta_destino: String contendo o caminho até a pasta que receberá a cópia do\n",
        "  arquivo.\n",
        "\n",
        "  ### Retornos:\n",
        "  - Bool que faz referência ao sucesso (True) ou não (False) da operação.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if not os.path.exists(pasta_destino):\n",
        "      os.makedirs(pasta_destino)\n",
        "\n",
        "    shutil.copy(caminho_arquivo_original, pasta_destino)\n",
        "  except Exception as e:\n",
        "    print(f'\\n! Falha: {e.__class__.__name__}: {str(e)}')\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def abrirArquivoMsgPack(full_filepath : str,\n",
        "                        encoding_type : str = None):\n",
        "    \"\"\"\n",
        "    Função responsável por abrir os arquivos no formato msgpack.\n",
        "\n",
        "    ### Parâmetros:\n",
        "    - full_filepath: String contendo o caminho completo até o arquivo que deseja-se\n",
        "    abrir e extrair o conteúdo (variável salva).\n",
        "    - encoding_type: String contendo o tipo de encoding, caso desejar.\n",
        "\n",
        "    ### Retornos:\n",
        "    - Variável salva (e agora aberta e lida) no arquivo msgpack.\n",
        "    \"\"\"\n",
        "    if not full_filepath.endswith('.msgpack'):\n",
        "        full_filepath += '.msgpack'\n",
        "    if encoding_type:\n",
        "        with open(full_filepath,'rb',encoding=encoding_type) as f:\n",
        "            variable_bytes = f.read()\n",
        "            variable_loaded = msgpack.unpackb(variable_bytes, raw=False)\n",
        "            f.close()\n",
        "            return variable_loaded\n",
        "    else:\n",
        "        with open(full_filepath,'rb') as f:\n",
        "            variable_bytes = f.read()\n",
        "            variable_loaded = msgpack.unpackb(variable_bytes, raw=False)\n",
        "            f.close()\n",
        "            return variable_loaded\n",
        "\n",
        "\n",
        "class GeradorCorpusTokenizado:\n",
        "    \"\"\"\n",
        "    Classe geradora de corpus amigável à memória RAM. Utiliza-se de iterador e gerador\n",
        "    para não saturar a RAM do sistema que for executá-la. Desta forma não é necessário\n",
        "    carregar todos os textos de todos os trabalhos e passar como parâmetros de frases\n",
        "    na hora de treinar os modelos, pois a função de treino só precisa de um objeto\n",
        "    \"iterável\" no parâmetro de frases (sentences). Sendo geradora só será carregado\n",
        "    na RAM o texto que está se passando no momento ao invés do corpus de textos\n",
        "    todos juntos ao mesmo tempo.\n",
        "\n",
        "    ### Parâmetros:\n",
        "    - intervalo_anos: Tupla de dois inteiros referentes ao ano de início e final\n",
        "    da escrita dos textos que serão inseridos no corpus de alimentação de treino\n",
        "    (ambos os extremos incluídos no intervalo).\n",
        "    - colecoes: Lista de strings referentes às coleções que serão contempladas\n",
        "    na criação do corpus de alimentação de treino.\n",
        "    - caminho_pasta_colecoes_tokenizadas: Caminho até o corpus pré-processado onde\n",
        "    será buscado as coleções, anos, trabalhos e arquivos de pré-processamento para\n",
        "    alimentação do treinamento.\n",
        "    - usando_reconhecimento_de_entidades: Bool que dirá se os arquivos de pré-processamento\n",
        "    procurados serão os que usaram (True) ou não (False) o reconhecimento de entidades\n",
        "    nos textos (atualmente o pré-processamento que foi totalmente concluído foi\n",
        "    utilizando o reconhecimento de entidades).\n",
        "\n",
        "    ### Retornos:\n",
        "    - Objeto gerador e iterável sobre o corpus de textos contemplados pelos parâmetros\n",
        "    (\"intervalo_anos\" e \"colecoes\") passados como entrada.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, intervalo_anos : tuple[int,int], usando_reconhecimento_de_entidades : bool = True, colecoes : list[str] | str ='todas', caminho_pasta_colecoes_tokenizadas : str =r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Textos_pre_processados/Colecoes_textos_pre_processados'):\n",
        "        self.intervalo_anos = range(intervalo_anos[0], intervalo_anos[1] + 1)\n",
        "        self.usando_reconhecimento_de_entidades = usando_reconhecimento_de_entidades\n",
        "        if usando_reconhecimento_de_entidades:\n",
        "            self.arquivo_pre_processamento = 'pre_processamento_c_re.msgpack'\n",
        "        else:\n",
        "            self.arquivo_pre_processamento = 'pre_processamento_s_re.msgpack'\n",
        "        if isinstance(colecoes, str):\n",
        "            if colecoes.lower() == 'todas':\n",
        "                colecoes = [c for c in os.listdir(caminho_pasta_colecoes_tokenizadas) if '.' not in c]\n",
        "        self.arquivos = []\n",
        "        for colecao in [os.path.join(caminho_pasta_colecoes_tokenizadas, c) for c in os.listdir(caminho_pasta_colecoes_tokenizadas) if c in colecoes]:\n",
        "            lista_anos = sorted([a for a in os.listdir(colecao) if a.isdigit()])\n",
        "            for ano in [os.path.join(colecao, a) for a in lista_anos if int(os.path.basename(a)) in self.intervalo_anos]:\n",
        "                for trabalho in [os.path.join(ano, t) for t in os.listdir(ano) if t.startswith('Trabalho')]:\n",
        "                    for arquivo in [os.path.join(trabalho, arq) for arq in os.listdir(trabalho) if arq == self.arquivo_pre_processamento]:\n",
        "                        self.arquivos.append(arquivo)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for cont,arquivo in enumerate(self.arquivos):\n",
        "            for frase_tokenizada in abrirArquivoMsgPack(arquivo):\n",
        "                yield frase_tokenizada"
      ],
      "metadata": {
        "id": "f4MfdVC6gkLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exec treino incremental"
      ],
      "metadata": {
        "id": "fZ4qLxdchP60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escolhendo o corpus de treino e os modelos que melhor performaram nas analogias\n",
        "# INFO MODELOS SAUDE-CORPO-03-10\n",
        "\n",
        "\n",
        "# Modelo 1: modelo_modo_1_dimensao_150_negative_10_window_12_epochs_5_alpha_0.025_min_count_15.msgpack\n",
        "\n",
        "# Modelo 2: modelo_modo_1_dimensao_200_negative_10_window_12_epochs_5_alpha_0.025_min_count_15.msgpack\n",
        "\n",
        "# Modelo 3: modelo_modo_0_dimensao_300_negative_10_window_12_epochs_5_alpha_0.025_min_count_30.msgpack\n",
        "\n",
        "\n",
        "# Criação de um dicionário que conterá a numeração e o nome do arquivo dos treinamentos\n",
        "dic_melhores_modelos = {'1':'modelo_modo_1_dimensao_150_negative_10_window_12_epochs_5_alpha_0.025_min_count_15.msgpack',\n",
        "                        '2':'modelo_modo_1_dimensao_200_negative_10_window_12_epochs_5_alpha_0.025_min_count_15.msgpack',\n",
        "                        '3':'modelo_modo_0_dimensao_300_negative_10_window_12_epochs_5_alpha_0.025_min_count_30.msgpack'}\n",
        "\n",
        "# Nome central do modelo que terá as séries temporais\n",
        "nome_central = 'SAUDE-CORPO'\n",
        "\n",
        "# Nome completo do modelo que terá as séries temporais\n",
        "nome_pasta_treino = 'SAUDE-CORPO-03-10'\n",
        "\n",
        "# Nome do modelo base das séries temporais\n",
        "nome_modelo_treinado = 'SAUDE_CORPO_2003_2010'\n",
        "\n",
        "# Caminho até a pasta que os modelos treinados foram salvos\n",
        "caminho_pasta_modelos_salvos = os.path.join(r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Resultados Múltiplos Treinamentos',nome_pasta_treino)\n",
        "\n",
        "# Caminho até a pasta que armazenará os modelos das séries temporais\n",
        "caminho_pasta_treino_incremental = os.path.join(r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização',nome_pasta_treino,'Com RE','Treinamento incremental')\n",
        "\n",
        "# Criando a pasta de treino incremental\n",
        "if not os.path.exists(caminho_pasta_treino_incremental):\n",
        "  os.makedirs(caminho_pasta_treino_incremental)\n",
        "\n",
        "# Armazenando num txt as informações dos modelos que melhor performaram que terão suas séries temporais construídas\n",
        "caminho_txt_info = os.path.join(caminho_pasta_treino_incremental,'Arquivo de info dos Modelos.txt')\n",
        "if not os.path.exists(caminho_txt_info):\n",
        "  txt = 'INFO MODELOS\\n\\n'\n",
        "  for n_modelo in dic_melhores_modelos.keys():\n",
        "    txt += f\"\\nModelo {n_modelo}: {dic_melhores_modelos[n_modelo]}\\n\"\n",
        "  with open(caminho_txt_info,'w',encoding='utf-8') as f:\n",
        "    f.write(txt)\n",
        "\n",
        "# Import de módulo para obtenção de informação durante os novos treinos\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "loss_list = []\n",
        "\n",
        "# Criação de classe para obtenção de informação durante os novos treinos\n",
        "class Callback(CallbackAny2Vec):\n",
        "  def __init__(self):\n",
        "      self.epoch = 0\n",
        "\n",
        "  def on_epoch_end(self, model):\n",
        "      loss = model.get_latest_training_loss()\n",
        "      loss_list.append(loss)\n",
        "      print('Loss after epoch {}:{}'.format(self.epoch, loss))\n",
        "      model.running_training_loss = 0.0\n",
        "      self.epoch = self.epoch + 1\n",
        "\n",
        "# Processo para salvar o modelo base como modelo inicial nas séries temporais\n",
        "for n_modelo in dic_melhores_modelos.keys():\n",
        "  output.clear()\n",
        "  print('Passando pelo modelo:',n_modelo)\n",
        "\n",
        "  caminho_pasta_treino_incremental_n_modelo = os.path.join(caminho_pasta_treino_incremental,f'Modelo {n_modelo}')\n",
        "\n",
        "  if not os.path.exists(caminho_pasta_treino_incremental_n_modelo):\n",
        "    os.makedirs(caminho_pasta_treino_incremental_n_modelo)\n",
        "\n",
        "  nome_arquivo_modelo_base = dic_melhores_modelos[n_modelo].replace('.msgpack','')\n",
        "\n",
        "  caminho_save_modelo_base = os.path.join(caminho_pasta_modelos_salvos,nome_arquivo_modelo_base)\n",
        "\n",
        "  caminho_modelo_base = os.path.join(caminho_pasta_treino_incremental_n_modelo,nome_arquivo_modelo_base)\n",
        "\n",
        "  caminho_modelo_base_atualizado = os.path.join(caminho_pasta_treino_incremental_n_modelo,f'WOKE_{n_modelo}_{nome_modelo_treinado}_w2v_inc.model')\n",
        "\n",
        "  if not os.path.exists(os.path.join(caminho_pasta_treino_incremental_n_modelo,f'WOKE_{n_modelo}_{nome_modelo_treinado}_w2v_inc.model')):\n",
        "    for arquivo in [os.path.join(caminho_pasta_modelos_salvos,arq) for arq in os.listdir(caminho_pasta_modelos_salvos)]:\n",
        "      if nome_arquivo_modelo_base in os.path.basename(arquivo):\n",
        "        copiarArquivo(caminho_arquivo_original=arquivo,pasta_destino=caminho_pasta_treino_incremental_n_modelo)\n",
        "        os.rename(os.path.join(caminho_pasta_treino_incremental_n_modelo,os.path.basename(arquivo)),os.path.join(caminho_pasta_treino_incremental_n_modelo,os.path.basename(arquivo).replace(nome_arquivo_modelo_base,f'WOKE_{n_modelo}_{nome_modelo_treinado}_w2v_inc')))\n",
        "\n",
        "  # Carregando o modelo base pelo seu caminho atualizado\n",
        "  modelo = Word2Vec.load(caminho_modelo_base_atualizado)\n",
        "\n",
        "  # Seleção das coleções que serão contempladas no corpus atualizado para o novo treinamento\n",
        "  lista_de_colecoes = ['Biologia_Celular_e_do_Desenvolvimento','Biotecnologia_e_Biociencias','Ciencias_da_Reabilitacao','Ciencias_Medicas','Cuidados_Intensivos_e_Paliativos_Mestrado_Profissional',\n",
        "                     'Educacao_Fisica','Enfermagem','Gestao_do_Cuidado_em_Enfermagem','Gestao_do_Cuidado_em_Enfermagem_Mestrado_Profissional','Medicina_Veterinaria_Convencional_e_Integrativa',\n",
        "                     'Neurociencias','Saude_Coletiva','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Saude_Publica','Programa_de_Pos_Graduacao_Multidisciplinar_em_Saude_Mestrado_Profissional']\n",
        "\n",
        "  # Seleção dos intervalos das séries temporais que serão construídas tendo o modelo anterior como base (a primeira terá o modelo base como anterior)\n",
        "  lista_intervalo_de_datas_posteriores = [(2011,2013),(2014,2016),(2017,2019),(2020,2024)]\n",
        "\n",
        "  # Construção das séries temporais depois do modelo base\n",
        "  for intervalo_de_datas in lista_intervalo_de_datas_posteriores:\n",
        "    data_ini = intervalo_de_datas[0]\n",
        "    data_fim = intervalo_de_datas[1]\n",
        "\n",
        "    print('\\nIntervalo de datas de treino atual:',data_ini,'-',data_fim)\n",
        "\n",
        "    if not os.path.exists(os.path.join(caminho_pasta_treino_incremental_n_modelo,f'WOKE_{n_modelo}_{nome_central}_{str(data_ini)}_{str(data_fim)}_w2v_inc.model')):\n",
        "      novas_frases = GeradorCorpusTokenizado(intervalo_anos=intervalo_de_datas, colecoes = lista_de_colecoes)\n",
        "\n",
        "      modelo.build_vocab(novas_frases, update=True)\n",
        "      modelo.train(novas_frases, total_examples=modelo.corpus_count,epochs=modelo.epochs,compute_loss=True,callbacks=[Callback()])\n",
        "\n",
        "      modelo.save(os.path.join(caminho_pasta_treino_incremental_n_modelo,f'WOKE_{n_modelo}_{nome_central}_{str(data_ini)}_{str(data_fim)}_w2v_inc.model'))\n",
        "    else:\n",
        "      print('Modelo treinado encontrado no armazenamento...\\n')\n",
        "\n",
        "# O loss dos treinos não deve ser um parâmetro tão importante como tratado por um dos desenvolvedores da gensim num fórum de dúvidas: https://stackoverflow.com/questions/73891182/why-does-the-loss-of-word2vec-model-trained-by-gensim-at-first-increase-for-a-fe\n",
        "# Resumidamente, nas palavras dele (Gordon Mohr ou \"gojomo\"):\n",
        "# \"[...] Do the vectors work well on task-specific evaluations? That, moreso than loss trends, is the reliable indicator of whether your approach is working. [...]\"\n"
      ],
      "metadata": {
        "id": "Do5YwaqehRqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6a02f6-b941-42ac-fa1a-b7b86c9aec69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passando pelo modelo: 3\n",
            "\n",
            "Intervalo de datas de treino atual: 2011 - 2013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0:1290017.625\n",
            "Loss after epoch 1:1224142.875\n",
            "Loss after epoch 2:1223427.375\n",
            "Loss after epoch 3:1202459.625\n",
            "Loss after epoch 4:1142923.25\n",
            "\n",
            "Intervalo de datas de treino atual: 2014 - 2016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0:2058788.125\n",
            "Loss after epoch 1:1891581.5\n",
            "Loss after epoch 2:1855159.5\n",
            "Loss after epoch 3:1790464.125\n",
            "Loss after epoch 4:1693556.25\n",
            "\n",
            "Intervalo de datas de treino atual: 2017 - 2019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0:2091020.25\n",
            "Loss after epoch 1:1689508.5\n",
            "Loss after epoch 2:1864340.125\n",
            "Loss after epoch 3:1731205.875\n",
            "Loss after epoch 4:1722055.0\n",
            "\n",
            "Intervalo de datas de treino atual: 2020 - 2024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0:2143344.25\n",
            "Loss after epoch 1:2038419.5\n",
            "Loss after epoch 2:1912598.75\n",
            "Loss after epoch 3:1841621.5\n",
            "Loss after epoch 4:1791478.375\n"
          ]
        }
      ]
    }
  ]
}