{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Link para melhor visualização do Notebook em seu ambiente de execução: [Notebook Colab](https://colab.research.google.com/drive/1pcckEs063FfUZgI9u61tKx7BNbwIEGMh?usp=sharing)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Notebook desenvolvido para construção de séries temporais baseada sem atualização de treinamentos. Os treinamentos incrementais e temporais precisam ter um modelo dado como \"modelo de base\", pois os seus treinamentos (séries) posteriores herdarão os parâmetros deste \"treinamento base\" para possibilitar algumas operações de comparação entre uma série e outra (principalmente similaridade de vetores que precisam ter a mesma quantidade de dimensões).\n",
        "As séries temporais treinadas de forma temporal acontecem da seguinte forma:\n",
        "\n",
        "1. Escolhe-se o Modelo base.\n",
        "2. Ajusta-se os parâmetos para formação do corpus de atualização escolhendo as coleções (que deverão ser as mesmas do treinamento que originou o modelo base) e o intervalo de datas (o que, de fato, vai dizer quais textos das coleções selecionadas serão incrementados no treinamento do novo modelo).\n",
        "3. Executa-se o treinamento temporal (sem atualização da base, apenas mudança no final do intervalo de datas na construção do corpus).\n",
        "4. Aguarda-se **pacientemente** a finalização dos treinos.\n",
        "\n",
        "*Observação: Este treinamento \"temporal\" tem como objetivo **remover a possibilidade de ocorrer o fenômeno de \"esquecimento catastrófico\"** que acomete atualizações de redes neurais nos treinamentos de inteligências artificiais. Pois, na lógica utilzada aqui não ocorre atualização, apenas adição de treinos com um corpus de textos maior a cada série temporal.*\n",
        "\n",
        "*Um dos desenvolvedores do gensim-Word2Vec abordou temas relacionados à atualização de treinamentos para modelos Word2Vec e o próprio esquecimento catastrófico em uma de suas respostas em fóruns de dúvidas: [word2vec gensim update learning rate - StackOverflow - Gordon Mohr (aka \"gojomo\")](https://stackoverflow.com/questions/51133162/word2vec-gensim-update-learning-rate)*"
      ],
      "metadata": {
        "id": "sqfWO0Xvc5OX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYpmoLN9gja4"
      },
      "source": [
        "# Preparação/Configuração de ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaAQDugcf4Xj",
        "outputId": "7fc51188-07d6-4376-8e56-2ff34fb4357f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Ambiente configurado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import os\n",
        "    import re\n",
        "    import shutil\n",
        "    import msgpack\n",
        "\n",
        "    from gensim.models import Word2Vec\n",
        "    from gensim.models.callbacks import CallbackAny2Vec\n",
        "    from gensim.utils import effective_n_jobs\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    from google.colab import output\n",
        "except Exception as e:\n",
        "    erro = f'{e._class__.__name__}: {str(e)}'\n",
        "    print(f'Erro ao configurar ambiente:\\n--> {erro}')\n",
        "else:\n",
        "    print('Ambiente configurado com sucesso!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZUQqMMPgj7H"
      },
      "source": [
        "# Funções"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4MfdVC6gkLw"
      },
      "outputs": [],
      "source": [
        "def copiarArquivo(caminho_arquivo_original : str,\n",
        "                  pasta_destino : str):\n",
        "  \"\"\"\n",
        "  Função responsável por copiar um arquivo de um diretório para outro.\n",
        "\n",
        "  ### Parâmetros:\n",
        "  - caminho_arquivo_original: String contendo o caminho até o arquivo que se deseja\n",
        "  copiar.\n",
        "  - pasta_destino: String contendo o caminho até a pasta que receberá a cópia do\n",
        "  arquivo.\n",
        "\n",
        "  ### Retornos:\n",
        "  - Bool que faz referência ao sucesso (True) ou não (False) da operação.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if not os.path.exists(pasta_destino):\n",
        "      os.makedirs(pasta_destino)\n",
        "\n",
        "    shutil.copy(caminho_arquivo_original, pasta_destino)\n",
        "  except Exception as e:\n",
        "    print(f'\\n! Falha: {e.__class__.__name__}: {str(e)}')\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def abrirArquivoMsgPack(full_filepath : str,\n",
        "                        encoding_type : str = None):\n",
        "    \"\"\"\n",
        "    Função responsável por abrir os arquivos no formato msgpack.\n",
        "\n",
        "    ### Parâmetros:\n",
        "    - full_filepath: String contendo o caminho completo até o arquivo que deseja-se\n",
        "    abrir e extrair o conteúdo (variável salva).\n",
        "    - encoding_type: String contendo o tipo de encoding, caso desejar.\n",
        "\n",
        "    ### Retornos:\n",
        "    - Variável salva (e agora aberta e lida) no arquivo msgpack.\n",
        "    \"\"\"\n",
        "    if not full_filepath.endswith('.msgpack'):\n",
        "        full_filepath += '.msgpack'\n",
        "    if encoding_type:\n",
        "        with open(full_filepath,'rb',encoding=encoding_type) as f:\n",
        "            variable_bytes = f.read()\n",
        "            variable_loaded = msgpack.unpackb(variable_bytes, raw=False)\n",
        "            f.close()\n",
        "            return variable_loaded\n",
        "    else:\n",
        "        with open(full_filepath,'rb') as f:\n",
        "            variable_bytes = f.read()\n",
        "            variable_loaded = msgpack.unpackb(variable_bytes, raw=False)\n",
        "            f.close()\n",
        "            return variable_loaded\n",
        "\n",
        "\n",
        "class GeradorCorpusTokenizado:\n",
        "    \"\"\"\n",
        "    Classe geradora de corpus amigável à memória RAM. Utiliza-se de iterador e gerador\n",
        "    para não saturar a RAM do sistema que for executá-la. Desta forma não é necessário\n",
        "    carregar todos os textos de todos os trabalhos e passar como parâmetros de frases\n",
        "    na hora de treinar os modelos, pois a função de treino só precisa de um objeto\n",
        "    \"iterável\" no parâmetro de frases (sentences). Sendo geradora só será carregado\n",
        "    na RAM o texto que está se passando no momento ao invés do corpus de textos\n",
        "    todos juntos ao mesmo tempo.\n",
        "\n",
        "    ### Parâmetros:\n",
        "    - intervalo_anos: Tupla de dois inteiros referentes ao ano de início e final\n",
        "    da escrita dos textos que serão inseridos no corpus de alimentação de treino\n",
        "    (ambos os extremos incluídos no intervalo).\n",
        "    - colecoes: Lista de strings referentes às coleções que serão contempladas\n",
        "    na criação do corpus de alimentação de treino.\n",
        "    - caminho_pasta_colecoes_tokenizadas: Caminho até o corpus pré-processado onde\n",
        "    será buscado as coleções, anos, trabalhos e arquivos de pré-processamento para\n",
        "    alimentação do treinamento.\n",
        "    - usando_reconhecimento_de_entidades: Bool que dirá se os arquivos de pré-processamento\n",
        "    procurados serão os que usaram (True) ou não (False) o reconhecimento de entidades\n",
        "    nos textos (atualmente o pré-processamento que foi totalmente concluído foi\n",
        "    utilizando o reconhecimento de entidades).\n",
        "\n",
        "    ### Retornos:\n",
        "    - Objeto gerador e iterável sobre o corpus de textos contemplados pelos parâmetros\n",
        "    (\"intervalo_anos\" e \"colecoes\") passados como entrada.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, intervalo_anos : tuple[int,int], usando_reconhecimento_de_entidades : bool = True, colecoes : list[str] | str ='todas', caminho_pasta_colecoes_tokenizadas : str =r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Textos_pre_processados/Colecoes_textos_pre_processados'):\n",
        "        self.intervalo_anos = range(intervalo_anos[0], intervalo_anos[1] + 1)\n",
        "        self.usando_reconhecimento_de_entidades = usando_reconhecimento_de_entidades\n",
        "        if usando_reconhecimento_de_entidades:\n",
        "            self.arquivo_pre_processamento = 'pre_processamento_c_re.msgpack'\n",
        "        else:\n",
        "            self.arquivo_pre_processamento = 'pre_processamento_s_re.msgpack'\n",
        "        if isinstance(colecoes, str):\n",
        "            if colecoes.lower() == 'todas':\n",
        "                colecoes = [c for c in os.listdir(caminho_pasta_colecoes_tokenizadas) if '.' not in c]\n",
        "        self.arquivos = []\n",
        "        for colecao in [os.path.join(caminho_pasta_colecoes_tokenizadas, c) for c in os.listdir(caminho_pasta_colecoes_tokenizadas) if c in colecoes]:\n",
        "            lista_anos = sorted([a for a in os.listdir(colecao) if a.isdigit()])\n",
        "            for ano in [os.path.join(colecao, a) for a in lista_anos if int(os.path.basename(a)) in self.intervalo_anos]:\n",
        "                for trabalho in [os.path.join(ano, t) for t in os.listdir(ano) if t.startswith('Trabalho')]:\n",
        "                    for arquivo in [os.path.join(trabalho, arq) for arq in os.listdir(trabalho) if arq == self.arquivo_pre_processamento]:\n",
        "                        self.arquivos.append(arquivo)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for cont,arquivo in enumerate(self.arquivos):\n",
        "            for frase_tokenizada in abrirArquivoMsgPack(arquivo):\n",
        "                yield frase_tokenizada\n",
        "\n",
        "\n",
        "def extrairParametrosUsados(nome : str):\n",
        "  \"\"\"\n",
        "  Função responsável por extrair os parâmetros utilizados no treinamento anterior\n",
        "  para usar nos próximos.\n",
        "\n",
        "  ### Parâmetros:\n",
        "  - nome: String contendo o nome do arquivo de modelo treinado que possui os parâmetros\n",
        "  utilizados.\n",
        "\n",
        "  ### Retornos:\n",
        "  - Tupla de dois elementos sendo o primeiro um bool de status de processo e o\n",
        "  segundo um dicionário com as chaves sendo os parâmetros de treino e os valores\n",
        "  sendo os valores dos parâmetros propriamente ditos (caso status do processo\n",
        "  seja positivo, ou seja, True).\n",
        "  \"\"\"\n",
        "  modo = re.search(r'modo\\_(\\d)',nome)\n",
        "  if modo:\n",
        "      modo = int(modo.group(1))\n",
        "  else:\n",
        "      modo = None\n",
        "\n",
        "  dimensao = re.search(r'dimensao\\_(\\d+)',nome)\n",
        "  if dimensao:\n",
        "      dimensao = int(dimensao.group(1))\n",
        "  else:\n",
        "      dimensao = None\n",
        "\n",
        "  negative = re.search(r'negative\\_(\\d+)',nome)\n",
        "  if negative:\n",
        "      negative = int(negative.group(1))\n",
        "  else:\n",
        "      negative = None\n",
        "\n",
        "  window = re.search(r'window\\_(\\d+)',nome)\n",
        "  if window:\n",
        "      window = int(window.group(1))\n",
        "  else:\n",
        "      window = None\n",
        "\n",
        "  epochs = re.search(r'epochs\\_(\\d+)',nome)\n",
        "  if epochs:\n",
        "      epochs = int(epochs.group(1))\n",
        "  else:\n",
        "      epochs = None\n",
        "\n",
        "  alpha = re.search(r'alpha\\_(\\d\\.\\d+)',nome)\n",
        "  if alpha:\n",
        "      alpha = float(alpha.group(1))\n",
        "  else:\n",
        "      alpha = None\n",
        "\n",
        "  min_count = re.search(r'min_count\\_(\\d+)',nome)\n",
        "  if min_count:\n",
        "      min_count = int(min_count.group(1))\n",
        "  else:\n",
        "      min_count = None\n",
        "  if modo in [1,0] and dimensao and negative and window and epochs and alpha and min_count:\n",
        "    resultado = {'modo':modo,\n",
        "                 'dimensao':dimensao,\n",
        "                 'negative':negative,\n",
        "                 'window':window,\n",
        "                 'epochs':epochs,\n",
        "                 'alpha':alpha,\n",
        "                 'min_count':min_count}\n",
        "\n",
        "    return True, resultado\n",
        "  else:\n",
        "    return False, []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMSPzR2IcgqV"
      },
      "source": [
        "# Exec treino com intervalos de data estendidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2T0X_MeclPx"
      },
      "source": [
        "Começamos um treinamento com corpus de 2003 até 2006 (é o que já temos atualmente). Depois treinamos um modelo com o corpus de 2003 até 2008, depois de 2003 até 2010, 2003 até 2012, 2003 - 2014, até chegar em 2024 usando os mesmos parâmetros do treinamento \"base\" (de 2003 até 2006) para que possamos comparar os vetores de diferentes treinamentos entre si."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvb0VMQkcj5q",
        "outputId": "2ed24f91-2239-4988-ddc1-25f840f8e0e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Passando pelo modelo: 3\n",
            "\n",
            "Intervalo de datas de treino atual: 2003 - 2008\n",
            "Treinamento encontrado para: WOKE_3_UFSC_2003_2008_w2v_tmp.model\n",
            "\n",
            "Intervalo de datas de treino atual: 2003 - 2010\n",
            "Treinamento encontrado para: WOKE_3_UFSC_2003_2010_w2v_tmp.model\n",
            "\n",
            "Intervalo de datas de treino atual: 2003 - 2012\n",
            "Treinamento encontrado para: WOKE_3_UFSC_2003_2012_w2v_tmp.model\n",
            "\n",
            "Intervalo de datas de treino atual: 2003 - 2014\n",
            "Treinamento encontrado para: WOKE_3_UFSC_2003_2014_w2v_tmp.model\n",
            "\n",
            "Intervalo de datas de treino atual: 2003 - 2016\n",
            "Treinamento encontrado para: WOKE_3_UFSC_2003_2016_w2v_tmp.model\n",
            "\n",
            "Intervalo de datas de treino atual: 2003 - 2018\n",
            "Treinamento encontrado para: WOKE_3_UFSC_2003_2018_w2v_tmp.model\n",
            "\n",
            "Intervalo de datas de treino atual: 2003 - 2020\n",
            "Treinamento encontrado para: WOKE_3_UFSC_2003_2020_w2v_tmp.model\n",
            "\n",
            "Intervalo de datas de treino atual: 2003 - 2022\n",
            "Loss after epoch 0:75319720.0\n",
            "Loss after epoch 1:74963976.0\n",
            "Loss after epoch 2:74799632.0\n",
            "Loss after epoch 3:74270568.0\n",
            "Loss after epoch 4:74805072.0\n",
            "\n",
            "Intervalo de datas de treino atual: 2003 - 2024\n",
            "Loss after epoch 0:76391576.0\n",
            "Loss after epoch 1:76296560.0\n",
            "Loss after epoch 2:75936296.0\n",
            "Loss after epoch 3:75185816.0\n",
            "Loss after epoch 4:75247312.0\n"
          ]
        }
      ],
      "source": [
        "# Escolhendo o corpus de treino e os modelos que melhor performaram nas analogias\n",
        "# INFO MODELOS TODAS_2003_2006 (RI TODO COM INÍCIO EM 2003 - 2006)\n",
        "\n",
        "\n",
        "# Modelo 1: modelo_modo_1_dimensao_500_negative_10_window_12_epochs_5_alpha_0.025_min_count_45.msgpack\n",
        "\n",
        "# Modelo 2: modelo_modo_1_dimensao_300_negative_10_window_12_epochs_5_alpha_0.025_min_count_60.msgpack\n",
        "\n",
        "# Modelo 3: modelo_modo_1_dimensao_300_negative_10_window_12_epochs_5_alpha_0.025_min_count_45.msgpack\n",
        "\n",
        "# Modelo 4: modelo_modo_1_dimensao_500_negative_10_window_12_epochs_5_alpha_0.025_min_count_60.msgpack\n",
        "\n",
        "\n",
        "# Criação de um dicionário que conterá a numeração e o nome do arquivo dos treinamentos\n",
        "dic_melhores_modelos = {'1':'modelo_modo_1_dimensao_500_negative_10_window_12_epochs_5_alpha_0.025_min_count_45.msgpack',\n",
        "                        '2':'modelo_modo_1_dimensao_300_negative_10_window_12_epochs_5_alpha_0.025_min_count_60.msgpack',\n",
        "                        '3':'modelo_modo_1_dimensao_300_negative_10_window_12_epochs_5_alpha_0.025_min_count_45.msgpack',\n",
        "                        '4':'modelo_modo_1_dimensao_500_negative_10_window_12_epochs_5_alpha_0.025_min_count_60.msgpack'}\n",
        "\n",
        "# Nome central do modelo que terá as séries temporais\n",
        "nome_central = 'UFSC'\n",
        "\n",
        "# Nome completo do modelo que terá as séries temporais\n",
        "nome_pasta_treino = 'Todas 2003 - 2006'\n",
        "\n",
        "# Nome do modelo base das séries temporais\n",
        "nome_modelo_treinado = 'UFSC_2003_2006'\n",
        "\n",
        "# Caminho até a pasta que os modelos treinados foram salvos\n",
        "caminho_pasta_modelos_salvos = os.path.join(r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Resultados Múltiplos Treinamentos',nome_pasta_treino)\n",
        "\n",
        "# Caminho até a pasta que armazenará os modelos das séries temporais\n",
        "caminho_pasta_treino_temporal = os.path.join(r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização',nome_pasta_treino,'Com RE','Treinamento temporal')\n",
        "\n",
        "# Criando a pasta de treino temporal\n",
        "if not os.path.exists(caminho_pasta_treino_temporal):\n",
        "  os.makedirs(caminho_pasta_treino_temporal)\n",
        "\n",
        "# Armazenando num txt as informações dos modelos que melhor performaram que terão suas séries temporais construídas\n",
        "caminho_txt_info = os.path.join(caminho_pasta_treino_temporal,'Arquivo de info dos Modelos.txt')\n",
        "if not os.path.exists(caminho_txt_info):\n",
        "  txt = 'INFO MODELOS\\n\\n'\n",
        "  for n_modelo in dic_melhores_modelos.keys():\n",
        "    txt += f\"\\nModelo {n_modelo}: {dic_melhores_modelos[n_modelo]}\\n\"\n",
        "  with open(caminho_txt_info,'w',encoding='utf-8') as f:\n",
        "    f.write(txt)\n",
        "\n",
        "# Obtenção da quantidade de threads disponíveis no processador do sistema que está executando o programa\n",
        "n_workers = effective_n_jobs(-1)\n",
        "\n",
        "# Criação de classe para obtenção de informação durante os novos treinos\n",
        "loss_list = []\n",
        "class Callback(CallbackAny2Vec):\n",
        "  def __init__(self):\n",
        "      self.epoch = 0\n",
        "\n",
        "  def on_epoch_end(self, model):\n",
        "      loss = model.get_latest_training_loss()\n",
        "      loss_list.append(loss)\n",
        "      print('Loss after epoch {}:{}'.format(self.epoch, loss))\n",
        "      model.running_training_loss = 0.0\n",
        "      self.epoch = self.epoch + 1\n",
        "\n",
        "# Processo para salvar o modelo base como modelo inicial nas séries temporais\n",
        "# for n_modelo in dic_melhores_modelos.keys():\n",
        "for n_modelo in list(dic_melhores_modelos.keys())[1:3]: # [1:3] (foi criado outros programas deste para rodarem outra parte da lista, pois treinamento temporal demora muito mais que o incremental)\n",
        "  output.clear()\n",
        "  print('Passando pelo modelo:',n_modelo)\n",
        "\n",
        "  status_parametros, dic_parametros = extrairParametrosUsados(dic_melhores_modelos[n_modelo])\n",
        "\n",
        "  if status_parametros:\n",
        "\n",
        "    caminho_pasta_treino_temporal_n_modelo = os.path.join(caminho_pasta_treino_temporal,f'Modelo {n_modelo}')\n",
        "\n",
        "    if not os.path.exists(caminho_pasta_treino_temporal_n_modelo):\n",
        "      os.makedirs(caminho_pasta_treino_temporal_n_modelo)\n",
        "\n",
        "    nome_arquivo_modelo_base = dic_melhores_modelos[n_modelo].replace('.msgpack','')\n",
        "\n",
        "    caminho_save_modelo_base = os.path.join(caminho_pasta_modelos_salvos,nome_arquivo_modelo_base)\n",
        "\n",
        "    caminho_modelo_base = os.path.join(caminho_pasta_treino_temporal_n_modelo,nome_arquivo_modelo_base)\n",
        "\n",
        "    caminho_modelo_base_atualizado = os.path.join(caminho_pasta_treino_temporal_n_modelo,f'WOKE_{n_modelo}_{nome_modelo_treinado}_w2v_tmp.model')\n",
        "\n",
        "    if not os.path.exists(os.path.join(caminho_pasta_treino_temporal_n_modelo,f'WOKE_{n_modelo}_{nome_modelo_treinado}_w2v_tmp.model')):\n",
        "      for arquivo in [os.path.join(caminho_pasta_modelos_salvos,arq) for arq in os.listdir(caminho_pasta_modelos_salvos)]:\n",
        "        if nome_arquivo_modelo_base in os.path.basename(arquivo):\n",
        "          copiarArquivo(caminho_arquivo_original=arquivo,pasta_destino=caminho_pasta_treino_temporal_n_modelo)\n",
        "          os.rename(os.path.join(caminho_pasta_treino_temporal_n_modelo,os.path.basename(arquivo)),os.path.join(caminho_pasta_treino_temporal_n_modelo,os.path.basename(arquivo).replace(nome_arquivo_modelo_base,f'WOKE_{n_modelo}_{nome_modelo_treinado}_w2v_tmp')))\n",
        "\n",
        "\n",
        "\n",
        "    # Seleção das coleções que serão contempladas no corpus atualizado para o novo treinamento\n",
        "\n",
        "    # SAUDE-CORPO\n",
        "    # lista_de_colecoes = ['Biologia_Celular_e_do_Desenvolvimento','Biotecnologia_e_Biociencias','Ciencias_da_Reabilitacao','Ciencias_Medicas','Cuidados_Intensivos_e_Paliativos_Mestrado_Profissional',\n",
        "    #                  'Educacao_Fisica','Enfermagem','Gestao_do_Cuidado_em_Enfermagem','Gestao_do_Cuidado_em_Enfermagem_Mestrado_Profissional','Medicina_Veterinaria_Convencional_e_Integrativa',\n",
        "    #                  'Neurociencias','Saude_Coletiva','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Saude_Publica','Programa_de_Pos_Graduacao_Multidisciplinar_em_Saude_Mestrado_Profissional']\n",
        "\n",
        "    # CFH\n",
        "    # lista_de_colecoes = ['Filosofia','Geografia','Geologia','Historia','Psicologia','Teses_e_dissertacoes_do_Centro_de_Filosofia_e_Ciencias_Humanas','Programa_de_Pos_Graduacao_Interdisciplinar_em_Ciencias_Humanas','Servico_Social','Sociologia_e_Ciencia_Politica','Sociologia_Politica','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Ensino_de_Historia_Mestrado_Profissional']\n",
        "\n",
        "    # UFSC\n",
        "    lista_de_colecoes = 'todas'\n",
        "\n",
        "\n",
        "    # Seleção dos intervalos das séries temporais que serão construídas tendo o modelo anterior como base (a primeira terá o modelo base como anterior)\n",
        "    lista_intervalo_de_datas_posteriores = [(2003,2008),(2003,2010),(2003,2012),(2003,2014),(2003,2016),(2003,2018),(2003,2020),(2003,2022),(2003,2024)]\n",
        "\n",
        "\n",
        "    # Construção das séries temporais depois do modelo selecionado como o primeiro\n",
        "    for intervalo_de_datas in lista_intervalo_de_datas_posteriores:\n",
        "      data_ini = intervalo_de_datas[0]\n",
        "      data_fim = intervalo_de_datas[1]\n",
        "\n",
        "      print('\\nIntervalo de datas de treino atual:',data_ini,'-',data_fim)\n",
        "\n",
        "      if not os.path.exists(os.path.join(caminho_pasta_treino_temporal_n_modelo,f'WOKE_{n_modelo}_{nome_central}_{str(data_ini)}_{str(data_fim)}_w2v_tmp.model')):\n",
        "\n",
        "        corpus_atual = GeradorCorpusTokenizado(intervalo_anos=intervalo_de_datas, colecoes=lista_de_colecoes)\n",
        "\n",
        "        modelo = Word2Vec(sentences=corpus_atual,sg=dic_parametros['modo'],vector_size=dic_parametros['dimensao'],\n",
        "                          negative=dic_parametros['negative'],window=dic_parametros['window'],epochs=dic_parametros['epochs'],\n",
        "                          alpha=dic_parametros['alpha'],min_count=dic_parametros['min_count'],workers=n_workers,\n",
        "                          compute_loss=True,callbacks=[Callback()])\n",
        "\n",
        "        modelo.save(os.path.join(caminho_pasta_treino_temporal_n_modelo,f'WOKE_{n_modelo}_{nome_central}_{str(data_ini)}_{str(data_fim)}_w2v_tmp.model'))\n",
        "      else:\n",
        "        print('Treinamento encontrado para:',f'WOKE_{n_modelo}_{nome_central}_{str(data_ini)}_{str(data_fim)}_w2v_tmp.model')\n",
        "\n",
        "  else:\n",
        "    print('Parâmetros não foram extraídos com êxito!')\n",
        "\n",
        "# O loss dos treinos não deve ser um parâmetro tão importante como tratado por um dos desenvolvedores da gensim num fórum de dúvidas: https://stackoverflow.com/questions/73891182/why-does-the-loss-of-word2vec-model-trained-by-gensim-at-first-increase-for-a-fe\n",
        "# Resumidamente, nas palavras dele (Gordon Mohr ou \"gojomo\"):\n",
        "# \"[...] Do the vectors work well on task-specific evaluations? That, moreso than loss trends, is the reliable indicator of whether your approach is working. [...]\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "bYpmoLN9gja4",
        "sZUQqMMPgj7H"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}