# WOKE - HST

- **Quantidade de coleções contempladas: 1**
- **Quantidade de trabalhos contemplados no total: 555**

## Hyperlinks para os temas desta página

- [Incremental](#incremental)
  - [WOKE HST MODELO 1](#woke-hst-modelo-1)
  - [WOKE HST MODELO 2](#woke-hst-modelo-2)
  - [WOKE HST MODELO 3](#woke-hst-modelo-3)
  - [WOKE HST MODELO 4](#woke-hst-modelo-4)


- [Temporal](#temporal)
  - [WOKE HST MODELO 1](#woke-hst-modelo-1-1)
  - [WOKE HST MODELO 2](#woke-hst-modelo-2-1)
  - [WOKE HST MODELO 3](#woke-hst-modelo-3-1)
  - [WOKE HST MODELO 4](#woke-hst-modelo-4-1)
 

## Incremental
### WOKE HST MODELO 1
#### WOKE_1_HST_2003_2010_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de palavras que entraram para o treinamento: 4.760.532
- Quantidade de tokens no vocabulário: 11.465
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2011_2013_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 243.348
- Quantidade de tokens no corpus usado no treino: 3.100.347
- Quantidade de palavras que entraram para o treinamento: 7.386.474
- Quantidade de tokens no vocabulário: 12.206
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2014_2016_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 305.802
- Quantidade de tokens no corpus usado no treino: 4.011.264
- Quantidade de palavras que entraram para o treinamento: 10.835.615
- Quantidade de tokens no vocabulário: 13.278
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2017_2019_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 336.958
- Quantidade de tokens no corpus usado no treino: 4.551.670
- Quantidade de palavras que entraram para o treinamento: 14.722.910
- Quantidade de tokens no vocabulário: 14.539
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2020_2024_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 355.330
- Quantidade de tokens no corpus usado no treino: 4.895.643
- Quantidade de palavras que entraram para o treinamento: 18.890.441
- Quantidade de tokens no vocabulário: 16.020
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

### WOKE HST MODELO 2
#### WOKE_2_HST_2003_2010_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de palavras que entraram para o treinamento: 4.660.133
- Quantidade de tokens no vocabulário: 9.514
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2011_2013_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 243.348
- Quantidade de tokens no corpus usado no treino: 3.100.347
- Quantidade de palavras que entraram para o treinamento: 7.224.898
- Quantidade de tokens no vocabulário: 10.064
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2014_2016_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 305.802
- Quantidade de tokens no corpus usado no treino: 4.011.264
- Quantidade de palavras que entraram para o treinamento: 10.598.618
- Quantidade de tokens no vocabulário: 10.829
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2017_2019_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 336.958
- Quantidade de tokens no corpus usado no treino: 4.551.670
- Quantidade de palavras que entraram para o treinamento: 14.402.473
- Quantidade de tokens no vocabulário: 11.801
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2020_2024_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 355.330
- Quantidade de tokens no corpus usado no treino: 4.895.643
- Quantidade de palavras que entraram para o treinamento: 18.481.091
- Quantidade de tokens no vocabulário: 12.962
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

### WOKE HST MODELO 3
#### WOKE_3_HST_2003_2010_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de palavras que entraram para o treinamento: 4.760.532
- Quantidade de tokens no vocabulário: 11.465
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2011_2013_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 243.348
- Quantidade de tokens no corpus usado no treino: 3.100.347
- Quantidade de palavras que entraram para o treinamento: 7.386.474
- Quantidade de tokens no vocabulário: 12.206
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2014_2016_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 305.802
- Quantidade de tokens no corpus usado no treino: 4.011.264
- Quantidade de palavras que entraram para o treinamento: 10.835.615
- Quantidade de tokens no vocabulário: 13.278
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2017_2019_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 336.958
- Quantidade de tokens no corpus usado no treino: 4.551.670
- Quantidade de palavras que entraram para o treinamento: 14.722.910
- Quantidade de tokens no vocabulário: 14.539
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2020_2024_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 355.330
- Quantidade de tokens no corpus usado no treino: 4.895.643
- Quantidade de palavras que entraram para o treinamento: 18.890.441
- Quantidade de tokens no vocabulário: 16.020
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

### WOKE HST MODELO 4
#### WOKE_4_HST_2003_2010_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de palavras que entraram para o treinamento: 4.883.691
- Quantidade de tokens no vocabulário: 14.878
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2011_2013_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 243.348
- Quantidade de tokens no corpus usado no treino: 3.100.347
- Quantidade de palavras que entraram para o treinamento: 7.583.838
- Quantidade de tokens no vocabulário: 16.027
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2014_2016_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 305.802
- Quantidade de tokens no corpus usado no treino: 4.011.264
- Quantidade de palavras que entraram para o treinamento: 11.124.215
- Quantidade de tokens no vocabulário: 17.670
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2017_2019_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 336.958
- Quantidade de tokens no corpus usado no treino: 4.551.670
- Quantidade de palavras que entraram para o treinamento: 15.113.971
- Quantidade de tokens no vocabulário: 19.486
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2020_2024_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 355.330
- Quantidade de tokens no corpus usado no treino: 4.895.643
- Quantidade de palavras que entraram para o treinamento: 19.389.677
- Quantidade de tokens no vocabulário: 21.650
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---


## Temporal
### WOKE HST MODELO 1
#### WOKE_1_HST_2003_2010_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de palavras que entraram para o treinamento: 4.760.532
- Quantidade de tokens no vocabulário: 11.465
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2003_2013_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 685.984
- Quantidade de tokens no corpus usado no treino: 8.649.140
- Quantidade de palavras que entraram para o treinamento: 7.576.514
- Quantidade de tokens no vocabulário: 15.352
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2003_2016_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 991.786
- Quantidade de tokens no corpus usado no treino: 12.660.404
- Quantidade de palavras que entraram para o treinamento: 11.275.737
- Quantidade de tokens no vocabulário: 19.327
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2003_2019_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 1.328.744
- Quantidade de tokens no corpus usado no treino: 17.212.074
- Quantidade de palavras que entraram para o treinamento: 15.466.865
- Quantidade de tokens no vocabulário: 23.511
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2003_2024_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 1.684.074
- Quantidade de tokens no corpus usado no treino: 22.107.717
- Quantidade de palavras que entraram para o treinamento: 19.957.792
- Quantidade de tokens no vocabulário: 27.647
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

### WOKE HST MODELO 2
#### WOKE_2_HST_2003_2010_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de palavras que entraram para o treinamento: 4.660.133
- Quantidade de tokens no vocabulário: 9.514
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2003_2013_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 685.984
- Quantidade de tokens no corpus usado no treino: 8.649.140
- Quantidade de palavras que entraram para o treinamento: 7.445.282
- Quantidade de tokens no vocabulário: 12.802
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2003_2016_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 991.786
- Quantidade de tokens no corpus usado no treino: 12.660.404
- Quantidade de palavras que entraram para o treinamento: 11.114.269
- Quantidade de tokens no vocabulário: 16.193
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2003_2019_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 1.328.744
- Quantidade de tokens no corpus usado no treino: 17.212.074
- Quantidade de palavras que entraram para o treinamento: 15.266.693
- Quantidade de tokens no vocabulário: 19.612
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2003_2024_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 1.684.074
- Quantidade de tokens no corpus usado no treino: 22.107.717
- Quantidade de palavras que entraram para o treinamento: 19.721.859
- Quantidade de tokens no vocabulário: 23.050
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

### WOKE HST MODELO 3
#### WOKE_3_HST_2003_2010_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de palavras que entraram para o treinamento: 4.760.532
- Quantidade de tokens no vocabulário: 11.465
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2003_2013_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 685.984
- Quantidade de tokens no corpus usado no treino: 8.649.140
- Quantidade de palavras que entraram para o treinamento: 7.576.514
- Quantidade de tokens no vocabulário: 15.352
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2003_2016_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 991.786
- Quantidade de tokens no corpus usado no treino: 12.660.404
- Quantidade de palavras que entraram para o treinamento: 11.275.737
- Quantidade de tokens no vocabulário: 19.327
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2003_2019_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 1.328.744
- Quantidade de tokens no corpus usado no treino: 17.212.074
- Quantidade de palavras que entraram para o treinamento: 15.466.865
- Quantidade de tokens no vocabulário: 23.511
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2003_2024_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 1.684.074
- Quantidade de tokens no corpus usado no treino: 22.107.717
- Quantidade de palavras que entraram para o treinamento: 19.957.792
- Quantidade de tokens no vocabulário: 27.647
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

### WOKE HST MODELO 4
#### WOKE_4_HST_2003_2010_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de palavras que entraram para o treinamento: 4.883.691
- Quantidade de tokens no vocabulário: 14.878
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2003_2013_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 685.984
- Quantidade de tokens no corpus usado no treino: 8.649.140
- Quantidade de palavras que entraram para o treinamento: 7.737.992
- Quantidade de tokens no vocabulário: 19.799
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2003_2016_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 991.786
- Quantidade de tokens no corpus usado no treino: 12.660.404
- Quantidade de palavras que entraram para o treinamento: 11.482.174
- Quantidade de tokens no vocabulário: 25.029
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2003_2019_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 1.328.744
- Quantidade de tokens no corpus usado no treino: 17.212.074
- Quantidade de palavras que entraram para o treinamento: 15.714.507
- Quantidade de tokens no vocabulário: 30.356
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2003_2024_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 1.684.074
- Quantidade de tokens no corpus usado no treino: 22.107.717
- Quantidade de palavras que entraram para o treinamento: 20.248.333
- Quantidade de tokens no vocabulário: 35.688
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---
