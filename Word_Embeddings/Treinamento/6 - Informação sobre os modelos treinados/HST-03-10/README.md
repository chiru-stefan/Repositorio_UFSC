# WOKE - HST

- **Quantidade de coleções contempladas: 1**
- **Quantidade de trabalhos contemplados no total: 555**

## Hyperlinks para os temas desta página

- [Incremental](#incremental)
  - [WOKE HST MODELO 1](#woke-hst-modelo-1)
  - [WOKE HST MODELO 2](#woke-hst-modelo-2)
  - [WOKE HST MODELO 3](#woke-hst-modelo-3)
  - [WOKE HST MODELO 4](#woke-hst-modelo-4)


- [Temporal](#temporal)
  - [WOKE HST MODELO 1](#woke-hst-modelo-1-1)
  - [WOKE HST MODELO 2](#woke-hst-modelo-2-1)
  - [WOKE HST MODELO 3](#woke-hst-modelo-3-1)
  - [WOKE HST MODELO 4](#woke-hst-modelo-4-1)
 

## Incremental
### WOKE HST MODELO 1
#### WOKE_1_HST_2003_2010_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de tokens no vocabulário: 11.465
- Quantidade de palavras que entraram para o treinamento: 4.760.532
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2011_2013_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 243.348
- Quantidade de tokens no corpus usado no treino: 3.100.347
- Quantidade de tokens no vocabulário: 12.206
- Quantidade de palavras que entraram para o treinamento: 7.386.474
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2014_2016_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 305.802
- Quantidade de tokens no corpus usado no treino: 4.011.264
- Quantidade de tokens no vocabulário: 13.278
- Quantidade de palavras que entraram para o treinamento: 10.835.615
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2017_2019_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 336.958
- Quantidade de tokens no corpus usado no treino: 4.551.670
- Quantidade de tokens no vocabulário: 14.539
- Quantidade de palavras que entraram para o treinamento: 14.722.910
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2020_2024_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 355.330
- Quantidade de tokens no corpus usado no treino: 4.895.643
- Quantidade de tokens no vocabulário: 16.020
- Quantidade de palavras que entraram para o treinamento: 18.890.441
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

### WOKE HST MODELO 2
#### WOKE_2_HST_2003_2010_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de tokens no vocabulário: 9.514
- Quantidade de palavras que entraram para o treinamento: 4.660.133
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2011_2013_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 243.348
- Quantidade de tokens no corpus usado no treino: 3.100.347
- Quantidade de tokens no vocabulário: 10.064
- Quantidade de palavras que entraram para o treinamento: 7.224.898
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2014_2016_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 305.802
- Quantidade de tokens no corpus usado no treino: 4.011.264
- Quantidade de tokens no vocabulário: 10.829
- Quantidade de palavras que entraram para o treinamento: 10.598.618
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2017_2019_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 336.958
- Quantidade de tokens no corpus usado no treino: 4.551.670
- Quantidade de tokens no vocabulário: 11.801
- Quantidade de palavras que entraram para o treinamento: 14.402.473
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2020_2024_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 355.330
- Quantidade de tokens no corpus usado no treino: 4.895.643
- Quantidade de tokens no vocabulário: 12.962
- Quantidade de palavras que entraram para o treinamento: 18.481.091
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

### WOKE HST MODELO 3
#### WOKE_3_HST_2003_2010_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de tokens no vocabulário: 11.465
- Quantidade de palavras que entraram para o treinamento: 4.760.532
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2011_2013_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 243.348
- Quantidade de tokens no corpus usado no treino: 3.100.347
- Quantidade de tokens no vocabulário: 12.206
- Quantidade de palavras que entraram para o treinamento: 7.386.474
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2014_2016_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 305.802
- Quantidade de tokens no corpus usado no treino: 4.011.264
- Quantidade de tokens no vocabulário: 13.278
- Quantidade de palavras que entraram para o treinamento: 10.835.615
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2017_2019_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 336.958
- Quantidade de tokens no corpus usado no treino: 4.551.670
- Quantidade de tokens no vocabulário: 14.539
- Quantidade de palavras que entraram para o treinamento: 14.722.910
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2020_2024_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 355.330
- Quantidade de tokens no corpus usado no treino: 4.895.643
- Quantidade de tokens no vocabulário: 16.020
- Quantidade de palavras que entraram para o treinamento: 18.890.441
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

### WOKE HST MODELO 4
#### WOKE_4_HST_2003_2010_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de tokens no vocabulário: 14.878
- Quantidade de palavras que entraram para o treinamento: 4.883.691
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2011_2013_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 243.348
- Quantidade de tokens no corpus usado no treino: 3.100.347
- Quantidade de tokens no vocabulário: 16.027
- Quantidade de palavras que entraram para o treinamento: 7.583.838
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2014_2016_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 305.802
- Quantidade de tokens no corpus usado no treino: 4.011.264
- Quantidade de tokens no vocabulário: 17.670
- Quantidade de palavras que entraram para o treinamento: 11.124.215
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2017_2019_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 336.958
- Quantidade de tokens no corpus usado no treino: 4.551.670
- Quantidade de tokens no vocabulário: 19.486
- Quantidade de palavras que entraram para o treinamento: 15.113.971
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2020_2024_w2v_inc
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 355.330
- Quantidade de tokens no corpus usado no treino: 4.895.643
- Quantidade de tokens no vocabulário: 21.650
- Quantidade de palavras que entraram para o treinamento: 19.389.677
##### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30

---

## Temporal
### WOKE HST MODELO 1
#### WOKE_1_HST_2003_2010_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de tokens no vocabulário: 11.465
- Quantidade de palavras que entraram para o treinamento: 4.760.532
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2003_2013_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 685.984
- Quantidade de tokens no corpus usado no treino: 8.649.140
- Quantidade de tokens no vocabulário: 15.352
- Quantidade de palavras que entraram para o treinamento: 7.576.514
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2003_2016_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 991.786
- Quantidade de tokens no corpus usado no treino: 12.660.404
- Quantidade de tokens no vocabulário: 19.327
- Quantidade de palavras que entraram para o treinamento: 11.275.737
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2003_2019_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 1.328.744
- Quantidade de tokens no corpus usado no treino: 17.212.074
- Quantidade de tokens no vocabulário: 23.511
- Quantidade de palavras que entraram para o treinamento: 15.466.865
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_1_HST_2003_2024_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 1.684.074
- Quantidade de tokens no corpus usado no treino: 22.107.717
- Quantidade de tokens no vocabulário: 27.647
- Quantidade de palavras que entraram para o treinamento: 19.957.792
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 45


---

### WOKE HST MODELO 2
#### WOKE_2_HST_2003_2010_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de tokens no vocabulário: 9.514
- Quantidade de palavras que entraram para o treinamento: 4.660.133
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2003_2013_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 685.984
- Quantidade de tokens no corpus usado no treino: 8.649.140
- Quantidade de tokens no vocabulário: 12.802
- Quantidade de palavras que entraram para o treinamento: 7.445.282
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2003_2016_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 991.786
- Quantidade de tokens no corpus usado no treino: 12.660.404
- Quantidade de tokens no vocabulário: 16.193
- Quantidade de palavras que entraram para o treinamento: 11.114.269
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2003_2019_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 1.328.744
- Quantidade de tokens no corpus usado no treino: 17.212.074
- Quantidade de tokens no vocabulário: 19.612
- Quantidade de palavras que entraram para o treinamento: 15.266.693
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

#### WOKE_2_HST_2003_2024_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 1.684.074
- Quantidade de tokens no corpus usado no treino: 22.107.717
- Quantidade de tokens no vocabulário: 23.050
- Quantidade de palavras que entraram para o treinamento: 19.721.859
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 150
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 60


---

### WOKE HST MODELO 3
#### WOKE_3_HST_2003_2010_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de tokens no vocabulário: 11.465
- Quantidade de palavras que entraram para o treinamento: 4.760.532
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2003_2013_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 685.984
- Quantidade de tokens no corpus usado no treino: 8.649.140
- Quantidade de tokens no vocabulário: 15.352
- Quantidade de palavras que entraram para o treinamento: 7.576.514
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2003_2016_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 991.786
- Quantidade de tokens no corpus usado no treino: 12.660.404
- Quantidade de tokens no vocabulário: 19.327
- Quantidade de palavras que entraram para o treinamento: 11.275.737
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2003_2019_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 1.328.744
- Quantidade de tokens no corpus usado no treino: 17.212.074
- Quantidade de tokens no vocabulário: 23.511
- Quantidade de palavras que entraram para o treinamento: 15.466.865
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

#### WOKE_3_HST_2003_2024_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 1.684.074
- Quantidade de tokens no corpus usado no treino: 22.107.717
- Quantidade de tokens no vocabulário: 27.647
- Quantidade de palavras que entraram para o treinamento: 19.957.792
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 5
- window: 8
- epochs 5
- alpha 0.025
- min_count: 45


---

### WOKE HST MODELO 4
#### WOKE_4_HST_2003_2010_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 167
- Quantidade de frases: 442.636
- Quantidade de tokens no corpus usado no treino: 5.548.793
- Quantidade de tokens no vocabulário: 14.878
- Quantidade de palavras que entraram para o treinamento: 4.883.691
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2003_2013_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 249
- Quantidade de frases: 685.984
- Quantidade de tokens no corpus usado no treino: 8.649.140
- Quantidade de tokens no vocabulário: 19.799
- Quantidade de palavras que entraram para o treinamento: 7.737.992
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2003_2016_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 344
- Quantidade de frases: 991.786
- Quantidade de tokens no corpus usado no treino: 12.660.404
- Quantidade de tokens no vocabulário: 25.029
- Quantidade de palavras que entraram para o treinamento: 11.482.174
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2003_2019_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 445
- Quantidade de frases: 1.328.744
- Quantidade de tokens no corpus usado no treino: 17.212.074
- Quantidade de tokens no vocabulário: 30.356
- Quantidade de palavras que entraram para o treinamento: 15.714.507
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30


---

#### WOKE_4_HST_2003_2024_w2v_tmp
##### Contagens
- Quantidade de trabalhos contemplados: 555
- Quantidade de frases: 1.684.074
- Quantidade de tokens no corpus usado no treino: 22.107.717
- Quantidade de tokens no vocabulário: 35.688
- Quantidade de palavras que entraram para o treinamento: 20.248.333
###### Parâmetros
- Modo: Skip-Gram
- vector_size: 100
- negative: 10
- window: 12
- epochs 5
- alpha 0.025
- min_count: 30

---

