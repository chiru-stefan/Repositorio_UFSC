{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8TSOUJlbDadS",
        "E5EYVoX0Dc1o",
        "awMMBoosD181"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Link para melhor visualização do Notebook em seu ambiente de execução: [Notebook Colab](https://colab.research.google.com/drive/18RYstN48y9nOYdmI5AVRRO_qHc0meK28?usp=sharing)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Notebook desenvolvido para obtenção de informações dos modelos WOKE"
      ],
      "metadata": {
        "id": "KdpF0fJ9dVWO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzppqb5YGibX",
        "outputId": "eeceb257-3089-4de0-d7ee-8a42563e5eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "caminho_colecoes = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Textos_pre_processados/Colecoes_textos_pre_processados'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HST"
      ],
      "metadata": {
        "id": "8TSOUJlbDadS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HST inc"
      ],
      "metadata": {
        "id": "M6nSPzD3GvOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_de_colecoes = ['Historia']\n",
        "\n",
        "tipo_treino = 'Treinamento incremental'\n",
        "\n",
        "treino = 'HST-03-10'\n",
        "\n",
        "caminho_treino_incremental = f'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/{treino}/Com RE/{tipo_treino}'\n",
        "\n",
        "print('## Incremental')\n",
        "for caminho_modelo_inc in sorted([os.path.join(caminho_treino_incremental,modelo) for modelo in os.listdir(caminho_treino_incremental) if '.' not in modelo]):\n",
        "\n",
        "  print(f'### WOKE HST {os.path.basename(caminho_modelo_inc).upper()}')\n",
        "  # c = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1'\n",
        "\n",
        "  for caminho_modelo in sorted([os.path.join(caminho_modelo_inc,modelo) for modelo in os.listdir(caminho_modelo_inc) if modelo.endswith('.model')]):\n",
        "  # caminho_modelo = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1/WOKE_1_HST_2003_2010_w2v_inc.model'\n",
        "\n",
        "    modelo = Word2Vec.load(caminho_modelo)\n",
        "\n",
        "    ano_final = os.path.basename(caminho_modelo)[-18:-14]\n",
        "    print(f'#### {os.path.basename(caminho_modelo).replace(\".model\",\"\")}')\n",
        "    print('##### Contagens')\n",
        "    qtd_trabalhos = 0\n",
        "    for caminho_colecao in sorted([os.path.join(caminho_colecoes,colecao) for colecao in os.listdir(caminho_colecoes) if '.' not in colecao and colecao in lista_de_colecoes]):\n",
        "      for caminho_ano in sorted([os.path.join(caminho_colecao,ano) for ano in os.listdir(caminho_colecao) if ano.isdigit() and ano in [str(a) for a in range(2003,int(ano_final)+1)]]):\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "        for caminho_trabalho in [os.path.join(caminho_ano,trabalho) for trabalho in os.listdir(caminho_ano) if trabalho.startswith('Trabalho')]:\n",
        "          if os.path.exists(os.path.join(caminho_trabalho,'pre_processamento_c_re.msgpack')):\n",
        "            qtd_trabalhos += 1\n",
        "      # if os.path.basename(caminho_ano) in ['2010','2013','2016','2019','2023','2024']:\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "    print('- Quantidade de trabalhos contemplados:','{0:,}'.format(qtd_trabalhos).replace(',','.'))\n",
        "    print('- Quantidade de frases:','{0:,}'.format(modelo.corpus_count).replace(',','.'))\n",
        "    print('- Quantidade de tokens no corpus usado no treino:','{0:,}'.format(modelo.corpus_total_words).replace(',','.'))\n",
        "    print('- Quantidade de palavras que entraram para o treinamento:','{0:,}'.format(sum(modelo.wv.get_vecattr(word, 'count') for word in modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('- Quantidade de tokens no vocabulário:','{0:,}'.format(len(modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('###### Parâmetros')\n",
        "    if modelo.sg == 1:\n",
        "      print('- Modo: Skip-Gram')\n",
        "    else:\n",
        "      print('- Modo: CBOW')\n",
        "    print('- vector_size:',modelo.vector_size)\n",
        "    print('- negative:',modelo.negative)\n",
        "    print('- window:',modelo.window)\n",
        "    print('- epochs',modelo.epochs)\n",
        "    print('- alpha',modelo.alpha)\n",
        "    print('- min_count:',modelo.min_count)\n",
        "    print('')\n",
        "\n",
        "      # print('')\n",
        "    print('\\n'+'---'+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfKYSS8571j4",
        "outputId": "6129036e-0c8a-4c63-c3ef-7412b31d68d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Incremental\n",
            "### WOKE HST MODELO 1\n",
            "#### WOKE_1_HST_2003_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 167\n",
            "- Quantidade de frases: 442.636\n",
            "- Quantidade de tokens no corpus usado no treino: 5.548.793\n",
            "- Quantidade de palavras que entraram para o treinamento: 4.760.532\n",
            "- Quantidade de tokens no vocabulário: 11.465\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_HST_2011_2013_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 249\n",
            "- Quantidade de frases: 243.348\n",
            "- Quantidade de tokens no corpus usado no treino: 3.100.347\n",
            "- Quantidade de palavras que entraram para o treinamento: 7.386.474\n",
            "- Quantidade de tokens no vocabulário: 12.206\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_HST_2014_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 344\n",
            "- Quantidade de frases: 305.802\n",
            "- Quantidade de tokens no corpus usado no treino: 4.011.264\n",
            "- Quantidade de palavras que entraram para o treinamento: 10.835.615\n",
            "- Quantidade de tokens no vocabulário: 13.278\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_HST_2017_2019_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 445\n",
            "- Quantidade de frases: 336.958\n",
            "- Quantidade de tokens no corpus usado no treino: 4.551.670\n",
            "- Quantidade de palavras que entraram para o treinamento: 14.722.910\n",
            "- Quantidade de tokens no vocabulário: 14.539\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_HST_2020_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 555\n",
            "- Quantidade de frases: 355.330\n",
            "- Quantidade de tokens no corpus usado no treino: 4.895.643\n",
            "- Quantidade de palavras que entraram para o treinamento: 18.890.441\n",
            "- Quantidade de tokens no vocabulário: 16.020\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE HST MODELO 2\n",
            "#### WOKE_2_HST_2003_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 167\n",
            "- Quantidade de frases: 442.636\n",
            "- Quantidade de tokens no corpus usado no treino: 5.548.793\n",
            "- Quantidade de palavras que entraram para o treinamento: 4.660.133\n",
            "- Quantidade de tokens no vocabulário: 9.514\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_HST_2011_2013_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 249\n",
            "- Quantidade de frases: 243.348\n",
            "- Quantidade de tokens no corpus usado no treino: 3.100.347\n",
            "- Quantidade de palavras que entraram para o treinamento: 7.224.898\n",
            "- Quantidade de tokens no vocabulário: 10.064\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_HST_2014_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 344\n",
            "- Quantidade de frases: 305.802\n",
            "- Quantidade de tokens no corpus usado no treino: 4.011.264\n",
            "- Quantidade de palavras que entraram para o treinamento: 10.598.618\n",
            "- Quantidade de tokens no vocabulário: 10.829\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_HST_2017_2019_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 445\n",
            "- Quantidade de frases: 336.958\n",
            "- Quantidade de tokens no corpus usado no treino: 4.551.670\n",
            "- Quantidade de palavras que entraram para o treinamento: 14.402.473\n",
            "- Quantidade de tokens no vocabulário: 11.801\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_HST_2020_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 555\n",
            "- Quantidade de frases: 355.330\n",
            "- Quantidade de tokens no corpus usado no treino: 4.895.643\n",
            "- Quantidade de palavras que entraram para o treinamento: 18.481.091\n",
            "- Quantidade de tokens no vocabulário: 12.962\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE HST MODELO 3\n",
            "#### WOKE_3_HST_2003_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 167\n",
            "- Quantidade de frases: 442.636\n",
            "- Quantidade de tokens no corpus usado no treino: 5.548.793\n",
            "- Quantidade de palavras que entraram para o treinamento: 4.760.532\n",
            "- Quantidade de tokens no vocabulário: 11.465\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 5\n",
            "- window: 8\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_HST_2011_2013_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 249\n",
            "- Quantidade de frases: 243.348\n",
            "- Quantidade de tokens no corpus usado no treino: 3.100.347\n",
            "- Quantidade de palavras que entraram para o treinamento: 7.386.474\n",
            "- Quantidade de tokens no vocabulário: 12.206\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 5\n",
            "- window: 8\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_HST_2014_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 344\n",
            "- Quantidade de frases: 305.802\n",
            "- Quantidade de tokens no corpus usado no treino: 4.011.264\n",
            "- Quantidade de palavras que entraram para o treinamento: 10.835.615\n",
            "- Quantidade de tokens no vocabulário: 13.278\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 5\n",
            "- window: 8\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_HST_2017_2019_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 445\n",
            "- Quantidade de frases: 336.958\n",
            "- Quantidade de tokens no corpus usado no treino: 4.551.670\n",
            "- Quantidade de palavras que entraram para o treinamento: 14.722.910\n",
            "- Quantidade de tokens no vocabulário: 14.539\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 5\n",
            "- window: 8\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_HST_2020_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 555\n",
            "- Quantidade de frases: 355.330\n",
            "- Quantidade de tokens no corpus usado no treino: 4.895.643\n",
            "- Quantidade de palavras que entraram para o treinamento: 18.890.441\n",
            "- Quantidade de tokens no vocabulário: 16.020\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 5\n",
            "- window: 8\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE HST MODELO 4\n",
            "#### WOKE_4_HST_2003_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 167\n",
            "- Quantidade de frases: 442.636\n",
            "- Quantidade de tokens no corpus usado no treino: 5.548.793\n",
            "- Quantidade de palavras que entraram para o treinamento: 4.883.691\n",
            "- Quantidade de tokens no vocabulário: 14.878\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_HST_2011_2013_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 249\n",
            "- Quantidade de frases: 243.348\n",
            "- Quantidade de tokens no corpus usado no treino: 3.100.347\n",
            "- Quantidade de palavras que entraram para o treinamento: 7.583.838\n",
            "- Quantidade de tokens no vocabulário: 16.027\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_HST_2014_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 344\n",
            "- Quantidade de frases: 305.802\n",
            "- Quantidade de tokens no corpus usado no treino: 4.011.264\n",
            "- Quantidade de palavras que entraram para o treinamento: 11.124.215\n",
            "- Quantidade de tokens no vocabulário: 17.670\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_HST_2017_2019_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 445\n",
            "- Quantidade de frases: 336.958\n",
            "- Quantidade de tokens no corpus usado no treino: 4.551.670\n",
            "- Quantidade de palavras que entraram para o treinamento: 15.113.971\n",
            "- Quantidade de tokens no vocabulário: 19.486\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_HST_2020_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 555\n",
            "- Quantidade de frases: 355.330\n",
            "- Quantidade de tokens no corpus usado no treino: 4.895.643\n",
            "- Quantidade de palavras que entraram para o treinamento: 19.389.677\n",
            "- Quantidade de tokens no vocabulário: 21.650\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HST tmp"
      ],
      "metadata": {
        "id": "yq4pGJ0NGx41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_de_colecoes = ['Historia']\n",
        "\n",
        "tipo_treino = 'Treinamento temporal'\n",
        "\n",
        "treino = 'HST-03-10'\n",
        "\n",
        "caminho_treino_incremental = f'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/{treino}/Com RE/{tipo_treino}'\n",
        "\n",
        "print('## Temporal')\n",
        "for caminho_modelo_inc in sorted([os.path.join(caminho_treino_incremental,modelo) for modelo in os.listdir(caminho_treino_incremental) if '.' not in modelo]):\n",
        "\n",
        "  print(f'### WOKE HST {os.path.basename(caminho_modelo_inc).upper()}')\n",
        "  # c = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1'\n",
        "\n",
        "  for caminho_modelo in sorted([os.path.join(caminho_modelo_inc,modelo) for modelo in os.listdir(caminho_modelo_inc) if modelo.endswith('.model')]):\n",
        "  # caminho_modelo = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1/WOKE_1_HST_2003_2010_w2v_inc.model'\n",
        "\n",
        "    modelo = Word2Vec.load(caminho_modelo)\n",
        "\n",
        "    ano_final = os.path.basename(caminho_modelo)[-18:-14]\n",
        "    print(f'#### {os.path.basename(caminho_modelo).replace(\".model\",\"\")}')\n",
        "    print('##### Contagens')\n",
        "    qtd_trabalhos = 0\n",
        "    for caminho_colecao in sorted([os.path.join(caminho_colecoes,colecao) for colecao in os.listdir(caminho_colecoes) if '.' not in colecao and colecao in lista_de_colecoes]):\n",
        "      for caminho_ano in sorted([os.path.join(caminho_colecao,ano) for ano in os.listdir(caminho_colecao) if ano.isdigit() and ano in [str(a) for a in range(2003,int(ano_final)+1)]]):\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "        for caminho_trabalho in [os.path.join(caminho_ano,trabalho) for trabalho in os.listdir(caminho_ano) if trabalho.startswith('Trabalho')]:\n",
        "          if os.path.exists(os.path.join(caminho_trabalho,'pre_processamento_c_re.msgpack')):\n",
        "            qtd_trabalhos += 1\n",
        "      # if os.path.basename(caminho_ano) in ['2010','2013','2016','2019','2023','2024']:\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "    print('- Quantidade de trabalhos contemplados:','{0:,}'.format(qtd_trabalhos).replace(',','.'))\n",
        "    print('- Quantidade de frases:','{0:,}'.format(modelo.corpus_count).replace(',','.'))\n",
        "    print('- Quantidade de tokens no corpus usado no treino:','{0:,}'.format(modelo.corpus_total_words).replace(',','.'))\n",
        "    print('- Quantidade de palavras que entraram para o treinamento:','{0:,}'.format(sum(modelo.wv.get_vecattr(word, 'count') for word in modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('- Quantidade de tokens no vocabulário:','{0:,}'.format(len(modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('###### Parâmetros')\n",
        "    if modelo.sg == 1:\n",
        "      print('- Modo: Skip-Gram')\n",
        "    else:\n",
        "      print('- Modo: CBOW')\n",
        "    print('- vector_size:',modelo.vector_size)\n",
        "    print('- negative:',modelo.negative)\n",
        "    print('- window:',modelo.window)\n",
        "    print('- epochs',modelo.epochs)\n",
        "    print('- alpha',modelo.alpha)\n",
        "    print('- min_count:',modelo.min_count)\n",
        "    print('')\n",
        "\n",
        "      # print('')\n",
        "    print('\\n'+'---'+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syj_FhEOGyzE",
        "outputId": "f2385bcf-1502-467d-b968-ad44521e0b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Temporal\n",
            "### WOKE HST MODELO 1\n",
            "#### WOKE_1_HST_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 167\n",
            "- Quantidade de frases: 442.636\n",
            "- Quantidade de tokens no corpus usado no treino: 5.548.793\n",
            "- Quantidade de palavras que entraram para o treinamento: 4.760.532\n",
            "- Quantidade de tokens no vocabulário: 11.465\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_HST_2003_2013_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 249\n",
            "- Quantidade de frases: 685.984\n",
            "- Quantidade de tokens no corpus usado no treino: 8.649.140\n",
            "- Quantidade de palavras que entraram para o treinamento: 7.576.514\n",
            "- Quantidade de tokens no vocabulário: 15.352\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_HST_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 344\n",
            "- Quantidade de frases: 991.786\n",
            "- Quantidade de tokens no corpus usado no treino: 12.660.404\n",
            "- Quantidade de palavras que entraram para o treinamento: 11.275.737\n",
            "- Quantidade de tokens no vocabulário: 19.327\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_HST_2003_2019_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 445\n",
            "- Quantidade de frases: 1.328.744\n",
            "- Quantidade de tokens no corpus usado no treino: 17.212.074\n",
            "- Quantidade de palavras que entraram para o treinamento: 15.466.865\n",
            "- Quantidade de tokens no vocabulário: 23.511\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_HST_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 555\n",
            "- Quantidade de frases: 1.684.074\n",
            "- Quantidade de tokens no corpus usado no treino: 22.107.717\n",
            "- Quantidade de palavras que entraram para o treinamento: 19.957.792\n",
            "- Quantidade de tokens no vocabulário: 27.647\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE HST MODELO 2\n",
            "#### WOKE_2_HST_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 167\n",
            "- Quantidade de frases: 442.636\n",
            "- Quantidade de tokens no corpus usado no treino: 5.548.793\n",
            "- Quantidade de palavras que entraram para o treinamento: 4.660.133\n",
            "- Quantidade de tokens no vocabulário: 9.514\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_HST_2003_2013_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 249\n",
            "- Quantidade de frases: 685.984\n",
            "- Quantidade de tokens no corpus usado no treino: 8.649.140\n",
            "- Quantidade de palavras que entraram para o treinamento: 7.445.282\n",
            "- Quantidade de tokens no vocabulário: 12.802\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_HST_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 344\n",
            "- Quantidade de frases: 991.786\n",
            "- Quantidade de tokens no corpus usado no treino: 12.660.404\n",
            "- Quantidade de palavras que entraram para o treinamento: 11.114.269\n",
            "- Quantidade de tokens no vocabulário: 16.193\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_HST_2003_2019_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 445\n",
            "- Quantidade de frases: 1.328.744\n",
            "- Quantidade de tokens no corpus usado no treino: 17.212.074\n",
            "- Quantidade de palavras que entraram para o treinamento: 15.266.693\n",
            "- Quantidade de tokens no vocabulário: 19.612\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_HST_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 555\n",
            "- Quantidade de frases: 1.684.074\n",
            "- Quantidade de tokens no corpus usado no treino: 22.107.717\n",
            "- Quantidade de palavras que entraram para o treinamento: 19.721.859\n",
            "- Quantidade de tokens no vocabulário: 23.050\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE HST MODELO 3\n",
            "#### WOKE_3_HST_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 167\n",
            "- Quantidade de frases: 442.636\n",
            "- Quantidade de tokens no corpus usado no treino: 5.548.793\n",
            "- Quantidade de palavras que entraram para o treinamento: 4.760.532\n",
            "- Quantidade de tokens no vocabulário: 11.465\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 5\n",
            "- window: 8\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_HST_2003_2013_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 249\n",
            "- Quantidade de frases: 685.984\n",
            "- Quantidade de tokens no corpus usado no treino: 8.649.140\n",
            "- Quantidade de palavras que entraram para o treinamento: 7.576.514\n",
            "- Quantidade de tokens no vocabulário: 15.352\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 5\n",
            "- window: 8\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_HST_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 344\n",
            "- Quantidade de frases: 991.786\n",
            "- Quantidade de tokens no corpus usado no treino: 12.660.404\n",
            "- Quantidade de palavras que entraram para o treinamento: 11.275.737\n",
            "- Quantidade de tokens no vocabulário: 19.327\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 5\n",
            "- window: 8\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_HST_2003_2019_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 445\n",
            "- Quantidade de frases: 1.328.744\n",
            "- Quantidade de tokens no corpus usado no treino: 17.212.074\n",
            "- Quantidade de palavras que entraram para o treinamento: 15.466.865\n",
            "- Quantidade de tokens no vocabulário: 23.511\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 5\n",
            "- window: 8\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_HST_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 555\n",
            "- Quantidade de frases: 1.684.074\n",
            "- Quantidade de tokens no corpus usado no treino: 22.107.717\n",
            "- Quantidade de palavras que entraram para o treinamento: 19.957.792\n",
            "- Quantidade de tokens no vocabulário: 27.647\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 5\n",
            "- window: 8\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE HST MODELO 4\n",
            "#### WOKE_4_HST_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 167\n",
            "- Quantidade de frases: 442.636\n",
            "- Quantidade de tokens no corpus usado no treino: 5.548.793\n",
            "- Quantidade de palavras que entraram para o treinamento: 4.883.691\n",
            "- Quantidade de tokens no vocabulário: 14.878\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_HST_2003_2013_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 249\n",
            "- Quantidade de frases: 685.984\n",
            "- Quantidade de tokens no corpus usado no treino: 8.649.140\n",
            "- Quantidade de palavras que entraram para o treinamento: 7.737.992\n",
            "- Quantidade de tokens no vocabulário: 19.799\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_HST_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 344\n",
            "- Quantidade de frases: 991.786\n",
            "- Quantidade de tokens no corpus usado no treino: 12.660.404\n",
            "- Quantidade de palavras que entraram para o treinamento: 11.482.174\n",
            "- Quantidade de tokens no vocabulário: 25.029\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_HST_2003_2019_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 445\n",
            "- Quantidade de frases: 1.328.744\n",
            "- Quantidade de tokens no corpus usado no treino: 17.212.074\n",
            "- Quantidade de palavras que entraram para o treinamento: 15.714.507\n",
            "- Quantidade de tokens no vocabulário: 30.356\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_HST_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 555\n",
            "- Quantidade de frases: 1.684.074\n",
            "- Quantidade de tokens no corpus usado no treino: 22.107.717\n",
            "- Quantidade de palavras que entraram para o treinamento: 20.248.333\n",
            "- Quantidade de tokens no vocabulário: 35.688\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 100\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CFH"
      ],
      "metadata": {
        "id": "E5EYVoX0Dc1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CFH inc"
      ],
      "metadata": {
        "id": "DLww7HR5Lnnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CFH (somente coleções do Centro de Filosofia e Ciências Humanas)\n",
        "treino = 'CFH-03-10'\n",
        "lista_de_colecoes = ['Filosofia','Geografia','Geologia','Historia','Psicologia','Teses_e_dissertacoes_do_Centro_de_Filosofia_e_Ciencias_Humanas','Programa_de_Pos_Graduacao_Interdisciplinar_em_Ciencias_Humanas','Servico_Social','Sociologia_e_Ciencia_Politica','Sociologia_Politica','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Ensino_de_Historia_Mestrado_Profissional'] # Corpus CFH\n",
        "\n",
        "tipo_treinamento = 'Treinamento incremental'\n",
        "\n",
        "caminho_treino_incremental = f'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/{treino}/Com RE/{tipo_treinamento}'\n",
        "\n",
        "print(f'## Incremental')\n",
        "for caminho_modelo_inc in sorted([os.path.join(caminho_treino_incremental,modelo) for modelo in os.listdir(caminho_treino_incremental) if '.' not in modelo]):\n",
        "\n",
        "  print(f'### WOKE CFH {os.path.basename(caminho_modelo_inc).upper()}')\n",
        "  # c = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1'\n",
        "\n",
        "  for caminho_modelo in sorted([os.path.join(caminho_modelo_inc,modelo) for modelo in os.listdir(caminho_modelo_inc) if modelo.endswith('.model')]):\n",
        "  # caminho_modelo = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1/WOKE_1_HST_2003_2010_w2v_inc.model'\n",
        "\n",
        "    modelo = Word2Vec.load(caminho_modelo)\n",
        "\n",
        "    ano_final = os.path.basename(caminho_modelo)[-18:-14]\n",
        "    print(f'#### {os.path.basename(caminho_modelo).replace(\".model\",\"\")}')\n",
        "    print('##### Contagens')\n",
        "    qtd_trabalhos = 0\n",
        "    for caminho_colecao in sorted([os.path.join(caminho_colecoes,colecao) for colecao in os.listdir(caminho_colecoes) if '.' not in colecao and colecao in lista_de_colecoes]):\n",
        "      for caminho_ano in sorted([os.path.join(caminho_colecao,ano) for ano in os.listdir(caminho_colecao) if ano.isdigit() and ano in [str(a) for a in range(2003,int(ano_final)+1)]]):\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "        for caminho_trabalho in [os.path.join(caminho_ano,trabalho) for trabalho in os.listdir(caminho_ano) if trabalho.startswith('Trabalho')]:\n",
        "          if os.path.exists(os.path.join(caminho_trabalho,'pre_processamento_c_re.msgpack')):\n",
        "            qtd_trabalhos += 1\n",
        "      # if os.path.basename(caminho_ano) in ['2010','2013','2016','2019','2023','2024']:\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "    print('- Quantidade de trabalhos contemplados:','{0:,}'.format(qtd_trabalhos).replace(',','.'))\n",
        "    print('- Quantidade de frases:','{0:,}'.format(modelo.corpus_count).replace(',','.'))\n",
        "    print('- Quantidade de tokens no corpus usado no treino:','{0:,}'.format(modelo.corpus_total_words).replace(',','.'))\n",
        "    print('- Quantidade de palavras que entraram para o treinamento:','{0:,}'.format(sum(modelo.wv.get_vecattr(word, 'count') for word in modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('- Quantidade de tokens no vocabulário:','{0:,}'.format(len(modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('###### Parâmetros')\n",
        "    if modelo.sg == 1:\n",
        "      print('- Modo: Skip-Gram')\n",
        "    else:\n",
        "      print('- Modo: CBOW')\n",
        "    print('- vector_size:',modelo.vector_size)\n",
        "    print('- negative:',modelo.negative)\n",
        "    print('- window:',modelo.window)\n",
        "    print('- epochs',modelo.epochs)\n",
        "    print('- alpha',modelo.alpha)\n",
        "    print('- min_count:',modelo.min_count)\n",
        "    print('')\n",
        "\n",
        "      # print('')\n",
        "    print('\\n'+'---'+'\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD5dbGAKDdvS",
        "outputId": "ed433f95-958a-451a-a334-b0af5205aceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Incremental\n",
            "### WOKE CFH MODELO 1\n",
            "#### WOKE_1_CFH_2003_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.074\n",
            "- Quantidade de frases: 2.354.048\n",
            "- Quantidade de tokens no corpus usado no treino: 34.137.504\n",
            "- Quantidade de palavras que entraram para o treinamento: 31.962.981\n",
            "- Quantidade de tokens no vocabulário: 41.628\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_CFH_2011_2013_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.608\n",
            "- Quantidade de frases: 1.191.385\n",
            "- Quantidade de tokens no corpus usado no treino: 17.669.890\n",
            "- Quantidade de palavras que entraram para o treinamento: 48.473.069\n",
            "- Quantidade de tokens no vocabulário: 44.563\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_CFH_2014_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.217\n",
            "- Quantidade de frases: 1.399.659\n",
            "- Quantidade de tokens no corpus usado no treino: 20.441.299\n",
            "- Quantidade de palavras que entraram para o treinamento: 67.510.694\n",
            "- Quantidade de tokens no vocabulário: 48.331\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_CFH_2017_2019_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.888\n",
            "- Quantidade de frases: 1.538.214\n",
            "- Quantidade de tokens no corpus usado no treino: 22.951.565\n",
            "- Quantidade de palavras que entraram para o treinamento: 88.948.727\n",
            "- Quantidade de tokens no vocabulário: 52.545\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_CFH_2020_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.554\n",
            "- Quantidade de frases: 1.502.951\n",
            "- Quantidade de tokens no corpus usado no treino: 22.512.208\n",
            "- Quantidade de palavras que entraram para o treinamento: 109.806.862\n",
            "- Quantidade de tokens no vocabulário: 57.032\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE CFH MODELO 2\n",
            "#### WOKE_2_CFH_2003_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.074\n",
            "- Quantidade de frases: 2.354.048\n",
            "- Quantidade de tokens no corpus usado no treino: 34.137.504\n",
            "- Quantidade de palavras que entraram para o treinamento: 31.631.016\n",
            "- Quantidade de tokens no vocabulário: 32.441\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_CFH_2011_2013_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.608\n",
            "- Quantidade de frases: 1.191.385\n",
            "- Quantidade de tokens no corpus usado no treino: 17.669.890\n",
            "- Quantidade de palavras que entraram para o treinamento: 47.968.421\n",
            "- Quantidade de tokens no vocabulário: 34.396\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_CFH_2014_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.217\n",
            "- Quantidade de frases: 1.399.659\n",
            "- Quantidade de tokens no corpus usado no treino: 20.441.299\n",
            "- Quantidade de palavras que entraram para o treinamento: 66.796.682\n",
            "- Quantidade de tokens no vocabulário: 36.894\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_CFH_2017_2019_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.888\n",
            "- Quantidade de frases: 1.538.214\n",
            "- Quantidade de tokens no corpus usado no treino: 22.951.565\n",
            "- Quantidade de palavras que entraram para o treinamento: 88.003.531\n",
            "- Quantidade de tokens no vocabulário: 39.709\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_CFH_2020_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.554\n",
            "- Quantidade de frases: 1.502.951\n",
            "- Quantidade de tokens no corpus usado no treino: 22.512.208\n",
            "- Quantidade de palavras que entraram para o treinamento: 108.622.050\n",
            "- Quantidade de tokens no vocabulário: 42.681\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE CFH MODELO 3\n",
            "#### WOKE_3_CFH_2003_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.074\n",
            "- Quantidade de frases: 2.354.048\n",
            "- Quantidade de tokens no corpus usado no treino: 34.137.504\n",
            "- Quantidade de palavras que entraram para o treinamento: 32.249.400\n",
            "- Quantidade de tokens no vocabulário: 53.592\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 20\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_CFH_2011_2013_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.608\n",
            "- Quantidade de frases: 1.191.385\n",
            "- Quantidade de tokens no corpus usado no treino: 17.669.890\n",
            "- Quantidade de palavras que entraram para o treinamento: 48.908.235\n",
            "- Quantidade de tokens no vocabulário: 58.091\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 20\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_CFH_2014_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.217\n",
            "- Quantidade de frases: 1.399.659\n",
            "- Quantidade de tokens no corpus usado no treino: 20.441.299\n",
            "- Quantidade de palavras que entraram para o treinamento: 68.131.281\n",
            "- Quantidade de tokens no vocabulário: 63.899\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 20\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_CFH_2017_2019_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.888\n",
            "- Quantidade de frases: 1.538.214\n",
            "- Quantidade de tokens no corpus usado no treino: 22.951.565\n",
            "- Quantidade de palavras que entraram para o treinamento: 89.770.760\n",
            "- Quantidade de tokens no vocabulário: 70.237\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 20\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_CFH_2020_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.554\n",
            "- Quantidade de frases: 1.502.951\n",
            "- Quantidade de tokens no corpus usado no treino: 22.512.208\n",
            "- Quantidade de palavras que entraram para o treinamento: 110.836.691\n",
            "- Quantidade de tokens no vocabulário: 76.964\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 20\n",
            "\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CFH tmp"
      ],
      "metadata": {
        "id": "IvqAxYAML6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CFH (somente coleções do Centro de Filosofia e Ciências Humanas)\n",
        "treino = 'CFH-03-10'\n",
        "lista_de_colecoes = ['Filosofia','Geografia','Geologia','Historia','Psicologia','Teses_e_dissertacoes_do_Centro_de_Filosofia_e_Ciencias_Humanas','Programa_de_Pos_Graduacao_Interdisciplinar_em_Ciencias_Humanas','Servico_Social','Sociologia_e_Ciencia_Politica','Sociologia_Politica','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Ensino_de_Historia_Mestrado_Profissional'] # Corpus CFH\n",
        "\n",
        "caminho_treino_incremental = f'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/{treino}/Com RE/Treinamento temporal'\n",
        "\n",
        "print('## Temporal')\n",
        "for caminho_modelo_inc in sorted([os.path.join(caminho_treino_incremental,modelo) for modelo in os.listdir(caminho_treino_incremental) if '.' not in modelo]):\n",
        "\n",
        "  print(f'### WOKE CFH {os.path.basename(caminho_modelo_inc).upper()}')\n",
        "  # c = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1'\n",
        "\n",
        "  for caminho_modelo in sorted([os.path.join(caminho_modelo_inc,modelo) for modelo in os.listdir(caminho_modelo_inc) if modelo.endswith('.model')]):\n",
        "  # caminho_modelo = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1/WOKE_1_HST_2003_2010_w2v_inc.model'\n",
        "\n",
        "    modelo = Word2Vec.load(caminho_modelo)\n",
        "\n",
        "    ano_final = os.path.basename(caminho_modelo)[-18:-14]\n",
        "    print(f'#### {os.path.basename(caminho_modelo).replace(\".model\",\"\")}')\n",
        "    print('##### Contagens')\n",
        "    qtd_trabalhos = 0\n",
        "    for caminho_colecao in sorted([os.path.join(caminho_colecoes,colecao) for colecao in os.listdir(caminho_colecoes) if '.' not in colecao and colecao in lista_de_colecoes]):\n",
        "      for caminho_ano in sorted([os.path.join(caminho_colecao,ano) for ano in os.listdir(caminho_colecao) if ano.isdigit() and ano in [str(a) for a in range(2003,int(ano_final)+1)]]):\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "        for caminho_trabalho in [os.path.join(caminho_ano,trabalho) for trabalho in os.listdir(caminho_ano) if trabalho.startswith('Trabalho')]:\n",
        "          if os.path.exists(os.path.join(caminho_trabalho,'pre_processamento_c_re.msgpack')):\n",
        "            qtd_trabalhos += 1\n",
        "      # if os.path.basename(caminho_ano) in ['2010','2013','2016','2019','2023','2024']:\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "    print('- Quantidade de trabalhos contemplados:','{0:,}'.format(qtd_trabalhos).replace(',','.'))\n",
        "    print('- Quantidade de frases:','{0:,}'.format(modelo.corpus_count).replace(',','.'))\n",
        "    print('- Quantidade de tokens no corpus usado no treino:','{0:,}'.format(modelo.corpus_total_words).replace(',','.'))\n",
        "    print('- Quantidade de palavras que entraram para o treinamento:','{0:,}'.format(sum(modelo.wv.get_vecattr(word, 'count') for word in modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('- Quantidade de tokens no vocabulário:','{0:,}'.format(len(modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('###### Parâmetros')\n",
        "    if modelo.sg == 1:\n",
        "      print('- Modo: Skip-Gram')\n",
        "    else:\n",
        "      print('- Modo: CBOW')\n",
        "    print('- vector_size:',modelo.vector_size)\n",
        "    print('- negative:',modelo.negative)\n",
        "    print('- window:',modelo.window)\n",
        "    print('- epochs',modelo.epochs)\n",
        "    print('- alpha',modelo.alpha)\n",
        "    print('- min_count:',modelo.min_count)\n",
        "    print('')\n",
        "\n",
        "      # print('')\n",
        "    print('\\n'+'---'+'\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdtBTTiZL7eL",
        "outputId": "456ca7da-3c74-44f5-fc66-57eaedc7c487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Temporal\n",
            "### WOKE CFH MODELO 1\n",
            "#### WOKE_1_CFH_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.074\n",
            "- Quantidade de frases: 2.354.048\n",
            "- Quantidade de tokens no corpus usado no treino: 34.137.504\n",
            "- Quantidade de palavras que entraram para o treinamento: 31.962.981\n",
            "- Quantidade de tokens no vocabulário: 41.628\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_CFH_2003_2013_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.608\n",
            "- Quantidade de frases: 3.545.433\n",
            "- Quantidade de tokens no corpus usado no treino: 51.807.394\n",
            "- Quantidade de palavras que entraram para o treinamento: 48.830.872\n",
            "- Quantidade de tokens no vocabulário: 53.418\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_CFH_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.217\n",
            "- Quantidade de frases: 4.945.092\n",
            "- Quantidade de tokens no corpus usado no treino: 72.248.693\n",
            "- Quantidade de palavras que entraram para o treinamento: 68.351.277\n",
            "- Quantidade de tokens no vocabulário: 65.992\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_CFH_2003_2019_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.888\n",
            "- Quantidade de frases: 6.483.306\n",
            "- Quantidade de tokens no corpus usado no treino: 95.200.258\n",
            "- Quantidade de palavras que entraram para o treinamento: 90.343.370\n",
            "- Quantidade de tokens no vocabulário: 78.479\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_CFH_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.554\n",
            "- Quantidade de frases: 7.986.257\n",
            "- Quantidade de tokens no corpus usado no treino: 117.712.466\n",
            "- Quantidade de palavras que entraram para o treinamento: 111.787.322\n",
            "- Quantidade de tokens no vocabulário: 90.891\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE CFH MODELO 2\n",
            "#### WOKE_2_CFH_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.074\n",
            "- Quantidade de frases: 2.354.048\n",
            "- Quantidade de tokens no corpus usado no treino: 34.137.504\n",
            "- Quantidade de palavras que entraram para o treinamento: 31.631.016\n",
            "- Quantidade de tokens no vocabulário: 32.441\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_CFH_2003_2013_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.608\n",
            "- Quantidade de frases: 3.545.433\n",
            "- Quantidade de tokens no corpus usado no treino: 51.807.394\n",
            "- Quantidade de palavras que entraram para o treinamento: 48.404.895\n",
            "- Quantidade de tokens no vocabulário: 41.650\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_CFH_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.217\n",
            "- Quantidade de frases: 4.945.092\n",
            "- Quantidade de tokens no corpus usado no treino: 72.248.693\n",
            "- Quantidade de palavras que entraram para o treinamento: 67.823.884\n",
            "- Quantidade de tokens no vocabulário: 51.415\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_CFH_2003_2019_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.888\n",
            "- Quantidade de frases: 6.483.306\n",
            "- Quantidade de tokens no corpus usado no treino: 95.200.258\n",
            "- Quantidade de palavras que entraram para o treinamento: 89.708.563\n",
            "- Quantidade de tokens no vocabulário: 60.901\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_CFH_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.554\n",
            "- Quantidade de frases: 7.986.257\n",
            "- Quantidade de tokens no corpus usado no treino: 117.712.466\n",
            "- Quantidade de palavras que entraram para o treinamento: 111.037.755\n",
            "- Quantidade de tokens no vocabulário: 70.118\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE CFH MODELO 3\n",
            "#### WOKE_3_CFH_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.074\n",
            "- Quantidade de frases: 2.354.048\n",
            "- Quantidade de tokens no corpus usado no treino: 34.137.504\n",
            "- Quantidade de palavras que entraram para o treinamento: 32.249.400\n",
            "- Quantidade de tokens no vocabulário: 53.592\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 20\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_CFH_2003_2013_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.608\n",
            "- Quantidade de frases: 3.545.433\n",
            "- Quantidade de tokens no corpus usado no treino: 51.807.394\n",
            "- Quantidade de palavras que entraram para o treinamento: 49.204.283\n",
            "- Quantidade de tokens no vocabulário: 69.026\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 20\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_CFH_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.217\n",
            "- Quantidade de frases: 4.945.092\n",
            "- Quantidade de tokens no corpus usado no treino: 72.248.693\n",
            "- Quantidade de palavras que entraram para o treinamento: 68.823.075\n",
            "- Quantidade de tokens no vocabulário: 85.736\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 20\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_CFH_2003_2019_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.888\n",
            "- Quantidade de frases: 6.483.306\n",
            "- Quantidade de tokens no corpus usado no treino: 95.200.258\n",
            "- Quantidade de palavras que entraram para o treinamento: 90.917.767\n",
            "- Quantidade de tokens no vocabulário: 102.478\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 20\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_CFH_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.554\n",
            "- Quantidade de frases: 7.986.257\n",
            "- Quantidade de tokens no corpus usado no treino: 117.712.466\n",
            "- Quantidade de palavras que entraram para o treinamento: 112.464.060\n",
            "- Quantidade de tokens no vocabulário: 119.178\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 20\n",
            "\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAUDE-CORPO"
      ],
      "metadata": {
        "id": "awMMBoosD181"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAUDE-CORPO inc"
      ],
      "metadata": {
        "id": "WP935XI4MWc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "treino = 'SAUDE-CORPO-03-10'\n",
        "# SAUDE-CORPO (somente coleções voltadas para saúde e corpo num geral)\n",
        "lista_de_colecoes = ['Biologia_Celular_e_do_Desenvolvimento','Biotecnologia_e_Biociencias','Ciencias_da_Reabilitacao','Ciencias_Medicas','Cuidados_Intensivos_e_Paliativos_Mestrado_Profissional',\n",
        "                     'Educacao_Fisica','Enfermagem','Gestao_do_Cuidado_em_Enfermagem','Gestao_do_Cuidado_em_Enfermagem_Mestrado_Profissional','Medicina_Veterinaria_Convencional_e_Integrativa',\n",
        "                     'Neurociencias','Saude_Coletiva','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Saude_Publica','Programa_de_Pos_Graduacao_Multidisciplinar_em_Saude_Mestrado_Profissional'] # Corpus SAUDE-CORPO\n",
        "\n",
        "tipo_treino = 'Treinamento incremental'\n",
        "\n",
        "caminho_treino_incremental = f'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/{treino}/Com RE/{tipo_treino}'\n",
        "\n",
        "print('## Incremental')\n",
        "for caminho_modelo_inc in sorted([os.path.join(caminho_treino_incremental,modelo) for modelo in os.listdir(caminho_treino_incremental) if '.' not in modelo]):\n",
        "\n",
        "  print(f'### WOKE SAUDE-CORPO {os.path.basename(caminho_modelo_inc).upper()}')\n",
        "  # c = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1'\n",
        "\n",
        "  for caminho_modelo in sorted([os.path.join(caminho_modelo_inc,modelo) for modelo in os.listdir(caminho_modelo_inc) if modelo.endswith('.model')]):\n",
        "  # caminho_modelo = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1/WOKE_1_HST_2003_2010_w2v_inc.model'\n",
        "\n",
        "    modelo = Word2Vec.load(caminho_modelo)\n",
        "\n",
        "    ano_final = os.path.basename(caminho_modelo)[-18:-14]\n",
        "    print(f'#### {os.path.basename(caminho_modelo).replace(\".model\",\"\")}')\n",
        "    print('##### Contagens')\n",
        "    qtd_trabalhos = 0\n",
        "    for caminho_colecao in sorted([os.path.join(caminho_colecoes,colecao) for colecao in os.listdir(caminho_colecoes) if '.' not in colecao and colecao in lista_de_colecoes]):\n",
        "      for caminho_ano in sorted([os.path.join(caminho_colecao,ano) for ano in os.listdir(caminho_colecao) if ano.isdigit() and ano in [str(a) for a in range(2003,int(ano_final)+1)]]):\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "        for caminho_trabalho in [os.path.join(caminho_ano,trabalho) for trabalho in os.listdir(caminho_ano) if trabalho.startswith('Trabalho')]:\n",
        "          if os.path.exists(os.path.join(caminho_trabalho,'pre_processamento_c_re.msgpack')):\n",
        "            qtd_trabalhos += 1\n",
        "      # if os.path.basename(caminho_ano) in ['2010','2013','2016','2019','2023','2024']:\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "    print('- Quantidade de trabalhos contemplados:','{0:,}'.format(qtd_trabalhos).replace(',','.'))\n",
        "    print('- Quantidade de frases:','{0:,}'.format(modelo.corpus_count).replace(',','.'))\n",
        "    print('- Quantidade de tokens no corpus usado no treino:','{0:,}'.format(modelo.corpus_total_words).replace(',','.'))\n",
        "    print('- Quantidade de palavras que entraram para o treinamento:','{0:,}'.format(sum(modelo.wv.get_vecattr(word, 'count') for word in modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('- Quantidade de tokens no vocabulário:','{0:,}'.format(len(modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('###### Parâmetros')\n",
        "    if modelo.sg == 1:\n",
        "      print('- Modo: Skip-Gram')\n",
        "    else:\n",
        "      print('- Modo: CBOW')\n",
        "    print('- vector_size:',modelo.vector_size)\n",
        "    print('- negative:',modelo.negative)\n",
        "    print('- window:',modelo.window)\n",
        "    print('- epochs',modelo.epochs)\n",
        "    print('- alpha',modelo.alpha)\n",
        "    print('- min_count:',modelo.min_count)\n",
        "    print('')\n",
        "\n",
        "      # print('')\n",
        "    print('\\n'+'---'+'\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg14iqpQD3FN",
        "outputId": "d338b647-2eda-4a52-d686-ac837f1a349b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Incremental\n",
            "### WOKE SAUDE-CORPO MODELO 1\n",
            "#### WOKE_1_SAUDE-CORPO_2003_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 691\n",
            "- Quantidade de frases: 947.942\n",
            "- Quantidade de tokens no corpus usado no treino: 13.614.492\n",
            "- Quantidade de palavras que entraram para o treinamento: 12.740.967\n",
            "- Quantidade de tokens no vocabulário: 35.655\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_SAUDE-CORPO_2011_2013_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.134\n",
            "- Quantidade de frases: 524.569\n",
            "- Quantidade de tokens no corpus usado no treino: 7.419.591\n",
            "- Quantidade de palavras que entraram para o treinamento: 19.588.648\n",
            "- Quantidade de tokens no vocabulário: 40.711\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_SAUDE-CORPO_2014_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.760\n",
            "- Quantidade de frases: 810.178\n",
            "- Quantidade de tokens no corpus usado no treino: 11.357.632\n",
            "- Quantidade de palavras que entraram para o treinamento: 30.101.935\n",
            "- Quantidade de tokens no vocabulário: 48.728\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_SAUDE-CORPO_2017_2019_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.380\n",
            "- Quantidade de frases: 783.823\n",
            "- Quantidade de tokens no corpus usado no treino: 11.007.741\n",
            "- Quantidade de palavras que entraram para o treinamento: 40.234.412\n",
            "- Quantidade de tokens no vocabulário: 56.172\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_SAUDE-CORPO_2020_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.054\n",
            "- Quantidade de frases: 839.403\n",
            "- Quantidade de tokens no corpus usado no treino: 11.684.040\n",
            "- Quantidade de palavras que entraram para o treinamento: 50.967.179\n",
            "- Quantidade de tokens no vocabulário: 62.939\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE SAUDE-CORPO MODELO 2\n",
            "#### WOKE_2_SAUDE-CORPO_2003_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 691\n",
            "- Quantidade de frases: 947.942\n",
            "- Quantidade de tokens no corpus usado no treino: 13.614.492\n",
            "- Quantidade de palavras que entraram para o treinamento: 12.740.967\n",
            "- Quantidade de tokens no vocabulário: 35.655\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_SAUDE-CORPO_2011_2013_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.134\n",
            "- Quantidade de frases: 524.569\n",
            "- Quantidade de tokens no corpus usado no treino: 7.419.591\n",
            "- Quantidade de palavras que entraram para o treinamento: 19.588.648\n",
            "- Quantidade de tokens no vocabulário: 40.711\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_SAUDE-CORPO_2014_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.760\n",
            "- Quantidade de frases: 810.178\n",
            "- Quantidade de tokens no corpus usado no treino: 11.357.632\n",
            "- Quantidade de palavras que entraram para o treinamento: 30.101.935\n",
            "- Quantidade de tokens no vocabulário: 48.728\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_SAUDE-CORPO_2017_2019_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.380\n",
            "- Quantidade de frases: 783.823\n",
            "- Quantidade de tokens no corpus usado no treino: 11.007.741\n",
            "- Quantidade de palavras que entraram para o treinamento: 40.234.412\n",
            "- Quantidade de tokens no vocabulário: 56.172\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_SAUDE-CORPO_2020_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.054\n",
            "- Quantidade de frases: 839.403\n",
            "- Quantidade de tokens no corpus usado no treino: 11.684.040\n",
            "- Quantidade de palavras que entraram para o treinamento: 50.967.179\n",
            "- Quantidade de tokens no vocabulário: 62.939\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE SAUDE-CORPO MODELO 3\n",
            "#### WOKE_3_SAUDE-CORPO_2003_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 691\n",
            "- Quantidade de frases: 947.942\n",
            "- Quantidade de tokens no corpus usado no treino: 13.614.492\n",
            "- Quantidade de palavras que entraram para o treinamento: 12.487.497\n",
            "- Quantidade de tokens no vocabulário: 23.354\n",
            "###### Parâmetros\n",
            "- Modo: CBOW\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_SAUDE-CORPO_2011_2013_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.134\n",
            "- Quantidade de frases: 524.569\n",
            "- Quantidade de tokens no corpus usado no treino: 7.419.591\n",
            "- Quantidade de palavras que entraram para o treinamento: 19.172.566\n",
            "- Quantidade de tokens no vocabulário: 25.798\n",
            "###### Parâmetros\n",
            "- Modo: CBOW\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_SAUDE-CORPO_2014_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.760\n",
            "- Quantidade de frases: 810.178\n",
            "- Quantidade de tokens no corpus usado no treino: 11.357.632\n",
            "- Quantidade de palavras que entraram para o treinamento: 29.452.598\n",
            "- Quantidade de tokens no vocabulário: 29.995\n",
            "###### Parâmetros\n",
            "- Modo: CBOW\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_SAUDE-CORPO_2017_2019_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.380\n",
            "- Quantidade de frases: 783.823\n",
            "- Quantidade de tokens no corpus usado no treino: 11.007.741\n",
            "- Quantidade de palavras que entraram para o treinamento: 39.340.812\n",
            "- Quantidade de tokens no vocabulário: 33.676\n",
            "###### Parâmetros\n",
            "- Modo: CBOW\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_SAUDE-CORPO_2020_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.054\n",
            "- Quantidade de frases: 839.403\n",
            "- Quantidade de tokens no corpus usado no treino: 11.684.040\n",
            "- Quantidade de palavras que entraram para o treinamento: 49.823.408\n",
            "- Quantidade de tokens no vocabulário: 37.020\n",
            "###### Parâmetros\n",
            "- Modo: CBOW\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAUDE-CORPO tmp"
      ],
      "metadata": {
        "id": "jxKCM3j8MYJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "treino = 'SAUDE-CORPO-03-10'\n",
        "# SAUDE-CORPO (somente coleções voltadas para saúde e corpo num geral)\n",
        "lista_de_colecoes = ['Biologia_Celular_e_do_Desenvolvimento','Biotecnologia_e_Biociencias','Ciencias_da_Reabilitacao','Ciencias_Medicas','Cuidados_Intensivos_e_Paliativos_Mestrado_Profissional',\n",
        "                     'Educacao_Fisica','Enfermagem','Gestao_do_Cuidado_em_Enfermagem','Gestao_do_Cuidado_em_Enfermagem_Mestrado_Profissional','Medicina_Veterinaria_Convencional_e_Integrativa',\n",
        "                     'Neurociencias','Saude_Coletiva','Saude_Mental_e_Atencao_Psicossocial_Mestrado_Profissional','Saude_Publica','Programa_de_Pos_Graduacao_Multidisciplinar_em_Saude_Mestrado_Profissional'] # Corpus SAUDE-CORPO\n",
        "\n",
        "tipo_treino = 'Treinamento temporal'\n",
        "\n",
        "caminho_treino_incremental = f'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/{treino}/Com RE/{tipo_treino}'\n",
        "\n",
        "print('## Temporal')\n",
        "for caminho_modelo_inc in sorted([os.path.join(caminho_treino_incremental,modelo) for modelo in os.listdir(caminho_treino_incremental) if '.' not in modelo]):\n",
        "\n",
        "  print(f'### WOKE SAUDE-CORPO {os.path.basename(caminho_modelo_inc).upper()}')\n",
        "  # c = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1'\n",
        "\n",
        "  for caminho_modelo in sorted([os.path.join(caminho_modelo_inc,modelo) for modelo in os.listdir(caminho_modelo_inc) if modelo.endswith('.model')]):\n",
        "  # caminho_modelo = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1/WOKE_1_HST_2003_2010_w2v_inc.model'\n",
        "\n",
        "    modelo = Word2Vec.load(caminho_modelo)\n",
        "\n",
        "    ano_final = os.path.basename(caminho_modelo)[-18:-14]\n",
        "    print(f'#### {os.path.basename(caminho_modelo).replace(\".model\",\"\")}')\n",
        "    print('##### Contagens')\n",
        "    qtd_trabalhos = 0\n",
        "    for caminho_colecao in sorted([os.path.join(caminho_colecoes,colecao) for colecao in os.listdir(caminho_colecoes) if '.' not in colecao and colecao in lista_de_colecoes]):\n",
        "      for caminho_ano in sorted([os.path.join(caminho_colecao,ano) for ano in os.listdir(caminho_colecao) if ano.isdigit() and ano in [str(a) for a in range(2003,int(ano_final)+1)]]):\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "        for caminho_trabalho in [os.path.join(caminho_ano,trabalho) for trabalho in os.listdir(caminho_ano) if trabalho.startswith('Trabalho')]:\n",
        "          if os.path.exists(os.path.join(caminho_trabalho,'pre_processamento_c_re.msgpack')):\n",
        "            qtd_trabalhos += 1\n",
        "      # if os.path.basename(caminho_ano) in ['2010','2013','2016','2019','2023','2024']:\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "    print('- Quantidade de trabalhos contemplados:','{0:,}'.format(qtd_trabalhos).replace(',','.'))\n",
        "    print('- Quantidade de frases:','{0:,}'.format(modelo.corpus_count).replace(',','.'))\n",
        "    print('- Quantidade de tokens no corpus usado no treino:','{0:,}'.format(modelo.corpus_total_words).replace(',','.'))\n",
        "    print('- Quantidade de palavras que entraram para o treinamento:','{0:,}'.format(sum(modelo.wv.get_vecattr(word, 'count') for word in modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('- Quantidade de tokens no vocabulário:','{0:,}'.format(len(modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('###### Parâmetros')\n",
        "    if modelo.sg == 1:\n",
        "      print('- Modo: Skip-Gram')\n",
        "    else:\n",
        "      print('- Modo: CBOW')\n",
        "    print('- vector_size:',modelo.vector_size)\n",
        "    print('- negative:',modelo.negative)\n",
        "    print('- window:',modelo.window)\n",
        "    print('- epochs',modelo.epochs)\n",
        "    print('- alpha',modelo.alpha)\n",
        "    print('- min_count:',modelo.min_count)\n",
        "    print('')\n",
        "\n",
        "      # print('')\n",
        "    print('\\n'+'---'+'\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H8u76RzMiA4",
        "outputId": "8fbc5ab9-29fb-442c-8d09-a2a09ee0b84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Temporal\n",
            "### WOKE SAUDE-CORPO MODELO 1\n",
            "#### WOKE_1_SAUDE-CORPO_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 691\n",
            "- Quantidade de frases: 947.942\n",
            "- Quantidade de tokens no corpus usado no treino: 13.614.492\n",
            "- Quantidade de palavras que entraram para o treinamento: 12.740.967\n",
            "- Quantidade de tokens no vocabulário: 35.655\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_SAUDE-CORPO_2003_2013_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.134\n",
            "- Quantidade de frases: 1.472.511\n",
            "- Quantidade de tokens no corpus usado no treino: 21.034.083\n",
            "- Quantidade de palavras que entraram para o treinamento: 19.753.598\n",
            "- Quantidade de tokens no vocabulário: 48.341\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_SAUDE-CORPO_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.760\n",
            "- Quantidade de frases: 2.282.689\n",
            "- Quantidade de tokens no corpus usado no treino: 32.391.715\n",
            "- Quantidade de palavras que entraram para o treinamento: 30.522.336\n",
            "- Quantidade de tokens no vocabulário: 64.903\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_SAUDE-CORPO_2003_2019_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.380\n",
            "- Quantidade de frases: 3.066.512\n",
            "- Quantidade de tokens no corpus usado no treino: 43.399.456\n",
            "- Quantidade de palavras que entraram para o treinamento: 40.937.663\n",
            "- Quantidade de tokens no vocabulário: 80.485\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_SAUDE-CORPO_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.054\n",
            "- Quantidade de frases: 3.905.915\n",
            "- Quantidade de tokens no corpus usado no treino: 55.083.496\n",
            "- Quantidade de palavras que entraram para o treinamento: 51.964.360\n",
            "- Quantidade de tokens no vocabulário: 95.329\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 150\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE SAUDE-CORPO MODELO 2\n",
            "#### WOKE_2_SAUDE-CORPO_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 691\n",
            "- Quantidade de frases: 947.942\n",
            "- Quantidade de tokens no corpus usado no treino: 13.614.492\n",
            "- Quantidade de palavras que entraram para o treinamento: 12.740.967\n",
            "- Quantidade de tokens no vocabulário: 35.655\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_SAUDE-CORPO_2003_2013_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.134\n",
            "- Quantidade de frases: 1.472.511\n",
            "- Quantidade de tokens no corpus usado no treino: 21.034.083\n",
            "- Quantidade de palavras que entraram para o treinamento: 19.753.598\n",
            "- Quantidade de tokens no vocabulário: 48.341\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_SAUDE-CORPO_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.760\n",
            "- Quantidade de frases: 2.282.689\n",
            "- Quantidade de tokens no corpus usado no treino: 32.391.715\n",
            "- Quantidade de palavras que entraram para o treinamento: 30.522.336\n",
            "- Quantidade de tokens no vocabulário: 64.903\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_SAUDE-CORPO_2003_2019_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.380\n",
            "- Quantidade de frases: 3.066.512\n",
            "- Quantidade de tokens no corpus usado no treino: 43.399.456\n",
            "- Quantidade de palavras que entraram para o treinamento: 40.937.663\n",
            "- Quantidade de tokens no vocabulário: 80.485\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_SAUDE-CORPO_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.054\n",
            "- Quantidade de frases: 3.905.915\n",
            "- Quantidade de tokens no corpus usado no treino: 55.083.496\n",
            "- Quantidade de palavras que entraram para o treinamento: 51.964.360\n",
            "- Quantidade de tokens no vocabulário: 95.329\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 200\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 15\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE SAUDE-CORPO MODELO 3\n",
            "#### WOKE_3_SAUDE-CORPO_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 691\n",
            "- Quantidade de frases: 947.942\n",
            "- Quantidade de tokens no corpus usado no treino: 13.614.492\n",
            "- Quantidade de palavras que entraram para o treinamento: 12.487.497\n",
            "- Quantidade de tokens no vocabulário: 23.354\n",
            "###### Parâmetros\n",
            "- Modo: CBOW\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_SAUDE-CORPO_2003_2013_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.134\n",
            "- Quantidade de frases: 1.472.511\n",
            "- Quantidade de tokens no corpus usado no treino: 21.034.083\n",
            "- Quantidade de palavras que entraram para o treinamento: 19.400.078\n",
            "- Quantidade de tokens no vocabulário: 31.166\n",
            "###### Parâmetros\n",
            "- Modo: CBOW\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_SAUDE-CORPO_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 1.760\n",
            "- Quantidade de frases: 2.282.689\n",
            "- Quantidade de tokens no corpus usado no treino: 32.391.715\n",
            "- Quantidade de palavras que entraram para o treinamento: 30.037.587\n",
            "- Quantidade de tokens no vocabulário: 41.302\n",
            "###### Parâmetros\n",
            "- Modo: CBOW\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_SAUDE-CORPO_2003_2019_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 2.380\n",
            "- Quantidade de frases: 3.066.512\n",
            "- Quantidade de tokens no corpus usado no treino: 43.399.456\n",
            "- Quantidade de palavras que entraram para o treinamento: 40.335.099\n",
            "- Quantidade de tokens no vocabulário: 51.148\n",
            "###### Parâmetros\n",
            "- Modo: CBOW\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_SAUDE-CORPO_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 3.054\n",
            "- Quantidade de frases: 3.905.915\n",
            "- Quantidade de tokens no corpus usado no treino: 55.083.496\n",
            "- Quantidade de palavras que entraram para o treinamento: 51.244.642\n",
            "- Quantidade de tokens no vocabulário: 60.168\n",
            "###### Parâmetros\n",
            "- Modo: CBOW\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 30\n",
            "\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UFSC"
      ],
      "metadata": {
        "id": "VHfIyowHQjLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UFSC inc"
      ],
      "metadata": {
        "id": "CTHf01xAQlW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "treino = 'Todas 2003 - 2006'\n",
        "\n",
        "lista_de_colecoes = sorted([colecao for colecao in os.listdir(caminho_colecoes) if '.' not in colecao])\n",
        "\n",
        "tipo_treinamento = 'Treinamento incremental'\n",
        "\n",
        "caminho_treino_incremental = f'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/{treino}/Com RE/{tipo_treinamento}'\n",
        "\n",
        "print(f'## Incremental')\n",
        "for caminho_modelo_inc in sorted([os.path.join(caminho_treino_incremental,modelo) for modelo in os.listdir(caminho_treino_incremental) if '.' not in modelo]):\n",
        "\n",
        "  print(f'### WOKE UFSC {os.path.basename(caminho_modelo_inc).upper()}')\n",
        "  # c = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1'\n",
        "\n",
        "  for caminho_modelo in sorted([os.path.join(caminho_modelo_inc,modelo) for modelo in os.listdir(caminho_modelo_inc) if modelo.endswith('.model')]):\n",
        "  # caminho_modelo = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1/WOKE_1_HST_2003_2010_w2v_inc.model'\n",
        "\n",
        "    modelo = Word2Vec.load(caminho_modelo)\n",
        "\n",
        "    ano_final = os.path.basename(caminho_modelo)[-18:-14]\n",
        "    print(f'#### {os.path.basename(caminho_modelo).replace(\".model\",\"\")}')\n",
        "    print('##### Contagens')\n",
        "    qtd_trabalhos = 0\n",
        "    for caminho_colecao in sorted([os.path.join(caminho_colecoes,colecao) for colecao in os.listdir(caminho_colecoes) if '.' not in colecao and colecao in lista_de_colecoes]):\n",
        "      for caminho_ano in sorted([os.path.join(caminho_colecao,ano) for ano in os.listdir(caminho_colecao) if ano.isdigit() and ano in [str(a) for a in range(2003,int(ano_final)+1)]]):\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "        for caminho_trabalho in [os.path.join(caminho_ano,trabalho) for trabalho in os.listdir(caminho_ano) if trabalho.startswith('Trabalho')]:\n",
        "          if os.path.exists(os.path.join(caminho_trabalho,'pre_processamento_c_re.msgpack')):\n",
        "            qtd_trabalhos += 1\n",
        "      # if os.path.basename(caminho_ano) in ['2010','2013','2016','2019','2023','2024']:\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "    print('- Quantidade de trabalhos contemplados:','{0:,}'.format(qtd_trabalhos).replace(',','.'))\n",
        "    print('- Quantidade de frases:','{0:,}'.format(modelo.corpus_count).replace(',','.'))\n",
        "    print('- Quantidade de tokens no corpus usado no treino:','{0:,}'.format(modelo.corpus_total_words).replace(',','.'))\n",
        "    print('- Quantidade de palavras que entraram para o treinamento:','{0:,}'.format(sum(modelo.wv.get_vecattr(word, 'count') for word in modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('- Quantidade de tokens no vocabulário:','{0:,}'.format(len(modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('###### Parâmetros')\n",
        "    if modelo.sg == 1:\n",
        "      print('- Modo: Skip-Gram')\n",
        "    else:\n",
        "      print('- Modo: CBOW')\n",
        "    print('- vector_size:',modelo.vector_size)\n",
        "    print('- negative:',modelo.negative)\n",
        "    print('- window:',modelo.window)\n",
        "    print('- epochs',modelo.epochs)\n",
        "    print('- alpha',modelo.alpha)\n",
        "    print('- min_count:',modelo.min_count)\n",
        "    print('')\n",
        "\n",
        "      # print('')\n",
        "    print('\\n'+'---'+'\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccRF95D5QmFU",
        "outputId": "258b24be-7e6f-4f4f-918e-34657b2a906b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cols 106\n",
            "## Incremental\n",
            "### WOKE UFSC MODELO 1\n",
            "#### WOKE_1_UFSC_2003_2006_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 5.008\n",
            "- Quantidade de frases: 6.954.953\n",
            "- Quantidade de tokens no corpus usado no treino: 103.409.477\n",
            "- Quantidade de palavras que entraram para o treinamento: 95.961.384\n",
            "- Quantidade de tokens no vocabulário: 68.574\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2007_2008_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 7.341\n",
            "- Quantidade de frases: 3.549.449\n",
            "- Quantidade de tokens no corpus usado no treino: 52.151.874\n",
            "- Quantidade de palavras que entraram para o treinamento: 144.010.288\n",
            "- Quantidade de tokens no vocabulário: 73.889\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2009_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 9.710\n",
            "- Quantidade de frases: 3.589.113\n",
            "- Quantidade de tokens no corpus usado no treino: 52.301.378\n",
            "- Quantidade de palavras que entraram para o treinamento: 191.952.178\n",
            "- Quantidade de tokens no vocabulário: 80.333\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2011_2012_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 12.442\n",
            "- Quantidade de frases: 4.019.859\n",
            "- Quantidade de tokens no corpus usado no treino: 58.322.060\n",
            "- Quantidade de palavras que entraram para o treinamento: 245.451.625\n",
            "- Quantidade de tokens no vocabulário: 86.666\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2013_2014_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 15.576\n",
            "- Quantidade de frases: 4.598.336\n",
            "- Quantidade de tokens no corpus usado no treino: 66.859.472\n",
            "- Quantidade de palavras que entraram para o treinamento: 306.903.625\n",
            "- Quantidade de tokens no vocabulário: 94.177\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2015_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 19.063\n",
            "- Quantidade de frases: 5.413.473\n",
            "- Quantidade de tokens no corpus usado no treino: 78.640.230\n",
            "- Quantidade de palavras que entraram para o treinamento: 379.295.588\n",
            "- Quantidade de tokens no vocabulário: 102.998\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2017_2018_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 22.883\n",
            "- Quantidade de frases: 5.979.335\n",
            "- Quantidade de tokens no corpus usado no treino: 86.818.224\n",
            "- Quantidade de palavras que entraram para o treinamento: 459.368.636\n",
            "- Quantidade de tokens no vocabulário: 111.849\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2019_2020_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 26.313\n",
            "- Quantidade de frases: 5.343.904\n",
            "- Quantidade de tokens no corpus usado no treino: 77.487.203\n",
            "- Quantidade de palavras que entraram para o treinamento: 530.778.205\n",
            "- Quantidade de tokens no vocabulário: 118.630\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2021_2022_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 29.436\n",
            "- Quantidade de frases: 4.971.954\n",
            "- Quantidade de tokens no corpus usado no treino: 72.756.257\n",
            "- Quantidade de palavras que entraram para o treinamento: 597.626.882\n",
            "- Quantidade de tokens no vocabulário: 124.445\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2023_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 30.664\n",
            "- Quantidade de frases: 1.807.267\n",
            "- Quantidade de tokens no corpus usado no treino: 26.813.030\n",
            "- Quantidade de palavras que entraram para o treinamento: 621.966.385\n",
            "- Quantidade de tokens no vocabulário: 126.252\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE UFSC MODELO 2\n",
            "#### WOKE_2_UFSC_2003_2006_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 5.008\n",
            "- Quantidade de frases: 6.954.953\n",
            "- Quantidade de tokens no corpus usado no treino: 103.409.477\n",
            "- Quantidade de palavras que entraram para o treinamento: 95.349.285\n",
            "- Quantidade de tokens no vocabulário: 56.669\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2007_2008_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 7.341\n",
            "- Quantidade de frases: 3.549.449\n",
            "- Quantidade de tokens no corpus usado no treino: 52.151.874\n",
            "- Quantidade de palavras que entraram para o treinamento: 143.041.124\n",
            "- Quantidade de tokens no vocabulário: 60.518\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2009_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 9.710\n",
            "- Quantidade de frases: 3.589.113\n",
            "- Quantidade de tokens no corpus usado no treino: 52.301.378\n",
            "- Quantidade de palavras que entraram para o treinamento: 190.598.557\n",
            "- Quantidade de tokens no vocabulário: 65.141\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2011_2012_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 12.442\n",
            "- Quantidade de frases: 4.019.859\n",
            "- Quantidade de tokens no corpus usado no treino: 58.322.060\n",
            "- Quantidade de palavras que entraram para o treinamento: 243.676.791\n",
            "- Quantidade de tokens no vocabulário: 69.862\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2013_2014_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 15.576\n",
            "- Quantidade de frases: 4.598.336\n",
            "- Quantidade de tokens no corpus usado no treino: 66.859.472\n",
            "- Quantidade de palavras que entraram para o treinamento: 304.658.774\n",
            "- Quantidade de tokens no vocabulário: 75.535\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2015_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 19.063\n",
            "- Quantidade de frases: 5.413.473\n",
            "- Quantidade de tokens no corpus usado no treino: 78.640.230\n",
            "- Quantidade de palavras que entraram para o treinamento: 376.514.234\n",
            "- Quantidade de tokens no vocabulário: 82.348\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2017_2018_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 22.883\n",
            "- Quantidade de frases: 5.979.335\n",
            "- Quantidade de tokens no corpus usado no treino: 86.818.224\n",
            "- Quantidade de palavras que entraram para o treinamento: 456.014.140\n",
            "- Quantidade de tokens no vocabulário: 89.051\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2019_2020_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 26.313\n",
            "- Quantidade de frases: 5.343.904\n",
            "- Quantidade de tokens no corpus usado no treino: 77.487.203\n",
            "- Quantidade de palavras que entraram para o treinamento: 526.905.542\n",
            "- Quantidade de tokens no vocabulário: 94.187\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2021_2022_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 29.436\n",
            "- Quantidade de frases: 4.971.954\n",
            "- Quantidade de tokens no corpus usado no treino: 72.756.257\n",
            "- Quantidade de palavras que entraram para o treinamento: 593.278.313\n",
            "- Quantidade de tokens no vocabulário: 98.473\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2023_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 30.664\n",
            "- Quantidade de frases: 1.807.267\n",
            "- Quantidade de tokens no corpus usado no treino: 26.813.030\n",
            "- Quantidade de palavras que entraram para o treinamento: 617.437.126\n",
            "- Quantidade de tokens no vocabulário: 99.731\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE UFSC MODELO 3\n",
            "#### WOKE_3_UFSC_2003_2006_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 5.008\n",
            "- Quantidade de frases: 6.954.953\n",
            "- Quantidade de tokens no corpus usado no treino: 103.409.477\n",
            "- Quantidade de palavras que entraram para o treinamento: 95.961.384\n",
            "- Quantidade de tokens no vocabulário: 68.574\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2007_2008_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 7.341\n",
            "- Quantidade de frases: 3.549.449\n",
            "- Quantidade de tokens no corpus usado no treino: 52.151.874\n",
            "- Quantidade de palavras que entraram para o treinamento: 144.010.288\n",
            "- Quantidade de tokens no vocabulário: 73.889\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2009_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 9.710\n",
            "- Quantidade de frases: 3.589.113\n",
            "- Quantidade de tokens no corpus usado no treino: 52.301.378\n",
            "- Quantidade de palavras que entraram para o treinamento: 191.952.178\n",
            "- Quantidade de tokens no vocabulário: 80.333\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2011_2012_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 12.442\n",
            "- Quantidade de frases: 4.019.859\n",
            "- Quantidade de tokens no corpus usado no treino: 58.322.060\n",
            "- Quantidade de palavras que entraram para o treinamento: 245.451.625\n",
            "- Quantidade de tokens no vocabulário: 86.666\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2013_2014_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 15.576\n",
            "- Quantidade de frases: 4.598.336\n",
            "- Quantidade de tokens no corpus usado no treino: 66.859.472\n",
            "- Quantidade de palavras que entraram para o treinamento: 157.246.483\n",
            "- Quantidade de tokens no vocabulário: 80.904\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2015_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 19.063\n",
            "- Quantidade de frases: 5.413.473\n",
            "- Quantidade de tokens no corpus usado no treino: 78.640.230\n",
            "- Quantidade de palavras que entraram para o treinamento: 229.513.880\n",
            "- Quantidade de tokens no vocabulário: 91.778\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2017_2018_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 22.883\n",
            "- Quantidade de frases: 5.979.335\n",
            "- Quantidade de tokens no corpus usado no treino: 86.818.224\n",
            "- Quantidade de palavras que entraram para o treinamento: 175.724.732\n",
            "- Quantidade de tokens no vocabulário: 88.224\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2019_2020_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 26.313\n",
            "- Quantidade de frases: 5.343.904\n",
            "- Quantidade de tokens no corpus usado no treino: 77.487.203\n",
            "- Quantidade de palavras que entraram para o treinamento: 246.916.980\n",
            "- Quantidade de tokens no vocabulário: 97.458\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2021_2022_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 29.436\n",
            "- Quantidade de frases: 4.971.954\n",
            "- Quantidade de tokens no corpus usado no treino: 72.756.257\n",
            "- Quantidade de palavras que entraram para o treinamento: 162.341.885\n",
            "- Quantidade de tokens no vocabulário: 84.988\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2023_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 30.664\n",
            "- Quantidade de frases: 1.807.267\n",
            "- Quantidade de tokens no corpus usado no treino: 26.813.030\n",
            "- Quantidade de palavras que entraram para o treinamento: 186.499.034\n",
            "- Quantidade de tokens no vocabulário: 87.504\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE UFSC MODELO 4\n",
            "#### WOKE_4_UFSC_2003_2006_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 5.008\n",
            "- Quantidade de frases: 6.954.953\n",
            "- Quantidade de tokens no corpus usado no treino: 103.409.477\n",
            "- Quantidade de palavras que entraram para o treinamento: 95.349.285\n",
            "- Quantidade de tokens no vocabulário: 56.669\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2007_2008_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 7.341\n",
            "- Quantidade de frases: 3.549.449\n",
            "- Quantidade de tokens no corpus usado no treino: 52.151.874\n",
            "- Quantidade de palavras que entraram para o treinamento: 143.041.124\n",
            "- Quantidade de tokens no vocabulário: 60.518\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2009_2010_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 9.710\n",
            "- Quantidade de frases: 3.589.113\n",
            "- Quantidade de tokens no corpus usado no treino: 52.301.378\n",
            "- Quantidade de palavras que entraram para o treinamento: 190.598.557\n",
            "- Quantidade de tokens no vocabulário: 65.141\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2011_2012_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 12.442\n",
            "- Quantidade de frases: 4.019.859\n",
            "- Quantidade de tokens no corpus usado no treino: 58.322.060\n",
            "- Quantidade de palavras que entraram para o treinamento: 243.676.791\n",
            "- Quantidade de tokens no vocabulário: 69.862\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2013_2014_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 15.576\n",
            "- Quantidade de frases: 4.598.336\n",
            "- Quantidade de tokens no corpus usado no treino: 66.859.472\n",
            "- Quantidade de palavras que entraram para o treinamento: 304.658.774\n",
            "- Quantidade de tokens no vocabulário: 75.535\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2015_2016_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 19.063\n",
            "- Quantidade de frases: 5.413.473\n",
            "- Quantidade de tokens no corpus usado no treino: 78.640.230\n",
            "- Quantidade de palavras que entraram para o treinamento: 166.975.166\n",
            "- Quantidade de tokens no vocabulário: 69.659\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2017_2018_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 22.883\n",
            "- Quantidade de frases: 5.979.335\n",
            "- Quantidade de tokens no corpus usado no treino: 86.818.224\n",
            "- Quantidade de palavras que entraram para o treinamento: 246.311.336\n",
            "- Quantidade de tokens no vocabulário: 78.262\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2019_2020_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 26.313\n",
            "- Quantidade de frases: 5.343.904\n",
            "- Quantidade de tokens no corpus usado no treino: 77.487.203\n",
            "- Quantidade de palavras que entraram para o treinamento: 165.801.271\n",
            "- Quantidade de tokens no vocabulário: 70.499\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2021_2022_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 29.436\n",
            "- Quantidade de frases: 4.971.954\n",
            "- Quantidade de tokens no corpus usado no treino: 72.756.257\n",
            "- Quantidade de palavras que entraram para o treinamento: 231.860.234\n",
            "- Quantidade de tokens no vocabulário: 77.136\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2023_2024_w2v_inc\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 30.664\n",
            "- Quantidade de frases: 1.807.267\n",
            "- Quantidade de tokens no corpus usado no treino: 26.813.030\n",
            "- Quantidade de palavras que entraram para o treinamento: 255.900.500\n",
            "- Quantidade de tokens no vocabulário: 78.645\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UFSC tmp"
      ],
      "metadata": {
        "id": "Si3T7vO2b6V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "treino = 'Todas 2003 - 2006'\n",
        "\n",
        "lista_de_colecoes = sorted([colecao for colecao in os.listdir(caminho_colecoes) if '.' not in colecao])\n",
        "\n",
        "tipo_treinamento = 'Treinamento temporal'\n",
        "\n",
        "caminho_treino_incremental = f'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/{treino}/Com RE/{tipo_treinamento}'\n",
        "\n",
        "print(f'## Temporal')\n",
        "for caminho_modelo_inc in sorted([os.path.join(caminho_treino_incremental,modelo) for modelo in os.listdir(caminho_treino_incremental) if '.' not in modelo]):\n",
        "\n",
        "  print(f'### WOKE UFSC {os.path.basename(caminho_modelo_inc).upper()}')\n",
        "  # c = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1'\n",
        "\n",
        "  for caminho_modelo in sorted([os.path.join(caminho_modelo_inc,modelo) for modelo in os.listdir(caminho_modelo_inc) if modelo.endswith('.model')]):\n",
        "  # caminho_modelo = r'/content/drive/MyDrive/Programa - Repositório Institucional UFSC/Word Embeddings/Treinamento do nosso modelo/Treinamento com temporalização/HST-03-10/Com RE/Treinamento incremental/Modelo 1/WOKE_1_HST_2003_2010_w2v_inc.model'\n",
        "\n",
        "    modelo = Word2Vec.load(caminho_modelo)\n",
        "\n",
        "    ano_final = os.path.basename(caminho_modelo)[-18:-14]\n",
        "    print(f'#### {os.path.basename(caminho_modelo).replace(\".model\",\"\")}')\n",
        "    print('##### Contagens')\n",
        "    qtd_trabalhos = 0\n",
        "    for caminho_colecao in sorted([os.path.join(caminho_colecoes,colecao) for colecao in os.listdir(caminho_colecoes) if '.' not in colecao and colecao in lista_de_colecoes]):\n",
        "      for caminho_ano in sorted([os.path.join(caminho_colecao,ano) for ano in os.listdir(caminho_colecao) if ano.isdigit() and ano in [str(a) for a in range(2003,int(ano_final)+1)]]):\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "        for caminho_trabalho in [os.path.join(caminho_ano,trabalho) for trabalho in os.listdir(caminho_ano) if trabalho.startswith('Trabalho')]:\n",
        "          if os.path.exists(os.path.join(caminho_trabalho,'pre_processamento_c_re.msgpack')):\n",
        "            qtd_trabalhos += 1\n",
        "      # if os.path.basename(caminho_ano) in ['2010','2013','2016','2019','2023','2024']:\n",
        "        # print(os.path.basename(caminho_ano))\n",
        "    print('- Quantidade de trabalhos contemplados:','{0:,}'.format(qtd_trabalhos).replace(',','.'))\n",
        "    print('- Quantidade de frases:','{0:,}'.format(modelo.corpus_count).replace(',','.'))\n",
        "    print('- Quantidade de tokens no corpus usado no treino:','{0:,}'.format(modelo.corpus_total_words).replace(',','.'))\n",
        "    print('- Quantidade de palavras que entraram para o treinamento:','{0:,}'.format(sum(modelo.wv.get_vecattr(word, 'count') for word in modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('- Quantidade de tokens no vocabulário:','{0:,}'.format(len(modelo.wv.index_to_key)).replace(',','.'))\n",
        "    print('###### Parâmetros')\n",
        "    if modelo.sg == 1:\n",
        "      print('- Modo: Skip-Gram')\n",
        "    else:\n",
        "      print('- Modo: CBOW')\n",
        "    print('- vector_size:',modelo.vector_size)\n",
        "    print('- negative:',modelo.negative)\n",
        "    print('- window:',modelo.window)\n",
        "    print('- epochs',modelo.epochs)\n",
        "    print('- alpha',modelo.alpha)\n",
        "    print('- min_count:',modelo.min_count)\n",
        "    print('')\n",
        "\n",
        "      # print('')\n",
        "    print('\\n'+'---'+'\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6-YiZ8Ib7TO",
        "outputId": "a85ef106-e727-4b63-99da-bee8c48cccad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cols 106\n",
            "## Temporal\n",
            "### WOKE UFSC MODELO 1\n",
            "#### WOKE_1_UFSC_2003_2006_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 5.008\n",
            "- Quantidade de frases: 6.954.953\n",
            "- Quantidade de tokens no corpus usado no treino: 103.409.477\n",
            "- Quantidade de palavras que entraram para o treinamento: 95.961.384\n",
            "- Quantidade de tokens no vocabulário: 68.574\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2003_2008_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 7.341\n",
            "- Quantidade de frases: 10.504.402\n",
            "- Quantidade de tokens no corpus usado no treino: 155.561.351\n",
            "- Quantidade de palavras que entraram para o treinamento: 145.089.242\n",
            "- Quantidade de tokens no vocabulário: 91.577\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 9.710\n",
            "- Quantidade de frases: 14.093.515\n",
            "- Quantidade de tokens no corpus usado no treino: 207.862.729\n",
            "- Quantidade de palavras que entraram para o treinamento: 194.339.554\n",
            "- Quantidade de tokens no vocabulário: 113.866\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2003_2012_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 12.442\n",
            "- Quantidade de frases: 18.113.374\n",
            "- Quantidade de tokens no corpus usado no treino: 266.184.789\n",
            "- Quantidade de palavras que entraram para o treinamento: 249.361.114\n",
            "- Quantidade de tokens no vocabulário: 135.690\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2003_2014_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 15.576\n",
            "- Quantidade de frases: 22.711.710\n",
            "- Quantidade de tokens no corpus usado no treino: 333.044.261\n",
            "- Quantidade de palavras que entraram para o treinamento: 312.582.656\n",
            "- Quantidade de tokens no vocabulário: 158.822\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 19.063\n",
            "- Quantidade de frases: 28.125.183\n",
            "- Quantidade de tokens no corpus usado no treino: 411.684.491\n",
            "- Quantidade de palavras que entraram para o treinamento: 387.107.285\n",
            "- Quantidade de tokens no vocabulário: 184.847\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2003_2018_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 22.883\n",
            "- Quantidade de frases: 34.104.518\n",
            "- Quantidade de tokens no corpus usado no treino: 498.502.715\n",
            "- Quantidade de palavras que entraram para o treinamento: 469.458.520\n",
            "- Quantidade de tokens no vocabulário: 211.144\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2003_2020_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 26.313\n",
            "- Quantidade de frases: 39.448.422\n",
            "- Quantidade de tokens no corpus usado no treino: 575.989.918\n",
            "- Quantidade de palavras que entraram para o treinamento: 542.979.577\n",
            "- Quantidade de tokens no vocabulário: 233.995\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2003_2022_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 29.436\n",
            "- Quantidade de frases: 44.420.376\n",
            "- Quantidade de tokens no corpus usado no treino: 648.746.175\n",
            "- Quantidade de palavras que entraram para o treinamento: 611.803.023\n",
            "- Quantidade de tokens no vocabulário: 254.264\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_1_UFSC_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 30.664\n",
            "- Quantidade de frases: 46.227.643\n",
            "- Quantidade de tokens no corpus usado no treino: 675.559.205\n",
            "- Quantidade de palavras que entraram para o treinamento: 636.930.327\n",
            "- Quantidade de tokens no vocabulário: 262.207\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE UFSC MODELO 2\n",
            "#### WOKE_2_UFSC_2003_2006_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 5.008\n",
            "- Quantidade de frases: 6.954.953\n",
            "- Quantidade de tokens no corpus usado no treino: 103.409.477\n",
            "- Quantidade de palavras que entraram para o treinamento: 95.349.285\n",
            "- Quantidade de tokens no vocabulário: 56.669\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2003_2008_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 7.341\n",
            "- Quantidade de frases: 10.504.402\n",
            "- Quantidade de tokens no corpus usado no treino: 155.561.351\n",
            "- Quantidade de palavras que entraram para o treinamento: 144.273.606\n",
            "- Quantidade de tokens no vocabulário: 75.709\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 9.710\n",
            "- Quantidade de frases: 14.093.515\n",
            "- Quantidade de tokens no corpus usado no treino: 207.862.729\n",
            "- Quantidade de palavras que entraram para o treinamento: 193.315.891\n",
            "- Quantidade de tokens no vocabulário: 93.940\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2003_2012_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 12.442\n",
            "- Quantidade de frases: 18.113.374\n",
            "- Quantidade de tokens no corpus usado no treino: 266.184.789\n",
            "- Quantidade de palavras que entraram para o treinamento: 248.119.080\n",
            "- Quantidade de tokens no vocabulário: 111.533\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2003_2014_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 15.576\n",
            "- Quantidade de frases: 22.711.710\n",
            "- Quantidade de tokens no corpus usado no treino: 333.044.261\n",
            "- Quantidade de palavras que entraram para o treinamento: 311.142.132\n",
            "- Quantidade de tokens no vocabulário: 130.795\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 19.063\n",
            "- Quantidade de frases: 28.125.183\n",
            "- Quantidade de tokens no corpus usado no treino: 411.684.491\n",
            "- Quantidade de palavras que entraram para o treinamento: 385.398.030\n",
            "- Quantidade de tokens no vocabulário: 151.603\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2003_2018_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 22.883\n",
            "- Quantidade de frases: 34.104.518\n",
            "- Quantidade de tokens no corpus usado no treino: 498.502.715\n",
            "- Quantidade de palavras que entraram para o treinamento: 467.498.259\n",
            "- Quantidade de tokens no vocabulário: 173.001\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2003_2020_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 26.313\n",
            "- Quantidade de frases: 39.448.422\n",
            "- Quantidade de tokens no corpus usado no treino: 575.989.918\n",
            "- Quantidade de palavras que entraram para o treinamento: 540.807.246\n",
            "- Quantidade de tokens no vocabulário: 191.718\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2003_2022_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 29.436\n",
            "- Quantidade de frases: 44.420.376\n",
            "- Quantidade de tokens no corpus usado no treino: 648.746.175\n",
            "- Quantidade de palavras que entraram para o treinamento: 609.438.264\n",
            "- Quantidade de tokens no vocabulário: 208.233\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_2_UFSC_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 30.664\n",
            "- Quantidade de frases: 46.227.643\n",
            "- Quantidade de tokens no corpus usado no treino: 675.559.205\n",
            "- Quantidade de palavras que entraram para o treinamento: 634.486.228\n",
            "- Quantidade de tokens no vocabulário: 214.627\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE UFSC MODELO 3\n",
            "#### WOKE_3_UFSC_2003_2006_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 5.008\n",
            "- Quantidade de frases: 6.954.953\n",
            "- Quantidade de tokens no corpus usado no treino: 103.409.477\n",
            "- Quantidade de palavras que entraram para o treinamento: 95.961.384\n",
            "- Quantidade de tokens no vocabulário: 68.574\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2003_2008_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 7.341\n",
            "- Quantidade de frases: 10.504.402\n",
            "- Quantidade de tokens no corpus usado no treino: 155.561.351\n",
            "- Quantidade de palavras que entraram para o treinamento: 145.089.242\n",
            "- Quantidade de tokens no vocabulário: 91.577\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 9.710\n",
            "- Quantidade de frases: 14.093.515\n",
            "- Quantidade de tokens no corpus usado no treino: 207.862.729\n",
            "- Quantidade de palavras que entraram para o treinamento: 194.339.554\n",
            "- Quantidade de tokens no vocabulário: 113.866\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2003_2012_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 12.442\n",
            "- Quantidade de frases: 18.113.374\n",
            "- Quantidade de tokens no corpus usado no treino: 266.184.789\n",
            "- Quantidade de palavras que entraram para o treinamento: 249.361.114\n",
            "- Quantidade de tokens no vocabulário: 135.690\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2003_2014_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 15.576\n",
            "- Quantidade de frases: 22.711.710\n",
            "- Quantidade de tokens no corpus usado no treino: 333.044.261\n",
            "- Quantidade de palavras que entraram para o treinamento: 312.582.656\n",
            "- Quantidade de tokens no vocabulário: 158.822\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 19.063\n",
            "- Quantidade de frases: 28.125.183\n",
            "- Quantidade de tokens no corpus usado no treino: 411.684.491\n",
            "- Quantidade de palavras que entraram para o treinamento: 387.107.285\n",
            "- Quantidade de tokens no vocabulário: 184.847\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2003_2018_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 22.883\n",
            "- Quantidade de frases: 34.104.518\n",
            "- Quantidade de tokens no corpus usado no treino: 498.502.715\n",
            "- Quantidade de palavras que entraram para o treinamento: 469.458.520\n",
            "- Quantidade de tokens no vocabulário: 211.144\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2003_2020_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 26.313\n",
            "- Quantidade de frases: 39.448.422\n",
            "- Quantidade de tokens no corpus usado no treino: 575.989.918\n",
            "- Quantidade de palavras que entraram para o treinamento: 542.979.577\n",
            "- Quantidade de tokens no vocabulário: 233.995\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2003_2022_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 29.436\n",
            "- Quantidade de frases: 44.420.376\n",
            "- Quantidade de tokens no corpus usado no treino: 648.746.175\n",
            "- Quantidade de palavras que entraram para o treinamento: 611.803.023\n",
            "- Quantidade de tokens no vocabulário: 254.264\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_3_UFSC_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 30.664\n",
            "- Quantidade de frases: 46.227.643\n",
            "- Quantidade de tokens no corpus usado no treino: 675.559.205\n",
            "- Quantidade de palavras que entraram para o treinamento: 636.930.327\n",
            "- Quantidade de tokens no vocabulário: 262.207\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 300\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 45\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "### WOKE UFSC MODELO 4\n",
            "#### WOKE_4_UFSC_2003_2006_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 5.008\n",
            "- Quantidade de frases: 6.954.953\n",
            "- Quantidade de tokens no corpus usado no treino: 103.409.477\n",
            "- Quantidade de palavras que entraram para o treinamento: 95.349.285\n",
            "- Quantidade de tokens no vocabulário: 56.669\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2003_2008_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 7.341\n",
            "- Quantidade de frases: 10.504.402\n",
            "- Quantidade de tokens no corpus usado no treino: 155.561.351\n",
            "- Quantidade de palavras que entraram para o treinamento: 144.273.606\n",
            "- Quantidade de tokens no vocabulário: 75.709\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2003_2010_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 9.710\n",
            "- Quantidade de frases: 14.093.515\n",
            "- Quantidade de tokens no corpus usado no treino: 207.862.729\n",
            "- Quantidade de palavras que entraram para o treinamento: 193.315.891\n",
            "- Quantidade de tokens no vocabulário: 93.940\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2003_2012_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 12.442\n",
            "- Quantidade de frases: 18.113.374\n",
            "- Quantidade de tokens no corpus usado no treino: 266.184.789\n",
            "- Quantidade de palavras que entraram para o treinamento: 248.119.080\n",
            "- Quantidade de tokens no vocabulário: 111.533\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2003_2014_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 15.576\n",
            "- Quantidade de frases: 22.711.710\n",
            "- Quantidade de tokens no corpus usado no treino: 333.044.261\n",
            "- Quantidade de palavras que entraram para o treinamento: 311.142.132\n",
            "- Quantidade de tokens no vocabulário: 130.795\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2003_2016_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 19.063\n",
            "- Quantidade de frases: 28.125.183\n",
            "- Quantidade de tokens no corpus usado no treino: 411.684.491\n",
            "- Quantidade de palavras que entraram para o treinamento: 385.398.030\n",
            "- Quantidade de tokens no vocabulário: 151.603\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2003_2018_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 22.883\n",
            "- Quantidade de frases: 34.104.518\n",
            "- Quantidade de tokens no corpus usado no treino: 498.502.715\n",
            "- Quantidade de palavras que entraram para o treinamento: 467.498.259\n",
            "- Quantidade de tokens no vocabulário: 173.001\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2003_2020_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 26.313\n",
            "- Quantidade de frases: 39.448.422\n",
            "- Quantidade de tokens no corpus usado no treino: 575.989.918\n",
            "- Quantidade de palavras que entraram para o treinamento: 540.807.246\n",
            "- Quantidade de tokens no vocabulário: 191.718\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2003_2022_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 29.436\n",
            "- Quantidade de frases: 44.420.376\n",
            "- Quantidade de tokens no corpus usado no treino: 648.746.175\n",
            "- Quantidade de palavras que entraram para o treinamento: 609.438.264\n",
            "- Quantidade de tokens no vocabulário: 208.233\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "#### WOKE_4_UFSC_2003_2024_w2v_tmp\n",
            "##### Contagens\n",
            "- Quantidade de trabalhos contemplados: 30.664\n",
            "- Quantidade de frases: 46.227.643\n",
            "- Quantidade de tokens no corpus usado no treino: 675.559.205\n",
            "- Quantidade de palavras que entraram para o treinamento: 634.486.228\n",
            "- Quantidade de tokens no vocabulário: 214.627\n",
            "###### Parâmetros\n",
            "- Modo: Skip-Gram\n",
            "- vector_size: 500\n",
            "- negative: 10\n",
            "- window: 12\n",
            "- epochs 5\n",
            "- alpha 0.025\n",
            "- min_count: 60\n",
            "\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    }
  ]
}